import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import os
import glob
import copy
import argparse
from sklearn.metrics import classification_report

# å¯¼å…¥ä½ çš„æ¨¡å‹å®šä¹‰å’Œé…ç½®
import sparse_model 
from sparse_auto_train import CONFIG, parse_filename_info, process_files, augment_dataset_in_memory
# ================= é…ç½®åŒºåŸŸ =================
# 1. åŸºç¡€è®¾ç½®
DEFAULT_MODEL_PATH = "sparse_train_logs_pytorch_tcn/best_model.pth"# ä½ çš„ .pth è·¯å¾„
FINETUNE_LR = 0.0001           # å¾®è°ƒé€šå¸¸ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
EPOCHS = 30                    # å¾®è°ƒè½®æ•°
BATCH_SIZE = 32

AUGMENT_CONFIG = {
    'enable_rest': True,       # ç¡®ä¿åŒ…å«é™æ¯æ•°æ®
    'multiplier': 20,          # âš¡ å¢å¼º 20 å€
    'enable_scaling': True,    # å¼€å¯å¹…åº¦ç¼©æ”¾
    'enable_noise': True,      # å¼€å¯é«˜æ–¯å™ªå£°
    'enable_warp': False,      # å…³é—­æ—¶é—´æ‰­æ›² (è®¡ç®—é‡å¤§ä¸”å¾®è°ƒé€šå¸¸ä¸éœ€è¦)
    'enable_shift': False,     
    'enable_mask': False       
}

# 2. å¾®è°ƒç­–ç•¥
# True: å…¨é‡å¾®è°ƒ (æ‰€æœ‰å±‚éƒ½å‚ä¸æ›´æ–°)
# False: åªè®­ç»ƒåˆ†ç±»å¤´ (å†»ç»“å·ç§¯å±‚ï¼Œé€‚åˆæ•°æ®æå°‘çš„æƒ…å†µ)
UNFREEZE_ALL = True            

# 3. æ–°çš„æ•°æ®é›† (ç”¨äºå¾®è°ƒçš„ç›®æ ‡)
TARGET_SUBJECTS = ["fred"] # æˆ–è€…æ˜¯ä½ æƒ³æµ‹è¯•çš„æ–°ç”¨æˆ·
TARGET_LABELS = [5, 6, 8]   # åŠ¨ä½œå¿…é¡»ä¸åŸæ¨¡å‹ä¸€è‡´
SHOTS_PER_CLASS = 2            # Few-shot: æ¯ä¸ªç±»åˆ«åªç”¨ 2 ä¸ªæ ·æœ¬è®­ç»ƒ

# ===========================================

def load_pretrained_model(path, device, num_classes, input_channels):
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½åŸºæ¨¡å‹: {path}")
    
    # 1. å®ä¾‹åŒ–æ–°æ¨¡å‹ (ä½¿ç”¨å½“å‰çš„ num_classesï¼Œä¾‹å¦‚ 4)
    model = sparse_model.TCNModel(
        input_channels=input_channels, 
        num_classes=num_classes
    )
    
    # 2. åŠ è½½æ—§æ¨¡å‹çš„æƒé‡å­—å…¸
    if os.path.exists(path):
        pretrained_dict = torch.load(path, map_location=device)
    else:
        print(f"âŒ æ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶: {path}")
        exit()

    # 3. è·å–æ–°æ¨¡å‹çš„æƒé‡å­—å…¸
    model_dict = model.state_dict()
    
    filtered_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and 'fc' not in k}
    
    # 4. æ›´æ–°æƒé‡
    model_dict.update(filtered_dict)
    model.load_state_dict(model_dict)
    
    print(f"âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡ (å·²ä¸¢å¼ƒæ—§åˆ†ç±»å¤´ {len(pretrained_dict)} -> {len(filtered_dict)} å±‚)")
    
    return model

def prepare_few_shot_data(X, y, shots=5):
    """ä»æ•°æ®é›†ä¸­æ¯ä¸ªç±»åˆ«éšæœºæŠ½å– k ä¸ªæ ·æœ¬ä½œä¸ºè®­ç»ƒé›†ï¼Œå…¶ä½™ä½œä¸ºæµ‹è¯•é›†"""
    train_indices = []
    test_indices = []
    
    unique_labels = np.unique(y)
    for label in unique_labels:
        # æ‰¾åˆ°è¯¥ç±»åˆ«çš„æ‰€æœ‰ç´¢å¼•
        idx = np.where(y == label)[0]
        np.random.shuffle(idx)
        
        if len(idx) >= shots:
            train_indices.extend(idx[:shots])
            test_indices.extend(idx[shots:])
        else:
            # å¦‚æœæ ·æœ¬ä¸å¤Ÿï¼Œå…¨æ”¾å…¥è®­ç»ƒé›†ï¼ˆæˆ–è€…æŠ¥é”™ï¼‰
            train_indices.extend(idx)
            
    return train_indices, test_indices

def run_finetuning(base_model_path):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # 1. æŸ¥æ‰¾å¹¶åŠ è½½æ–°æ•°æ®
    search_pattern = os.path.join("data", "*", "*", "RAW_EMG*.csv")
    all_files = glob.glob(search_pattern)
    target_files = []
    for f in all_files:
        s, d, l, _ = parse_filename_info(f)
        if s in TARGET_SUBJECTS and l in TARGET_LABELS:
            target_files.append(f)
            
    if not target_files:
        print(f"âŒ æœªæ‰¾åˆ° {TARGET_SUBJECTS} çš„æ•°æ®ï¼Œæ— æ³•å¾®è°ƒã€‚")
        return

    # å¤ç”¨ sparse_auto_train çš„æ•°æ®å¤„ç†å‡½æ•°
    # æ³¨æ„ï¼šå¾®è°ƒæ—¶é€šå¸¸å…³é—­å¢å¼ºï¼Œæˆ–è€…åªåšè½»å¾®å¢å¼º
    dummy_aug = {'enable_rest': True, 'multiplier': 1}
    
    # âœ… 1. æ•è· groups (å¢å¼ºå‡½æ•°éœ€è¦ç”¨åˆ°)
    X_all, y_all, groups_all = process_files(target_files, CONFIG, dummy_aug)
    
    unique_labels = sorted(TARGET_LABELS)
    
    # æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦æœ‰é™æ¯ç±» (0)ï¼Œå¦‚æœæœ‰ä¸”æ²¡åŒ…å«åœ¨ç›®æ ‡é‡Œï¼Œæ‰‹åŠ¨åŠ ä¸Š
    # è¿™æ ·åšèƒ½ä¿è¯ 0 (Rest) æ˜ å°„ä¸º 0ï¼Œ5->1, 6->2 ... ä¸è®­ç»ƒæ—¶çš„é€»è¾‘å¯¹é½
    if 0 in y_all and 0 not in unique_labels:
        unique_labels = [0] + unique_labels
        
    label_map = {orig: new for new, orig in enumerate(unique_labels)}
    
    y_mapped = np.array([label_map[y] for y in y_all])
    
    # 2. åˆ’åˆ† Few-shot æ•°æ®é›†
    train_idx, test_idx = prepare_few_shot_data(X_all, y_mapped, shots=SHOTS_PER_CLASS)
    
    # æå–åŸå§‹ numpy æ•°æ®
    X_train_raw = X_all[train_idx]
    y_train_raw = y_all[train_idx] # æ³¨æ„ï¼šè¿™æ˜¯åŸå§‹æ ‡ç­¾è¿˜æ˜¯æ˜ å°„åçš„ï¼Ÿè¿™é‡Œåº”ä½¿ç”¨åŸå§‹y_allå¯¹åº”çš„å€¼ï¼Œæˆ–è€…æ³¨æ„å¢å¼ºå‡½æ•°å¯¹yçš„å¤„ç†
    # ä¿®æ­£ï¼šaugment_dataset_in_memory éœ€è¦åŸå§‹ y è¿˜æ˜¯æ˜ å°„åçš„ y éƒ½å¯ä»¥ï¼Œä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬ç”¨æ˜ å°„å‰çš„é€»è¾‘ï¼Œæˆ–è€…ç›´æ¥å¯¹ mapped åçš„åšå¢å¼ºï¼ˆåªè¦å®ƒæ˜¯ arrayï¼‰
    # ç®€å•åšæ³•ï¼šç›´æ¥å¯¹ split å‡ºæ¥çš„ numpy array åšå¢å¼º
    
    groups_train = groups_all[train_idx]
    
    # âœ… 2. æ‰§è¡Œå†…å­˜å¢å¼º (ä»…å¯¹è®­ç»ƒé›†)
    print(f"ğŸš€ æ­£åœ¨å¯¹è®­ç»ƒé›†è¿›è¡Œ 20 å€å¢å¼º...")
    # æ³¨æ„ï¼šè¿™é‡Œçš„ y_mapped[train_idx] å·²ç»æ˜¯æ˜ å°„å¥½çš„ 0-4 æ ‡ç­¾ï¼Œå¯ä»¥ç›´æ¥å¢å¼º
    X_train_aug, y_train_aug, _ = augment_dataset_in_memory(
        X_train_raw, 
        y_mapped[train_idx], # ä¼ å…¥æ˜ å°„åçš„æ ‡ç­¾
        groups_train, 
        AUGMENT_CONFIG
    )
    
    print(f"ğŸ“ˆ å¢å¼ºåè®­ç»ƒæ ·æœ¬æ•°: {len(X_train_aug)}")

    # âœ… 3. è½¬ Tensor (æ³¨æ„ç”¨å¢å¼ºåçš„æ•°æ®)
    X_train = torch.FloatTensor(X_train_aug).permute(0, 2, 1) 
    y_train = torch.LongTensor(y_train_aug)
    
    # æµ‹è¯•é›†ä¿æŒåŸæ ·
    X_test = torch.FloatTensor(X_all[test_idx]).permute(0, 2, 1)
    y_test = torch.LongTensor(y_mapped[test_idx])
    
    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)
    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32, shuffle=False)
    
    print(f"ğŸ“Š å¾®è°ƒæ•°æ®: è®­ç»ƒé›† {len(train_idx)} (æ¯ä¸ªç±» {SHOTS_PER_CLASS} ä¸ª), æµ‹è¯•é›† {len(test_idx)}")
    
    # 3. åŠ è½½æ¨¡å‹
    input_channels = X_train.shape[1] # 11 (IMU) or 8 (EMG)
    num_classes = len(unique_labels)
    model = load_pretrained_model(base_model_path, device, num_classes, input_channels)
    model.to(device)
    
    # 4. å†»ç»“/è§£å†»ç­–ç•¥
    if not UNFREEZE_ALL:
        print("â„ï¸ å†»ç»“ç‰¹å¾æå–å±‚ï¼Œä»…è®­ç»ƒåˆ†ç±»å¤´...")
        for name, param in model.named_parameters():
            # å‡è®¾æœ€åä¸€å±‚å« 'fc' æˆ– 'linear'ï¼Œå…¶ä»–éƒ½å†»ç»“
            if 'fc' not in name and 'classifier' not in name:
                param.requires_grad = False
    else:
        print("ğŸ”¥ å…¨é‡å¾®è°ƒ: æ‰€æœ‰å‚æ•°å‚ä¸æ›´æ–°")

    # 5. å®šä¹‰ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(model.parameters(), lr=FINETUNE_LR, weight_decay=1e-2)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    
    # 6. å¾®è°ƒå¾ªç¯
    best_acc = 0.0
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            
        # éªŒè¯
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()
        
        acc = correct / total
        if acc > best_acc:
            best_acc = acc
            torch.save(model.state_dict(), "finetuned_best.pth")
            
        if (epoch+1) % 5 == 0:
            print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss:.4f} | Val Acc: {acc:.4f}")
            
    print(f"âœ… å¾®è°ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
    print("æ¨¡å‹å·²ä¿å­˜ä¸º finetuned_best.pth")

if __name__ == "__main__":
    # âœ… æ·»åŠ å‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="Few-shot Finetuning Script")
    parser.add_argument(
        '--model_path', 
        type=str, 
        default=DEFAULT_MODEL_PATH,
        help='Path to the pretrained base model weights (.pth)'
    )
    
    args = parser.parse_args()
    
    # âœ… å°†è§£æåˆ°çš„è·¯å¾„ä¼ ç»™å‡½æ•°
    run_finetuning(base_model_path=args.model_path)
