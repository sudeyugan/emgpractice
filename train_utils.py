import time
import os
import numpy as np
import tensorflow as tf
import streamlit as st
from tensorflow.keras.callbacks import Callback
from sklearn.model_selection import GroupShuffleSplit


# ================= åŸæœ‰åŠŸèƒ½ä¿æŒä¸å˜ =================

class StreamlitKerasCallback(Callback):
    """
    ç”¨äºè¿æ¥ Keras è®­ç»ƒè¿‡ç¨‹ä¸ Streamlit è¿›åº¦æ¡çš„è‡ªå®šä¹‰å›è°ƒ
    """
    def __init__(self, total_epochs, progress_bar, status_text):
        super().__init__()
        self.total_epochs = total_epochs
        self.progress_bar = progress_bar
        self.status_text = status_text
        self.start_time = None

    def on_train_begin(self, logs=None):
        self.start_time = time.time()
        self.progress_bar.progress(0)
        self.status_text.text("ğŸš€ å‡†å¤‡å¼€å§‹è®­ç»ƒ...")

    def on_epoch_end(self, epoch, logs=None):
        current_epoch = epoch + 1
        progress = min(current_epoch / self.total_epochs, 1.0)
        self.progress_bar.progress(progress)
        
        elapsed_time = time.time() - self.start_time
        avg_time_per_epoch = elapsed_time / current_epoch
        remaining_epochs = self.total_epochs - current_epoch
        eta_seconds = avg_time_per_epoch * remaining_epochs
        
        eta_str = time.strftime("%M:%S", time.gmtime(eta_seconds))
        
        loss = logs.get('loss', 0)
        acc = logs.get('accuracy', 0)
        val_loss = logs.get('val_loss', 0)
        val_acc = logs.get('val_accuracy', 0)
        
        status_msg = (
            f"Epoch {current_epoch}/{self.total_epochs} | "
            f"â³ å‰©ä½™: {eta_str} | "
            f"Loss: {loss:.4f} Acc: {acc:.1%} | "
            f"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.1%}"
        )
        self.status_text.text(status_msg)

    def on_train_end(self, logs=None):
        self.progress_bar.progress(100)
        self.status_text.text("âœ… è®­ç»ƒå·²å®Œæˆï¼")

def smart_split(X, y, groups, strategy, test_size=0.2, manual_target=None):
    """
    æ ¹æ®ä¸åŒç­–ç•¥åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    """
    indices = np.arange(len(X))
    train_idx, test_idx = [], []
    
    unique_files = np.unique(groups)
    
    # --- ç­–ç•¥ 1: æ··åˆåˆ‡åˆ† ---
    if strategy == "æ··åˆåˆ‡åˆ† (çœ‹åˆ°æ‰€æœ‰å¤©/äºº)":
        for f in unique_files:
            f_indices = indices[groups == f]
            split_point = int(len(f_indices) * (1 - test_size))
            train_idx.extend(f_indices[:split_point])
            test_idx.extend(f_indices[split_point:])
            
    # --- ç­–ç•¥ 2: ç•™æ–‡ä»¶éªŒè¯ ---
    elif strategy == "ç•™æ–‡ä»¶éªŒè¯ (åŒå¤©/åŒäºº)":
        if manual_target:
            is_test = np.array([os.path.basename(g.split('_seg')[0]) == manual_target for g in groups])
            test_idx = indices[is_test]
            train_idx = indices[~is_test]
        else:
            gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=42)
            train_i, test_i = next(gss.split(X, y, groups=groups))
            train_idx, test_idx = indices[train_i], indices[test_i]

    # --- ç­–ç•¥ 3: ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯ ---
    elif strategy == "ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯ (æ³›åŒ–èƒ½åŠ›)":
        real_groups = np.array([os.path.basename(os.path.dirname(f)) for f in groups])
        
        if manual_target:
            is_test = (real_groups == manual_target)
            test_idx = indices[is_test]
            train_idx = indices[~is_test]
        else:
            gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=42)
            train_i, test_i = next(gss.split(X, y, groups=real_groups))
            train_idx, test_idx = indices[train_i], indices[test_i]
        
    return np.array(train_idx), np.array(test_idx)

def get_few_shot_split(X, y, n_samples_per_class):
    train_idx = []
    test_idx = []
    unique_labels = np.unique(y)
    
    for label in unique_labels:
        label_indices = np.where(y == label)[0]
        np.random.shuffle(label_indices)
        train_idx.extend(label_indices[:n_samples_per_class])
        test_idx.extend(label_indices[n_samples_per_class:])
        
    return np.array(train_idx), np.array(test_idx)

# ================= æ–°å¢ï¼šæŠ•ç¥¨è®­ç»ƒæ”¯æŒå‡½æ•° =================

def group_batch_generator(X, y, groups, batch_size, samples_per_group=5):
    """
    ç”Ÿæˆå™¨ï¼šæ¯æ¬¡äº§å‡ºä¸€ä¸ª Batchï¼Œå…¶ä¸­åŒ…å« `batch_size` ä¸ªç»„ï¼ˆSegmentï¼‰ã€‚
    æ¯ä¸ªç»„åŒ…å« `samples_per_group` ä¸ªåˆ‡ç‰‡ã€‚
    è¿”å›å½¢çŠ¶: (batch_size, samples_per_group, time_steps, features)
    """
    unique_groups = np.unique(groups)
    num_groups = len(unique_groups)
    indices_by_group = {g: np.where(groups == g)[0] for g in unique_groups}
    
    # æ‰“ä¹±ç»„çš„é¡ºåº
    np.random.shuffle(unique_groups)
    
    for i in range(0, num_groups, batch_size):
        batch_groups = unique_groups[i : i + batch_size]
        if len(batch_groups) < batch_size: continue # ä¸¢å¼ƒæœ€åä¸è¶³çš„ä¸€ä¸ªbatch
        
        batch_X = []
        batch_y = []
        
        for g in batch_groups:
            indices = indices_by_group[g]
            # å¦‚æœè¯¥ç»„åˆ‡ç‰‡ä¸å¤Ÿï¼Œå…è®¸é‡å¤é‡‡æ ·ï¼›å¦‚æœå¤Ÿï¼Œä¸é‡å¤
            replace = len(indices) < samples_per_group
            chosen_idx = np.random.choice(indices, samples_per_group, replace=replace)
            
            batch_X.append(X[chosen_idx])
            # å‡è®¾åŒä¸€ç»„çš„æ ‡ç­¾æ˜¯ä¸€æ ·çš„ï¼Œå–ç¬¬ä¸€ä¸ªå³å¯
            batch_y.append(y[indices[0]])
            
        yield np.array(batch_X), np.array(batch_y)

def train_with_voting_mechanism(model, X_train, y_train, groups_train, 
                                X_test, y_test, 
                                epochs, batch_size, 
                                samples_per_group, vote_weight, 
                                st_progress_bar, st_status_text):
    """
    è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼šå¼•å…¥æŠ•ç¥¨ä¸€è‡´æ€§ Loss
    """
    # ä¼˜åŒ–å™¨ä¸æŸå¤±å‡½æ•°
    optimizer = tf.keras.optimizers.Adam()
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()
    
    # è®°å½•å™¨
    train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()
    val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()
    
    history = {'accuracy': [], 'loss': [], 'val_accuracy': [], 'val_loss': []}
    
    start_time = time.time()
    st_progress_bar.progress(0)
    st_status_text.text("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æŠ•ç¥¨è®­ç»ƒæœºåˆ¶...")

    # é¢„å¤„ç†éªŒè¯é›† (ä¸éœ€è¦åˆ†ç»„ï¼ŒæŒ‰æ ‡å‡†æ–¹å¼è¯„ä¼°)
    # ä½¿ç”¨ tf.data æå‡æ€§èƒ½
    val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size * samples_per_group)

    for epoch in range(epochs):
        epoch_loss_avg = tf.keras.metrics.Mean()
        
        # è·å–åˆ†ç»„æ•°æ®ç”Ÿæˆå™¨
        data_gen = group_batch_generator(X_train, y_train, groups_train, batch_size, samples_per_group)
        
        # --- è®­ç»ƒæ­¥ ---
        for step, (x_batch_groups, y_batch) in enumerate(data_gen):
            # x_batch_groups shape: (B, N, T, F)
            # y_batch shape: (B,)
            
            B, N, T, F = x_batch_groups.shape
            
            # å±•å¹³è¾“å…¥ä»¥å–‚ç»™æ¨¡å‹: (B*N, T, F)
            x_flat = tf.reshape(x_batch_groups, (B * N, T, F))
            # æ‰©å±•æ ‡ç­¾: (B,) -> (B*N,)
            y_flat = np.repeat(y_batch, N)
            
            with tf.GradientTape() as tape:
                # 1. å‰å‘ä¼ æ’­ (å¾—åˆ° Logits æˆ– Softmaxï¼Œå‡è®¾æ¨¡å‹æœ€åä¸€å±‚æ˜¯ Softmax)
                logits_flat = model(x_flat, training=True) # (B*N, Classes)
                
                # 2. è®¡ç®— Instance Loss (æ ‡å‡†åˆ‡ç‰‡çº§ Loss)
                loss_instance = loss_fn(y_flat, logits_flat)
                
                # 3. è®¡ç®— Voting Loss (ç»„çº§ Loss)
                # å˜å› (B, N, Classes)
                logits_grouped = tf.reshape(logits_flat, (B, N, -1))
                
                # æ ¸å¿ƒï¼šè®¡ç®—è¯¥ç»„çš„å¹³å‡æ¦‚ç‡åˆ†å¸ƒ (Soft Voting)
                # è¿™ä¸€æ­¥å¼ºè¿«æ¨¡å‹å­¦ä¼šï¼šå“ªæ€•å•å¼ åˆ‡ç‰‡ä¸å‡†ï¼Œå¹³å‡ä¸‹æ¥å¿…é¡»å‡†
                avg_preds = tf.reduce_mean(logits_grouped, axis=1) # (B, Classes)
                
                loss_vote = loss_fn(y_batch, avg_preds)
                
                # 4. æ··åˆ Loss
                total_loss = (1.0 - vote_weight) * loss_instance + vote_weight * loss_vote

            # åå‘ä¼ æ’­
            grads = tape.gradient(total_loss, model.trainable_weights)
            optimizer.apply_gradients(zip(grads, model.trainable_weights))
            
            # è®°å½•æŒ‡æ ‡
            epoch_loss_avg.update_state(total_loss)
            train_acc_metric.update_state(y_flat, logits_flat)
            
        # --- éªŒè¯æ­¥ ---
        for x_val, y_val in val_dataset:
            val_logits = model(x_val, training=False)
            val_acc_metric.update_state(y_val, val_logits)
            # è®¡ç®— val_loss (è¿™é‡Œåªç®—æ ‡å‡†çš„)
            v_loss = loss_fn(y_val, val_logits)

        # --- æ”¶é›† Epoch ç»“æœ ---
        train_acc = train_acc_metric.result()
        val_acc = val_acc_metric.result()
        curr_loss = epoch_loss_avg.result()
        
        history['accuracy'].append(float(train_acc))
        history['loss'].append(float(curr_loss))
        history['val_accuracy'].append(float(val_acc))
        history['val_loss'].append(float(v_loss)) # è¿‘ä¼¼å€¼
        
        # é‡ç½®çŠ¶æ€
        train_acc_metric.reset_state()
        val_acc_metric.reset_state()
        
        # --- æ›´æ–° UI ---
        progress = (epoch + 1) / epochs
        st_progress_bar.progress(progress)
        
        elapsed = time.time() - start_time
        st_status_text.text(f"Epoch {epoch+1}/{epochs} | Loss: {curr_loss:.4f} (VoteWt: {vote_weight}) | Train Acc: {train_acc:.1%} | Val Acc: {val_acc:.1%}")
        
    st_status_text.text("âœ… æŠ•ç¥¨å¢å¼ºè®­ç»ƒå®Œæˆï¼")
    return history
