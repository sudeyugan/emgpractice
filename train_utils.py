import time
import os
import numpy as np
import tensorflow as tf
import streamlit as st
from tensorflow.keras.callbacks import Callback
from sklearn.model_selection import GroupShuffleSplit


# ================= åŸæœ‰åŠŸèƒ½ä¿æŒä¸å˜ =================

class StreamlitKerasCallback(Callback):
    """
    ç”¨äºè¿æ¥ Keras è®­ç»ƒè¿‡ç¨‹ä¸ Streamlit è¿›åº¦æ¡çš„è‡ªå®šä¹‰å›è°ƒ
    """
    def __init__(self, total_epochs, progress_bar, status_text):
        super().__init__()
        self.total_epochs = total_epochs
        self.progress_bar = progress_bar
        self.status_text = status_text
        self.start_time = None

    def on_train_begin(self, logs=None):
        self.start_time = time.time()
        self.progress_bar.progress(0)
        self.status_text.text("ğŸš€ å‡†å¤‡å¼€å§‹è®­ç»ƒ...")

    def on_epoch_end(self, epoch, logs=None):
        current_epoch = epoch + 1
        progress = min(current_epoch / self.total_epochs, 1.0)
        self.progress_bar.progress(progress)
        
        elapsed_time = time.time() - self.start_time
        avg_time_per_epoch = elapsed_time / current_epoch
        remaining_epochs = self.total_epochs - current_epoch
        eta_seconds = avg_time_per_epoch * remaining_epochs
        
        eta_str = time.strftime("%M:%S", time.gmtime(eta_seconds))
        
        loss = logs.get('loss', 0)
        acc = logs.get('accuracy', 0)
        val_loss = logs.get('val_loss', 0)
        val_acc = logs.get('val_accuracy', 0)
        
        status_msg = (
            f"Epoch {current_epoch}/{self.total_epochs} | "
            f"â³ å‰©ä½™: {eta_str} | "
            f"Loss: {loss:.4f} Acc: {acc:.1%} | "
            f"Val Loss: {val_loss:.4f} Val Acc: {val_acc:.1%}"
        )
        self.status_text.text(status_msg)

    def on_train_end(self, logs=None):
        self.progress_bar.progress(100)
        self.status_text.text("âœ… è®­ç»ƒå·²å®Œæˆï¼")

def smart_split(X, y, groups, strategy, test_size=0.2, manual_target=None):
    """
    æ ¹æ®ä¸åŒç­–ç•¥åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    """
    indices = np.arange(len(X))
    train_idx, test_idx = [], []
    
    unique_files = np.unique(groups)
    
    # --- ç­–ç•¥ 1: æ··åˆåˆ‡åˆ† ---
    if strategy == "æ··åˆåˆ‡åˆ† (çœ‹åˆ°æ‰€æœ‰å¤©/äºº)":
        for f in unique_files:
            f_indices = indices[groups == f]
            split_point = int(len(f_indices) * (1 - test_size))
            train_idx.extend(f_indices[:split_point])
            test_idx.extend(f_indices[split_point:])
            
    # --- ç­–ç•¥ 2: ç•™æ–‡ä»¶éªŒè¯ ---
    elif strategy == "ç•™æ–‡ä»¶éªŒè¯ (åŒå¤©/åŒäºº)":
        if manual_target:
            is_test = np.array([os.path.basename(g.split('_seg')[0]) == manual_target for g in groups])
            test_idx = indices[is_test]
            train_idx = indices[~is_test]
        else:
            gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=42)
            train_i, test_i = next(gss.split(X, y, groups=groups))
            train_idx, test_idx = indices[train_i], indices[test_i]

    # --- ç­–ç•¥ 3: ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯ ---
    elif strategy == "ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯ (æ³›åŒ–èƒ½åŠ›)":
        real_groups = np.array([os.path.basename(os.path.dirname(f)) for f in groups])
        
        if manual_target:
            is_test = (real_groups == manual_target)
            test_idx = indices[is_test]
            train_idx = indices[~is_test]
        else:
            gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=42)
            train_i, test_i = next(gss.split(X, y, groups=real_groups))
            train_idx, test_idx = indices[train_i], indices[test_i]
        
    return np.array(train_idx), np.array(test_idx)

def get_few_shot_split(X, y, n_samples_per_class):
    train_idx = []
    test_idx = []
    unique_labels = np.unique(y)
    
    for label in unique_labels:
        label_indices = np.where(y == label)[0]
        np.random.shuffle(label_indices)
        train_idx.extend(label_indices[:n_samples_per_class])
        test_idx.extend(label_indices[n_samples_per_class:])
        
    return np.array(train_idx), np.array(test_idx)

def mixup(x, y, alpha=0.2):
    """
    å¯¹ batch æ•°æ®è¿›è¡Œ Mixup å¢å¼º
    x: (Batch, Time, Feat)
    y: (Batch, Classes) -> å¿…é¡»æ˜¯ One-Hot ç¼–ç 
    """
    if alpha <= 0: return x, y
    
    batch_size = tf.shape(x)[0]
    
    # ç”Ÿæˆ Mixup ç³»æ•° lambda (Betaåˆ†å¸ƒ)
    # tf.random.gamma ç”¨äºç”Ÿæˆ Beta åˆ†å¸ƒ
    weight = tf.random.gamma([batch_size], alpha, 1.0)
    beta = tf.random.gamma([batch_size], alpha, 1.0)
    lam = weight / (weight + beta)
    lam = tf.reshape(lam, [batch_size, 1, 1]) # å¹¿æ’­ç»´åº¦
    
    # æ‰“ä¹±æ•°æ®é¡ºåº
    indices = tf.range(batch_size)
    shuffled_indices = tf.random.shuffle(indices)
    
    x_shuffled = tf.gather(x, shuffled_indices)
    y_shuffled = tf.gather(y, shuffled_indices)
    
    # æ··åˆ
    x_mix = x * lam + x_shuffled * (1 - lam)
    
    # æ ‡ç­¾æ··åˆ (lam ç»´åº¦è°ƒæ•´ä¸º [batch, 1])
    lam_y = tf.reshape(lam, [batch_size, 1])
    y_mix = y * lam_y + y_shuffled * (1 - lam_y)
    
    return x_mix, y_mix

# ================= æ–°å¢ï¼šæŠ•ç¥¨è®­ç»ƒæ”¯æŒå‡½æ•° =================

def group_batch_generator(X, y, groups, batch_size, samples_per_group=5):
    """
    ç”Ÿæˆå™¨ï¼šæ¯æ¬¡äº§å‡ºä¸€ä¸ª Batchï¼Œå…¶ä¸­åŒ…å« `batch_size` ä¸ªç»„ï¼ˆSegmentï¼‰ã€‚
    æ¯ä¸ªç»„åŒ…å« `samples_per_group` ä¸ªåˆ‡ç‰‡ã€‚
    è¿”å›å½¢çŠ¶: (batch_size, samples_per_group, time_steps, features)
    """
    unique_groups = np.unique(groups)
    num_groups = len(unique_groups)
    indices_by_group = {g: np.where(groups == g)[0] for g in unique_groups}
    
    # æ‰“ä¹±ç»„çš„é¡ºåº
    np.random.shuffle(unique_groups)
    
    for i in range(0, num_groups, batch_size):
        batch_groups = unique_groups[i : i + batch_size]
        if len(batch_groups) < batch_size: continue # ä¸¢å¼ƒæœ€åä¸è¶³çš„ä¸€ä¸ªbatch
        
        batch_X = []
        batch_y = []
        
        for g in batch_groups:
            indices = indices_by_group[g]
            # å¦‚æœè¯¥ç»„åˆ‡ç‰‡ä¸å¤Ÿï¼Œå…è®¸é‡å¤é‡‡æ ·ï¼›å¦‚æœå¤Ÿï¼Œä¸é‡å¤
            replace = len(indices) < samples_per_group
            chosen_idx = np.random.choice(indices, samples_per_group, replace=replace)
            
            batch_X.append(X[chosen_idx])
            # å‡è®¾åŒä¸€ç»„çš„æ ‡ç­¾æ˜¯ä¸€æ ·çš„ï¼Œå–ç¬¬ä¸€ä¸ªå³å¯
            batch_y.append(y[indices[0]])
            
        yield np.array(batch_X), np.array(batch_y)

def train_with_voting_mechanism(model, X_train, y_train, groups_train, 
                                X_test, y_test, 
                                epochs, batch_size, 
                                samples_per_group, vote_weight, 
                                st_progress_bar, st_status_text,
                                use_mixup=False, 
                                label_smoothing=0.0):
    
    # 1. ç¡®å®š Loss å‡½æ•°
    # å¦‚æœç”¨ Mixup æˆ– Smoothingï¼Œå¿…é¡»ç”¨ CategoricalCrossentropy (æ”¯æŒè½¯æ ‡ç­¾)
    loss_fn = tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing)
    optimizer = tf.keras.optimizers.Adam()

    # 2. å‡†å¤‡ Metrics
    train_acc_metric = tf.keras.metrics.CategoricalAccuracy()
    val_acc_metric = tf.keras.metrics.CategoricalAccuracy()
    
    history = {'accuracy': [], 'loss': [], 'val_accuracy': [], 'val_loss': []}
    
    # è·å–ç±»åˆ«æ•°ï¼Œç”¨äº One-Hot è½¬æ¢
    num_classes = y_train.max() + 1 
    
    # éªŒè¯é›†é¢„å¤„ç† (è½¬ One-Hot)
    y_test_onehot = tf.one_hot(y_test, depth=num_classes)
    val_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test_onehot)).batch(batch_size * samples_per_group)

    start_time = time.time()
    
    for epoch in range(epochs):
        epoch_loss_avg = tf.keras.metrics.Mean()
        
        # è·å–åˆ†ç»„æ•°æ®ç”Ÿæˆå™¨
        data_gen = group_batch_generator(X_train, y_train, groups_train, batch_size, samples_per_group)
        
        for step, (x_batch_groups, y_batch) in enumerate(data_gen):
            # x_batch_groups: (B, N, T, F)
            # y_batch: (B,) åŸå§‹æ•´æ•°æ ‡ç­¾
            
            B, N, T, F = x_batch_groups.shape
            
            # å±•å¹³è¾“å…¥: (B*N, T, F)
            x_flat = tf.reshape(x_batch_groups, (B * N, T, F))
            # æ‰©å±•æ ‡ç­¾å¹¶è½¬ One-Hot: (B,) -> (B*N,) -> (B*N, Classes)
            y_flat_int = np.repeat(y_batch, N)
            y_flat_onehot = tf.one_hot(y_flat_int, depth=num_classes)
            
            # [NEW] åº”ç”¨ Mixup
            if use_mixup:
                # Mixup ä¼šæ”¹å˜ x_flat å’Œ y_flat_onehot çš„å€¼
                x_flat, y_flat_onehot = mixup(x_flat, y_flat_onehot, alpha=0.2)
            
            with tf.GradientTape() as tape:
                # å‰å‘ä¼ æ’­
                logits_flat = model(x_flat, training=True) # (B*N, Classes)
                
                # 1. Instance Loss
                loss_instance = loss_fn(y_flat_onehot, logits_flat)
                
                # 2. Voting Loss (ç»„çº§ Loss)
                # å˜å› (B, N, Classes)
                logits_grouped = tf.reshape(logits_flat, (B, N, -1))
                avg_preds = tf.reduce_mean(logits_grouped, axis=1) # (B, Classes)
                
                # Voting Loss çš„ç›®æ ‡æ˜¯çœŸå®çš„ y_batch (è½¬One-Hot)
                y_batch_onehot = tf.one_hot(y_batch, depth=num_classes)
                
                # æ³¨æ„ï¼šå¦‚æœ Mixup å¼€å¯äº†ï¼Œè¿™é‡Œ Voting Loss æ¯”è¾ƒéš¾å®šä¹‰ï¼Œ
                # å› ä¸ºç»„å†…çš„æ¯ä¸ªæ ·æœ¬å¯èƒ½æ··äº†ä¸åŒçš„ç±»ã€‚
                # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬è§„å®šï¼šMixupåªå½±å“ Instance Lossï¼ŒVoting Loss ä¾ç„¶å¯¹é½åŸå§‹æ ‡ç­¾ã€‚
                # ä½†è¿™è¦æ±‚ logits ä¹Ÿæ˜¯æœªæ··åˆçš„é¢„æµ‹ã€‚
                # å¦¥åæ–¹æ¡ˆï¼šå¦‚æœ Mixup å¼€å¯ï¼Œæš‚æ—¶é™ä½ Voting Weight æˆ–è€…åªè®¡ç®— Instance Lossã€‚
                # è¿™é‡Œä¸ºäº†ä»£ç ç®€æ´ï¼Œæˆ‘ä»¬å‡å®š Mixup æ—¶æ¨¡å‹è¾“å‡ºçš„æ˜¯æ··åˆé¢„æµ‹ï¼Œ
                # å¼ºè¡Œè·ŸåŸå§‹æ ‡ç­¾ç®— Loss ä¼šæœ‰åå·®ï¼Œä½†ä¹Ÿèƒ½è®­ç»ƒã€‚
                # æ›´ä¸¥è°¨çš„åšæ³•æ˜¯å¯¹ y_batch ä¹ŸåšåŒæ ·çš„ Mixup (å¾ˆéš¾å®ç°å› ä¸º shuffle æ˜¯éšæœºçš„)ã€‚
                # **å®ç”¨æ–¹æ¡ˆ**ï¼šMixup æ—¶ï¼Œavg_preds ä¹Ÿæ˜¯æ··åˆçš„ï¼Œæˆ‘ä»¬è®©å®ƒé€¼è¿‘ y_batch_onehot (æœªæ··åˆ) 
                # è¿™å…¶å®èµ·åˆ°äº†æ­£åˆ™åŒ–ä½œç”¨ã€‚
                
                loss_vote = loss_fn(y_batch_onehot, avg_preds)
                
                total_loss = (1.0 - vote_weight) * loss_instance + vote_weight * loss_vote

            grads = tape.gradient(total_loss, model.trainable_weights)
            optimizer.apply_gradients(zip(grads, model.trainable_weights))
            
            epoch_loss_avg.update_state(total_loss)
            train_acc_metric.update_state(y_flat_onehot, logits_flat)
            
        # éªŒè¯æ­¥
        for x_val, y_val in val_dataset:
            val_logits = model(x_val, training=False)
            val_acc_metric.update_state(y_val, val_logits)
            # è®°å½• val_loss
            loss_val = loss_fn(y_val, val_logits)

        # è®°å½•
        train_acc = train_acc_metric.result()
        val_acc = val_acc_metric.result()
        curr_loss = epoch_loss_avg.result()
        
        history['accuracy'].append(float(train_acc))
        history['loss'].append(float(curr_loss))
        history['val_accuracy'].append(float(val_acc))
        history['val_loss'].append(float(loss_val))
        
        train_acc_metric.reset_state()
        val_acc_metric.reset_state()
        
        progress = (epoch + 1) / epochs
        st_progress_bar.progress(progress)
        st_status_text.text(f"Epoch {epoch+1}/{epochs} | Loss: {curr_loss:.4f} | Train Acc: {train_acc:.1%} | Val Acc: {val_acc:.1%}")
        
    return history