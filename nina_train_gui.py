import streamlit as st
import os
import time
import numpy as np
import pandas as pd
import scipy.io as sio
import scipy.ndimage as ndimage
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import datetime
import train_utils


# ================= 0. é…ç½®ä¸å·¥å…·å‡½æ•° =================
st.set_page_config(layout="wide", page_title="NinaPro å¾®è°ƒå·¥ä½œç«™")

# é˜²æ­¢ GPU OOM
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError:
        pass

# åˆå§‹åŒ– Session State
if 'trained_model' not in st.session_state:
    st.session_state['trained_model'] = None
if 'train_history' not in st.session_state:
    st.session_state['train_history'] = None

# ================= 1. æ ¸å¿ƒï¼šç§»æ¤è‡ª nina_auto_train.py çš„æ•°æ®å¤„ç† =================
def process_nina_data(data_root, selected_subjects, target_labels, 
                      stride_ms=50, split_strategy="mixed", 
                      progress_callback=None):
    """
    å®Œå…¨å¤åˆ» nina_auto_train.py çš„æ•°æ®å¤„ç†é€»è¾‘ 
    """
    X_list = []
    y_list = []
    groups_list = []
    
    total_files = len(selected_subjects)
    
    # ç¡¬ç¼–ç é…ç½® (ä¿æŒä¸ nina_auto_train ä¸€è‡´)
    # nina_auto_train ä¸­ window æ˜¯ Center Â± 150ï¼Œå³ 300 ç‚¹
    WINDOW_RADIUS = 150 
    WINDOW_SIZE = WINDOW_RADIUS * 2
    
    for idx, subject_name in enumerate(selected_subjects):
        subj_upper = subject_name.upper()
        
        # å¢å¼ºçš„æ–‡ä»¶åæœç´¢
        possible_filenames = [
            f"{subj_upper}_A1_E2.mat",   # S1_A1_E2.mat
            f"{subject_name}_A1_E2.mat", # s1_A1_E2.mat
            f"{subj_upper}_E2_A1.mat"    # S1_E2_A1.mat
        ]
        
        mat_file = None
        folder_path = os.path.join(data_root, subject_name)
        
        for fname in possible_filenames:
            full_path = os.path.join(folder_path, fname)
            if os.path.exists(full_path):
                mat_file = full_path
                break
        
        if progress_callback:
            progress_callback((idx / total_files), f"æ­£åœ¨å¤„ç†: {subject_name}")

        if not mat_file:
            print(f"âš ï¸ è·³è¿‡: æ‰¾ä¸åˆ° {subject_name} çš„ .mat æ–‡ä»¶")
            continue

        try:
            # === è¯»å– MAT æ–‡ä»¶ ===
            mat_data = sio.loadmat(mat_file)
            
            # 1. è·å– EMG (å–å‰8åˆ—ï¼Œå½’ä¸€åŒ–)
            if 'emg' in mat_data:
                raw_emg = mat_data['emg']
            else:
                keys = [k for k in mat_data.keys() if 'emg' in k.lower()]
                if keys: raw_emg = mat_data[keys[0]]
                else: continue
            
            raw_emg = raw_emg[:, :8]
            emg_data = (raw_emg / 0.0024) - 1 # å½’ä¸€åŒ–
            
            # 2. è·å–æ ‡ç­¾
            if 'restimulus' in mat_data:
                stimulus = mat_data['restimulus'].flatten()
            elif 'stimulus' in mat_data:
                stimulus = mat_data['stimulus'].flatten()
            else:
                continue

            # === åˆ‡ç‰‡é€»è¾‘ ===
            labeled_array, num_features = ndimage.label(stimulus > 0)
            
            subj_act_X, subj_act_y, subj_act_groups = [], [], []
            subj_rest_X, subj_rest_y, subj_rest_groups = [], [], []
            
            # --- A. æå–åŠ¨ä½œæ ·æœ¬ ---
            for i in range(1, num_features + 1):
                indices = np.where(labeled_array == i)[0]
                current_label = int(np.median(stimulus[indices]))
                
                if current_label not in target_labels:
                    continue
                
                center_idx = int((indices[0] + indices[-1]) / 2)
                start_win = center_idx - WINDOW_RADIUS
                end_win = center_idx + WINDOW_RADIUS
                
                if start_win < 0 or end_win > len(emg_data):
                    continue
                
                window = emg_data[start_win:end_win]
                
                if window.shape[0] == WINDOW_SIZE:
                    subj_act_X.append(window)
                    subj_act_y.append(current_label)
                    subj_act_groups.append(f"{subject_name}_act_{i}")

            # --- B. æå–é™æ¯æ ·æœ¬ (Rest) 
            if 0 in target_labels or (0 in [0]): # å¼ºåˆ¶æ£€æŸ¥æ˜¯å¦éœ€è¦æå– Restï¼Œé€šå¸¸ç”¨æˆ·è™½ç„¶æ²¡é€‰0ä½†ä»£ç é€»è¾‘å¯èƒ½éœ€è¦
                # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ˜¾å¼é€‰æ‹©äº† Label 0ï¼Œæˆ–è€…æˆ‘ä»¬ä½œä¸ºé»˜è®¤å¢å¼ºç­–ç•¥æ·»åŠ 
                # è¿™é‡Œå‡è®¾åªè¦ target_labels é‡Œæœ‰ 0 å°±æå–
                pass
            
            if 0 in target_labels:
                # 1. è†¨èƒ€åŠ¨ä½œåŒºåŸŸ (ä½œä¸º Buffer)ï¼Œé¿å¼€åŠ¨ä½œè¾¹ç¼˜
                buffer_size = 100
                mask_active = stimulus > 0
                mask_forbidden = ndimage.binary_dilation(mask_active, structure=np.ones(buffer_size))
                mask_rest = ~mask_forbidden # å–åï¼Œå¾—åˆ°çº¯å‡€é™æ¯åŒº
                
                labeled_rest, num_rest = ndimage.label(mask_rest)
                
                for i in range(1, num_rest + 1):
                    r_indices = np.where(labeled_rest == i)[0]
                    # å¦‚æœè¿™æ®µé™æ¯å¤ªçŸ­ (å°äº 300)ï¼Œå°±è·³è¿‡
                    if len(r_indices) < 300: continue
                    
                    # åªå–è¿™æ®µé™æ¯çš„æœ€ä¸­é—´ä¸€æ®µ
                    center_idx = int((r_indices[0] + r_indices[-1]) / 2)
                    start_win = center_idx - WINDOW_RADIUS
                    end_win = center_idx + WINDOW_RADIUS
                    
                    if start_win < 0 or end_win > len(emg_data): continue

                    window = emg_data[start_win:end_win]
                    
                    if window.shape[0] == WINDOW_SIZE:
                        subj_rest_X.append(window)
                        subj_rest_y.append(0) # Label 0
                        subj_rest_groups.append(f"{subject_name}_rest_{i}")

            # --- C. åˆå¹¶ä¸å¹³è¡¡ (Balancing) ---
            if len(subj_act_X) > 0:
                # 1. åŠ¨ä½œæ•°æ®ç›´æ¥åŠ å…¥
                X_list.extend(subj_act_X)
                y_list.extend(subj_act_y)
                groups_list.extend(subj_act_groups)

                # 2. è®¡ç®—åˆé€‚çš„é™æ¯æ•°é‡ (1:1 å¹³è¡¡ç­–ç•¥)
                if len(subj_rest_X) > 0:
                    # ç­–ç•¥ï¼šè®©é™æ¯æ€»æ•° â‰ˆ å•ä¸ªåŠ¨ä½œç±»åˆ«çš„å¹³å‡æ•°é‡
                    # è¿™æ ·é™æ¯ç±»ä¸ä¼šå› ä¸ºåœ¨è¿™ä¸ªå—è¯•è€…èº«ä¸Šæ—¶é—´å¾ˆé•¿è€Œç”±äºæ•°æ®é‡è¿‡å¤§ä¸»å¯¼ Loss
                    unique_act_classes = np.unique(subj_act_y)
                    num_act_classes_found = len(unique_act_classes)
                    
                    if num_act_classes_found > 0:
                        target_rest_count = int(len(subj_act_X) / num_act_classes_found)
                    else:
                        target_rest_count = len(subj_rest_X) # å…¨æ˜¯é™æ¯çš„æƒ…å†µ
                    
                    # 3. éšæœºé‡‡æ ·
                    if len(subj_rest_X) > target_rest_count and target_rest_count > 0:
                        selected_indices = np.random.choice(len(subj_rest_X), target_rest_count, replace=False)
                        for s_idx in selected_indices:
                            X_list.append(subj_rest_X[s_idx])
                            y_list.append(subj_rest_y[s_idx])
                            groups_list.append(subj_rest_groups[s_idx])
                    else:
                        # ä¸å¤Ÿæˆ–è€…åˆšå¥½ï¼Œå°±å…¨éƒ½è¦
                        X_list.extend(subj_rest_X)
                        y_list.extend(subj_rest_y)
                        groups_list.extend(subj_rest_groups)
            elif len(subj_rest_X) > 0:
                # åªæœ‰é™æ¯æ•°æ®çš„æƒ…å†µ
                X_list.extend(subj_rest_X)
                y_list.extend(subj_rest_y)
                groups_list.extend(subj_rest_groups)

        except Exception as e:
            print(f"âŒ å¤„ç†å‡ºé”™ {mat_file}: {e}")
            
    return np.array(X_list), np.array(y_list), np.array(groups_list)

# ================= 2. ä¾§è¾¹æ ï¼šé…ç½® =================
st.sidebar.title("ğŸ› ï¸ NinaPro å¾®è°ƒé…ç½®")

# --- A. æ¨¡å‹ ---
st.sidebar.header("1. åŸºæ¨¡å‹ (Base Model)")
base_model_file = st.sidebar.file_uploader(
    "ä¸Šä¼  nina_auto_train ç”Ÿæˆçš„ .keras/.h5", 
    type=["keras", "h5"]
)

# --- B. æ•°æ®æº ---
st.sidebar.header("2. æ•°æ®æº (NinaPro Data)")
# è®©ç”¨æˆ·è¾“å…¥æ•°æ®æ ¹ç›®å½•ï¼Œä¸å†æ‰«æ CSV
data_root_input = st.sidebar.text_input("æ•°æ®æ ¹ç›®å½• (åŒ…å« s1, s2...)", value="data")

# æ‰‹åŠ¨è¾“å…¥æˆ–æ‰«æ Subject
all_subjects = []
if os.path.exists(data_root_input):
    # ç®€å•çš„æ‰«æï¼šæŸ¥æ‰¾ s1, s2... æ–‡ä»¶å¤¹
    try:
        items = os.listdir(data_root_input)
        all_subjects = sorted([d for d in items if os.path.isdir(os.path.join(data_root_input, d)) and d.startswith('s')])
    except:
        pass

if not all_subjects:
    st.sidebar.warning("æœªæ£€æµ‹åˆ° 's*' æ–‡ä»¶å¤¹ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥è·¯å¾„")
    # å…è®¸æ‰‹åŠ¨è¾“å…¥ fallback
    manual_subjs = st.sidebar.text_input("æˆ–æ‰‹åŠ¨è¾“å…¥ Subject (é€—å·åˆ†éš”)", "s1, s2")
    if manual_subjs:
        selected_subjects = [s.strip() for s in manual_subjs.split(',')]
else:
    selected_subjects = st.sidebar.multiselect("é€‰æ‹© Subject è¿›è¡Œå¾®è°ƒ", all_subjects, default=all_subjects[:1])

# é€‰æ‹© Labels
# nina_auto_train é»˜è®¤æ˜¯ [1, 2, 5, 6]ï¼Œè¿™é‡Œå…è®¸ç”¨æˆ·æ”¹
target_labels_str = st.sidebar.text_input("ç›®æ ‡åŠ¨ä½œ ID (é€—å·åˆ†éš”)", "1, 2, 5, 6")
try:
    target_labels = [int(x.strip()) for x in target_labels_str.split(',') if x.strip()]
except:
    st.sidebar.error("Label æ ¼å¼é”™è¯¯")
    target_labels = []

# --- C. å¾®è°ƒå‚æ•° ---
st.sidebar.header("3. è®­ç»ƒå‚æ•°")
fine_tune_mode = st.sidebar.radio("å¾®è°ƒæ¨¡å¼", ["Few-shot (å†»ç»“ç‰¹å¾)", "Full Fine-tune (å…¨é‡å¾®è°ƒ)"], index=0)
unfreeze_all = (fine_tune_mode == "Full Fine-tune (å…¨é‡å¾®è°ƒ)")

epochs = st.sidebar.number_input("Epochs", 10, 200, 30)
batch_size = st.sidebar.selectbox("Batch Size", [16, 32, 64, 128], index=1)
lr = st.sidebar.number_input("Learning Rate", value=0.001, format="%.5f")
num_shots = st.sidebar.slider("æ¯ç±»æ ·æœ¬æ•° (Few-shotç”¨)", 1, 10, 2) if not unfreeze_all else 9999

run_btn = st.sidebar.button("ğŸš€ å¼€å§‹å¾®è°ƒ", type="primary")

# ================= 3. ä¸»ç•Œé¢é€»è¾‘ =================
st.title("ğŸ§  NinaPro æ¨¡å‹å¾®è°ƒ (MATç‰ˆ)")
st.caption("åŸºäº nina_auto_train.py æ ¸å¿ƒé€»è¾‘")

if run_btn:
    if not base_model_file:
        st.error("è¯·ä¸Šä¼ åŸºæ¨¡å‹æ–‡ä»¶ï¼")
        st.stop()
    if not selected_subjects:
        st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ª Subjectï¼")
        st.stop()
        
    # --- Step 1: åŠ è½½æ•°æ® ---
    st.subheader("1. è¯»å– MAT æ•°æ®")
    bar = st.progress(0)
    status = st.empty()
    
    X, y, groups = process_nina_data(
        data_root_input, 
        selected_subjects, 
        target_labels,
        progress_callback=lambda p, t: (bar.progress(p), status.text(t))
    )
    
    bar.progress(100)
    if len(X) == 0:
        st.error(f"æœªæå–åˆ°ä»»ä½•æ ·æœ¬ï¼è¯·æ£€æŸ¥ {data_root_input} ä¸‹æ˜¯å¦æœ‰ .mat æ–‡ä»¶ï¼Œä»¥åŠ Label æ˜¯å¦å­˜åœ¨ã€‚")
        st.stop()
        
    X = X.astype(np.float32)
    # æ ‡ç­¾é‡æ˜ å°„ (ä¾‹å¦‚æŠŠ 1,2,5,6 -> 0,1,2,3)
    unique_labels = np.unique(y)
    label_map = {original: new for new, original in enumerate(unique_labels)}
    y_mapped = np.array([label_map[i] for i in y])
    num_classes = len(unique_labels)
    
    st.success(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: X={X.shape}, y={y.shape} | åŒ…å«åŠ¨ä½œ: {unique_labels}")
    
    # --- Step 2: åˆ’åˆ†æ•°æ®é›† ---
    if unfreeze_all:
        # å…¨é‡å¾®è°ƒï¼šä½¿ç”¨ç•™æ–‡ä»¶éªŒè¯ (å¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶) æˆ– ç®€å•åˆ‡åˆ†
        # è¿™é‡Œå› ä¸º nina_auto_train é€šå¸¸æ¯ä¸ª subject åªæœ‰ä¸€ä¸ªæ–‡ä»¶ (E2)ï¼Œ
        # å¦‚æœåªé€‰äº†ä¸€ä¸ª subjectï¼Œåªèƒ½éšæœºåˆ‡åˆ†ï¼›é€‰äº†å¤šä¸ªå¯ä»¥ç•™ä¸€ä¸ªåšéªŒè¯ã€‚
        if len(selected_subjects) > 1:
            # ç®€å•ç­–ç•¥ï¼šç•™æœ€åä¸€ä¸ª subject åšéªŒè¯
            test_mask = np.char.startswith(groups, selected_subjects[-1])
            train_idx = np.where(~test_mask)[0]
            test_idx = np.where(test_mask)[0]
            st.info(f"éªŒè¯ç­–ç•¥: ä½¿ç”¨ {selected_subjects[-1]} ä½œä¸ºéªŒè¯é›†")
        else:
            # åªæœ‰ä¸€ä¸ª subjectï¼Œåªèƒ½éšæœºåˆ‡
            from sklearn.model_selection import train_test_split
            idx = np.arange(len(X))
            train_idx, test_idx = train_test_split(idx, test_size=0.2, random_state=42, stratify=y_mapped)
            st.info("éªŒè¯ç­–ç•¥: å• Subject å†…éƒ¨éšæœºåˆ’åˆ† (80/20)")
    else:
        # Few-shot: éšæœºæŠ½å– N ä¸ªæ ·æœ¬
        train_idx, test_idx = train_utils.get_few_shot_split(X, y_mapped, num_shots)
        st.info(f"éªŒè¯ç­–ç•¥: Few-shot (æ¯ç±» {num_shots} ä¸ªè®­ç»ƒæ ·æœ¬)")
        
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y_mapped[train_idx], y_mapped[test_idx]
    
    # --- Step 3: åŠ è½½ä¸é€‚é…æ¨¡å‹ ---
    st.subheader("2. æ¨¡å‹é€‚é…")
    
    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    temp_path = f"temp_{base_model_file.name}"
    with open(temp_path, "wb") as f: f.write(base_model_file.getbuffer())
    
    try:
        base_model = tf.keras.models.load_model(temp_path)
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        st.stop()
        
    # æ£€æŸ¥ç»´åº¦
    if base_model.input_shape[-1] != X.shape[-1]:
        st.error(f"âŒ ç»´åº¦ä¸åŒ¹é…: æ¨¡å‹è¾“å…¥é€šé“ {base_model.input_shape[-1]} vs æ•°æ®é€šé“ {X.shape[-1]}")
        st.stop()
        
    old_classes = base_model.output_shape[-1]
    
    # æ”¹é€ æ¨¡å‹
    if unfreeze_all:
        base_model.trainable = True
        if old_classes == num_classes:
            model = base_model
        else:
            st.warning(f"é‡ç½®åˆ†ç±»å¤´: {old_classes} -> {num_classes} ç±»")
            # å‰¥ç¦»æ—§å¤´ (å–å€’æ•°ç¬¬äºŒå±‚)
            feature_out = base_model.layers[-2].output
            new_out = tf.keras.layers.Dense(num_classes, activation='softmax')(feature_out)
            model = tf.keras.models.Model(base_model.input, new_out)
    else:
        base_model.trainable = False # å†»ç»“
        # å¯»æ‰¾ç‰¹å¾å±‚ (ç®€å•å¯å‘å¼ï¼šæ‰¾ GlobalAvgPool æˆ– Flatten)
        feature_layer = None
        for layer in reversed(base_model.layers):
            if "global" in layer.name or "flatten" in layer.name:
                feature_layer = layer
                break
        
        feat_out = feature_layer.output if feature_layer else base_model.layers[-2].output
        
        # åŠ æ–°çš„åˆ†ç±»å¤´
        x = tf.keras.layers.Dropout(0.5)(feat_out)
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)
        model = tf.keras.models.Model(base_model.input, outputs)
        
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    # --- Step 4: è®­ç»ƒ ---
    st.subheader("3. å¼€å§‹è®­ç»ƒ")
    t_prog = st.progress(0)
    t_status = st.empty()
    st_cb = train_utils.StreamlitKerasCallback(epochs, t_prog, t_status)
    
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[st_cb],
        verbose=0
    )
    
    # --- Step 5: ç»“æœ ---
    st.subheader("4. è¯„ä¼°æŠ¥å‘Š")
    final_acc = history.history['val_accuracy'][-1]
    st.metric("æœ€ç»ˆéªŒè¯é›†å‡†ç¡®ç‡", f"{final_acc:.2%}")
    
    # æ··æ·†çŸ©é˜µ
    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)
    cm = confusion_matrix(y_test, y_pred)
    
    c1, c2 = st.columns(2)
    with c1:
        fig, ax = plt.subplots()
        ax.plot(history.history['loss'], label='Train')
        ax.plot(history.history['val_loss'], label='Val')
        ax.legend()
        ax.set_title("Loss Curve")
        st.pyplot(fig)
    with c2:
        fig2, ax2 = plt.subplots()
        names = [str(k) for k in label_map.keys()]
        sns.heatmap(cm, annot=True, fmt='d', xticklabels=names, yticklabels=names, cmap='Blues', ax=ax2)
        ax2.set_title("Confusion Matrix")
        st.pyplot(fig2)
        
    # ä¿å­˜æ¨¡å‹
    st.markdown("---")
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M")
    save_name = st.text_input("ä¿å­˜æ¨¡å‹åç§°", f"finetuned_nina_{ts}.keras")
    if st.button("ğŸ’¾ ä¿å­˜å½“å‰æ¨¡å‹"):
        if not os.path.exists("trained_models"): os.makedirs("trained_models")
        path = os.path.join("trained_models", save_name)
        model.save(path)
        st.success(f"å·²ä¿å­˜è‡³ {path}")