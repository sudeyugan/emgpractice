import streamlit as st
import os
import time
import numpy as np
import pandas as pd
import scipy.io as sio
import scipy.ndimage as ndimage
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import datetime
import train_utils
import data_loader  # [NEW] å¼•å…¥æ•°æ®å¢å¼ºå·¥å…·åº“


# ================= 0. é…ç½®ä¸å·¥å…·å‡½æ•° =================
st.set_page_config(layout="wide", page_title="NinaPro å¾®è°ƒå·¥ä½œç«™")

# é˜²æ­¢ GPU OOM
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError:
        pass

# åˆå§‹åŒ– Session State
if 'trained_model' not in st.session_state:
    st.session_state['trained_model'] = None
if 'train_history' not in st.session_state:
    st.session_state['train_history'] = None

# [NEW] æ•°æ®é›†å¢å¼ºå‡½æ•° (Post-Split Augmentation)
def augment_dataset(X, y, groups, config, progress_bar=None):
    """
    å¯¹è®­ç»ƒé›†è¿›è¡Œå†…å­˜å†…å¢å¼º
    """
    multiplier = config.get('multiplier', 1)
    if multiplier <= 1:
        return X, y, groups
    
    X_aug, y_aug, groups_aug = [], [], []
    total = len(X)
    
    # æå–é…ç½®
    enable_warp = config.get('enable_warp', False)
    enable_shift = config.get('enable_shift', False)
    enable_scale = config.get('enable_scaling', False)
    enable_mask = config.get('enable_mask', False)
    enable_noise = config.get('enable_noise', False)
    
    for i in range(total):
        # 1. åŠ å…¥åŸå§‹æ ·æœ¬
        X_aug.append(X[i])
        y_aug.append(y[i])
        groups_aug.append(groups[i])
        
        # 2. ç”Ÿæˆå¢å¼ºæ ·æœ¬
        for _ in range(multiplier - 1):
            aug_x = X[i].copy()
            
            # æŒ‰æ¦‚ç‡åº”ç”¨å„ç§å¢å¼º
            if enable_warp and np.random.random() > 0.5:
                aug_x = data_loader.time_warp(aug_x)
            
            if enable_shift and np.random.random() > 0.5:
                aug_x = data_loader.time_shift(aug_x)
                
            if enable_scale and np.random.random() > 0.3:
                aug_x = data_loader.scale_amplitude(aug_x)
                
            if enable_mask and np.random.random() > 0.7:
                aug_x = data_loader.channel_mask(aug_x)
                
            if enable_noise: # å™ªå£°é€šå¸¸æœ€ååŠ 
                aug_x = data_loader.add_noise(aug_x)
            
            X_aug.append(aug_x)
            y_aug.append(y[i])
            groups_aug.append(f"{groups[i]}_aug")
            
        if progress_bar and i % 10 == 0:
            progress_bar.progress(i / total)
            
    if progress_bar: progress_bar.progress(1.0)
    
    return np.array(X_aug, dtype=np.float32), np.array(y_aug), np.array(groups_aug)

# ================= 1. æ ¸å¿ƒï¼šç§»æ¤è‡ª nina_auto_train.py çš„æ•°æ®å¤„ç† =================
def process_nina_data(data_root, selected_subjects, target_labels, 
                      stride_ms=50, split_strategy="mixed", 
                      progress_callback=None):
    """
    å®Œå…¨å¤åˆ» nina_auto_train.py çš„æ•°æ®å¤„ç†é€»è¾‘ 
    """
    X_list = []
    y_list = []
    groups_list = []
    
    total_files = len(selected_subjects)
    
    # ç¡¬ç¼–ç é…ç½® (ä¿æŒä¸ nina_auto_train ä¸€è‡´)
    # nina_auto_train ä¸­ window æ˜¯ Center Â± 150ï¼Œå³ 300 ç‚¹
    WINDOW_RADIUS = 150 
    WINDOW_SIZE = WINDOW_RADIUS * 2
    
    for idx, subject_name in enumerate(selected_subjects):
        subj_upper = subject_name.upper()
        
        # å¢å¼ºçš„æ–‡ä»¶åæœç´¢
        possible_filenames = [
            f"{subj_upper}_A1_E1.mat",   # S1_A1_E1.mat
            f"{subject_name}_A1_E1.mat", # s1_A1_E1.mat
        ]
        
        mat_file = None
        folder_path = os.path.join(data_root, subject_name)
        
        for fname in possible_filenames:
            full_path = os.path.join(folder_path, fname)
            if os.path.exists(full_path):
                mat_file = full_path
                break
        
        if progress_callback:
            progress_callback((idx / total_files), f"æ­£åœ¨å¤„ç†: {subject_name}")

        if not mat_file:
            print(f"âš ï¸ è·³è¿‡: æ‰¾ä¸åˆ° {subject_name} çš„ .mat æ–‡ä»¶")
            continue

        try:
            # === è¯»å– MAT æ–‡ä»¶ ===
            mat_data = sio.loadmat(mat_file)
            
            # 1. è·å– EMG (å–å‰8åˆ—ï¼Œå½’ä¸€åŒ–)
            if 'emg' in mat_data:
                raw_emg = mat_data['emg']
            else:
                keys = [k for k in mat_data.keys() if 'emg' in k.lower()]
                if keys: raw_emg = mat_data[keys[0]]
                else: continue
            
            raw_emg = raw_emg[:, :8]
            emg_data = (raw_emg / 0.0024) - 1 # å½’ä¸€åŒ–
            
            # 2. è·å–æ ‡ç­¾
            if 'restimulus' in mat_data:
                stimulus = mat_data['restimulus'].flatten()
            elif 'stimulus' in mat_data:
                stimulus = mat_data['stimulus'].flatten()
            else:
                continue

            # === åˆ‡ç‰‡é€»è¾‘ ===
            labeled_array, num_features = ndimage.label(stimulus > 0)
            
            subj_act_X, subj_act_y, subj_act_groups = [], [], []
            subj_rest_X, subj_rest_y, subj_rest_groups = [], [], []
            
            # --- A. æå–åŠ¨ä½œæ ·æœ¬ ---
            for i in range(1, num_features + 1):
                indices = np.where(labeled_array == i)[0]
                current_label = int(np.median(stimulus[indices]))
                
                if current_label not in target_labels:
                    continue
                
                center_idx = int((indices[0] + indices[-1]) / 2)
                start_win = center_idx - WINDOW_RADIUS
                end_win = center_idx + WINDOW_RADIUS
                
                if start_win < 0 or end_win > len(emg_data):
                    continue
                
                window = emg_data[start_win:end_win]
                
                if window.shape[0] == WINDOW_SIZE:
                    subj_act_X.append(window)
                    subj_act_y.append(current_label)
                    subj_act_groups.append(f"{subject_name}_act_{i}")

            # --- B. æå–é™æ¯æ ·æœ¬ (Rest) 
            if 0 in target_labels:
                # 1. è†¨èƒ€åŠ¨ä½œåŒºåŸŸ (ä½œä¸º Buffer)ï¼Œé¿å¼€åŠ¨ä½œè¾¹ç¼˜
                buffer_size = 100
                mask_active = stimulus > 0
                mask_forbidden = ndimage.binary_dilation(mask_active, structure=np.ones(buffer_size))
                mask_rest = ~mask_forbidden # å–åï¼Œå¾—åˆ°çº¯å‡€é™æ¯åŒº
                
                labeled_rest, num_rest = ndimage.label(mask_rest)
                
                for i in range(1, num_rest + 1):
                    r_indices = np.where(labeled_rest == i)[0]
                    # å¦‚æœè¿™æ®µé™æ¯å¤ªçŸ­ (å°äº 300)ï¼Œå°±è·³è¿‡
                    if len(r_indices) < 300: continue
                    
                    # åªå–è¿™æ®µé™æ¯çš„æœ€ä¸­é—´ä¸€æ®µ
                    center_idx = int((r_indices[0] + r_indices[-1]) / 2)
                    start_win = center_idx - WINDOW_RADIUS
                    end_win = center_idx + WINDOW_RADIUS
                    
                    if start_win < 0 or end_win > len(emg_data): continue

                    window = emg_data[start_win:end_win]
                    
                    if window.shape[0] == WINDOW_SIZE:
                        subj_rest_X.append(window)
                        subj_rest_y.append(0) # Label 0
                        subj_rest_groups.append(f"{subject_name}_rest_{i}")

            # --- C. åˆå¹¶ä¸å¹³è¡¡ (Balancing) ---
            if len(subj_act_X) > 0:
                # 1. åŠ¨ä½œæ•°æ®ç›´æ¥åŠ å…¥
                X_list.extend(subj_act_X)
                y_list.extend(subj_act_y)
                groups_list.extend(subj_act_groups)

                # 2. è®¡ç®—åˆé€‚çš„é™æ¯æ•°é‡ (1:1 å¹³è¡¡ç­–ç•¥)
                if len(subj_rest_X) > 0:
                    unique_act_classes = np.unique(subj_act_y)
                    num_act_classes_found = len(unique_act_classes)
                    
                    if num_act_classes_found > 0:
                        target_rest_count = int(len(subj_act_X) / num_act_classes_found)
                    else:
                        target_rest_count = len(subj_rest_X) 
                    
                    # 3. éšæœºé‡‡æ ·
                    if len(subj_rest_X) > target_rest_count and target_rest_count > 0:
                        selected_indices = np.random.choice(len(subj_rest_X), target_rest_count, replace=False)
                        for s_idx in selected_indices:
                            X_list.append(subj_rest_X[s_idx])
                            y_list.append(subj_rest_y[s_idx])
                            groups_list.append(subj_rest_groups[s_idx])
                    else:
                        X_list.extend(subj_rest_X)
                        y_list.extend(subj_rest_y)
                        groups_list.extend(subj_rest_groups)
            elif len(subj_rest_X) > 0:
                X_list.extend(subj_rest_X)
                y_list.extend(subj_rest_y)
                groups_list.extend(subj_rest_groups)

        except Exception as e:
            print(f"âŒ å¤„ç†å‡ºé”™ {mat_file}: {e}")
            
    return np.array(X_list), np.array(y_list), np.array(groups_list)

# ================= 2. ä¾§è¾¹æ ï¼šé…ç½® =================
st.sidebar.title("ğŸ› ï¸ NinaPro å¾®è°ƒé…ç½®")

# --- A. æ¨¡å‹ ---
st.sidebar.header("1. åŸºæ¨¡å‹ (Base Model)")
base_model_file = st.sidebar.file_uploader(
    "ä¸Šä¼  nina_auto_train ç”Ÿæˆçš„ .keras/.h5", 
    type=["keras", "h5"]
)

# --- B. æ•°æ®æº ---
st.sidebar.header("2. æ•°æ®æº (NinaPro Data)")
data_root_input = st.sidebar.text_input("æ•°æ®æ ¹ç›®å½• (åŒ…å« s1, s2...)", value="data")

all_subjects = []
if os.path.exists(data_root_input):
    try:
        items = os.listdir(data_root_input)
        all_subjects = sorted([d for d in items if os.path.isdir(os.path.join(data_root_input, d)) and d.startswith('s')])
    except:
        pass

if not all_subjects:
    st.sidebar.warning("æœªæ£€æµ‹åˆ° 's*' æ–‡ä»¶å¤¹ï¼Œè¯·æ‰‹åŠ¨æ£€æŸ¥è·¯å¾„")
    manual_subjs = st.sidebar.text_input("æˆ–æ‰‹åŠ¨è¾“å…¥ Subject (é€—å·åˆ†éš”)", "s1, s2")
    if manual_subjs:
        selected_subjects = [s.strip() for s in manual_subjs.split(',')]
else:
    selected_subjects = st.sidebar.multiselect("é€‰æ‹© Subject è¿›è¡Œå¾®è°ƒ", all_subjects, default=all_subjects[:1])

target_labels_str = st.sidebar.text_input("ç›®æ ‡åŠ¨ä½œ ID (é€—å·åˆ†éš”)", "1, 2, 5, 6")
try:
    target_labels = [int(x.strip()) for x in target_labels_str.split(',') if x.strip()]
except:
    st.sidebar.error("Label æ ¼å¼é”™è¯¯")
    target_labels = []

# --- C. å¾®è°ƒå‚æ•° ---
st.sidebar.header("3. è®­ç»ƒå‚æ•°")
# [MODIFIED] å¢åŠ  "ç›´æ¥è¯„ä¼° (Inference Only)" é€‰é¡¹
fine_tune_mode = st.sidebar.radio(
    "æ¨¡å¼é€‰æ‹©", 
    ["Few-shot (å†»ç»“ç‰¹å¾)", "Full Fine-tune (å…¨é‡å¾®è°ƒ)", "ç›´æ¥è¯„ä¼° (Inference Only)"], 
    index=0
)

# ä»…åœ¨éç›´æ¥è¯„ä¼°æ¨¡å¼ä¸‹æ˜¾ç¤ºè¿™äº›å‚æ•°
is_inference_only = (fine_tune_mode == "ç›´æ¥è¯„ä¼° (Inference Only)")
unfreeze_all = (fine_tune_mode == "Full Fine-tune (å…¨é‡å¾®è°ƒ)")

if not is_inference_only:
    epochs = st.sidebar.number_input("Epochs", 10, 200, 30)
    batch_size = st.sidebar.selectbox("Batch Size", [16, 32, 64, 128], index=1)
    lr = st.sidebar.number_input("Learning Rate", value=0.001, format="%.5f")
    num_shots = st.sidebar.slider("æ¯ç±»æ ·æœ¬æ•° (Few-shotç”¨)", 1, 10, 2) if not unfreeze_all else 9999
else:
    # æ¨ç†æ¨¡å¼ä¸‹åªéœ€å°‘è®¸å‚æ•°
    batch_size = 32
    epochs = 0
    st.sidebar.info("â„¹ï¸ ç›´æ¥è¯„ä¼°æ¨¡å¼ï¼šè·³è¿‡è®­ç»ƒï¼Œç›´æ¥ä½¿ç”¨åŸºæ¨¡å‹é¢„æµ‹æ‰€é€‰æ•°æ®ã€‚è¯·ç¡®ä¿æ‰€é€‰åŠ¨ä½œæ ‡ç­¾çš„é¡ºåºä¸æ¨¡å‹è®­ç»ƒæ—¶ä¸€è‡´ã€‚")

# [NEW] æ•°æ®å¢å¼ºé…ç½®
with st.sidebar.expander("ğŸ§ª æ•°æ®å¢å¼º (Data Augmentation)", expanded=False):
    st.caption("å°æ ·æœ¬(Few-shot)è®­ç»ƒæ—¶å»ºè®®å¼€å¯")
    aug_multiplier = st.slider("æ ·æœ¬å€å¢ç³»æ•° (Multiplier)", 1, 20, 1, help="å°†è®­ç»ƒé›†æ‰©å¤§Nå€")
    
    c1, c2 = st.columns(2)
    enable_noise = c1.checkbox("é«˜æ–¯å™ªå£°", True)
    enable_scale = c2.checkbox("å¹…åº¦ç¼©æ”¾", True)
    enable_warp = c1.checkbox("æ—¶é—´æ‰­æ›²", False, help="è®¡ç®—é‡è¾ƒå¤§")
    enable_shift = c2.checkbox("æ—¶é—´å¹³ç§»", False)
    enable_mask = st.checkbox("é€šé“é®æŒ¡", False)

    augment_config = {
        'multiplier': aug_multiplier,
        'enable_noise': enable_noise,
        'enable_scaling': enable_scale,
        'enable_warp': enable_warp,
        'enable_shift': enable_shift,
        'enable_mask': enable_mask
    }

run_btn = st.sidebar.button("ğŸš€ å¼€å§‹å¾®è°ƒ", type="primary")

# ================= 3. ä¸»ç•Œé¢é€»è¾‘ =================
st.title("ğŸ§  NinaPro æ¨¡å‹å¾®è°ƒ (MATç‰ˆ)")
st.caption("åŸºäº nina_auto_train.py æ ¸å¿ƒé€»è¾‘ + Few-shot æ•°æ®å¢å¼º")

if run_btn:
    if not base_model_file:
        st.error("è¯·ä¸Šä¼ åŸºæ¨¡å‹æ–‡ä»¶ï¼")
        st.stop()
    if not selected_subjects:
        st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ª Subjectï¼")
        st.stop()
        
    # --- Step 1: åŠ è½½æ•°æ® ---
    st.subheader("1. è¯»å– MAT æ•°æ®")
    bar = st.progress(0)
    status = st.empty()
    
    X, y, groups = process_nina_data(
        data_root_input, 
        selected_subjects, 
        target_labels,
        progress_callback=lambda p, t: (bar.progress(p), status.text(t))
    )
    
    bar.progress(100)
    if len(X) == 0:
        st.error(f"æœªæå–åˆ°ä»»ä½•æ ·æœ¬ï¼è¯·æ£€æŸ¥ {data_root_input} ä¸‹æ˜¯å¦æœ‰ .mat æ–‡ä»¶ï¼Œä»¥åŠ Label æ˜¯å¦å­˜åœ¨ã€‚")
        st.stop()
        
    if len(X) == 0:
        st.error(f"æœªæå–åˆ°ä»»ä½•æ ·æœ¬ï¼è¯·æ£€æŸ¥ {data_root_input} ä¸‹æ˜¯å¦æœ‰ .mat æ–‡ä»¶ï¼Œä»¥åŠ Label æ˜¯å¦å­˜åœ¨ã€‚")
        st.stop()
        
    X = X.astype(np.float32)
    
    unique_labels = np.unique(y)
    y_mapped = y.astype(int)  # ç›´æ¥ä½¿ç”¨åŸå§‹å€¼
    
    label_map = {val: val for val in unique_labels} 

    num_classes = int(np.max(y_mapped)) + 1
    
    st.success(f"âœ… åŸå§‹æ•°æ®åŠ è½½æˆåŠŸ: X={X.shape}, y={y.shape} | åŒ…å«åŠ¨ä½œ: {unique_labels}")
    
    # --- Step 2: åˆ’åˆ†æ•°æ®é›† ---
    if is_inference_only:
        # [NEW] ç›´æ¥è¯„ä¼°æ¨¡å¼ï¼šæ‰€æœ‰æ•°æ®éƒ½æ˜¯æµ‹è¯•é›†
        st.info("æ¨¡å¼: ç›´æ¥è¯„ä¼° (Inference Only) - æ‰€æœ‰åŠ è½½çš„æ•°æ®å°†ç›´æ¥ç”¨äºæµ‹è¯•ï¼Œä¸è¿›è¡Œè®­ç»ƒã€‚")
        X_train = np.array([]) # ç©ºæ•°ç»„
        y_train = np.array([])
        groups_train = np.array([])
        
        X_test = X
        y_test = y_mapped
        groups_test = groups # å‡è®¾ groups ä¹Ÿæœ‰ç”¨
        
    elif unfreeze_all:
        if len(selected_subjects) > 1:
            test_mask = np.char.startswith(groups, selected_subjects[-1])
            train_idx = np.where(~test_mask)[0]
            test_idx = np.where(test_mask)[0]
            st.info(f"éªŒè¯ç­–ç•¥: ä½¿ç”¨ {selected_subjects[-1]} ä½œä¸ºéªŒè¯é›†")
        else:
            from sklearn.model_selection import train_test_split
            idx = np.arange(len(X))
            train_idx, test_idx = train_test_split(idx, test_size=0.2, random_state=42, stratify=y_mapped)
            st.info("éªŒè¯ç­–ç•¥: å• Subject å†…éƒ¨éšæœºåˆ’åˆ† (80/20)")
    else:
        # Few-shot: éšæœºæŠ½å– N ä¸ªæ ·æœ¬
        train_idx, test_idx = train_utils.get_few_shot_split(X, y_mapped, num_shots)
        st.info(f"éªŒè¯ç­–ç•¥: Few-shot (æ¯ç±» {num_shots} ä¸ªè®­ç»ƒæ ·æœ¬)")
        
        X_train, X_test = X[train_idx], X[test_idx]
        y_train, y_test = y_mapped[train_idx], y_mapped[test_idx]
        groups_train = groups[train_idx]

    # --- [NEW] Step 2.5: æ•°æ®å¢å¼º (ä»…é’ˆå¯¹è®­ç»ƒé›†) ---
    # [MODIFIED] åªæœ‰åœ¨éæ¨ç†æ¨¡å¼ä¸”å¼€å¯å¢å¼ºæ—¶æ‰æ‰§è¡Œ
    if not is_inference_only and augment_config['multiplier'] > 1:
        st.subheader("2. æ‰§è¡Œæ•°æ®å¢å¼º")
        aug_bar = st.progress(0)
        st.info(f"æ­£åœ¨å°†è®­ç»ƒé›†æ‰©å¤§ {augment_config['multiplier']} å€ (åº”ç”¨: å™ªå£°={enable_noise}, æ‰­æ›²={enable_warp}...)")
        
        X_train, y_train, groups_train = augment_dataset(
            X_train, y_train, groups_train, augment_config, progress_bar=aug_bar
        )
        st.success(f"ğŸ“ˆ å¢å¼ºåè®­ç»ƒé›†è§„æ¨¡: {X_train.shape}")
    
    # --- Step 3: åŠ è½½ä¸é€‚é…æ¨¡å‹ ---
    st.subheader("3. æ¨¡å‹å‡†å¤‡")
    
    temp_path = f"temp_{base_model_file.name}"
    with open(temp_path, "wb") as f: f.write(base_model_file.getbuffer())
    
    try:
        base_model = tf.keras.models.load_model(temp_path)
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        st.stop()
        
    if base_model.input_shape[-1] != X.shape[-1]:
        st.error(f"âŒ ç»´åº¦ä¸åŒ¹é…: æ¨¡å‹è¾“å…¥é€šé“ {base_model.input_shape[-1]} vs æ•°æ®é€šé“ {X.shape[-1]}")
        st.stop()
        
    old_classes = base_model.output_shape[-1]
    
    if old_classes >= num_classes:
        num_classes = old_classes
        # st.info(f"å·²å¯¹é½åŸºæ¨¡å‹è¾“å‡ºç»´åº¦: {num_classes} ç±»")
    
    # [MODIFIED] æ”¹é€ æ¨¡å‹é€»è¾‘
    if is_inference_only:
        # æ¨ç†æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨åŸæ¨¡å‹
        if old_classes != num_classes:
            st.warning(f"âš ï¸ è­¦å‘Š: æ¨¡å‹è¾“å‡ºç±»åˆ«æ•° ({old_classes}) ä¸å½“å‰æ•°æ®ç±»åˆ«æ•° ({num_classes}) ä¸ä¸€è‡´ï¼æ··æ·†çŸ©é˜µå¯èƒ½æ— æ³•æ­£ç¡®å¯¹åº”ã€‚")
        model = base_model
        # å³ä½¿ä¸è®­ç»ƒï¼Œcompile ä¹Ÿæ˜¯ä¸ºäº†åç»­ evaluate èƒ½è®¡ç®— loss/accuracy
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        
    elif unfreeze_all:
        base_model.trainable = True
        if old_classes == num_classes:
            model = base_model
        else:
            st.warning(f"é‡ç½®åˆ†ç±»å¤´: {old_classes} -> {num_classes} ç±»")
            feature_out = base_model.layers[-2].output
            new_out = tf.keras.layers.Dense(num_classes, activation='softmax')(feature_out)
            model = tf.keras.models.Model(base_model.input, new_out)
    else:
        base_model.trainable = False 
        feature_layer = None
        for layer in reversed(base_model.layers):
            if "global" in layer.name or "flatten" in layer.name:
                feature_layer = layer
                break
        
        feat_out = feature_layer.output if feature_layer else base_model.layers[-2].output
        
        x = tf.keras.layers.Dropout(0.5)(feat_out)
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)
        model = tf.keras.models.Model(base_model.input, outputs)
        
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )
    
    # --- Step 4: è®­ç»ƒ ---
    # [MODIFIED] ä»…åœ¨éæ¨ç†æ¨¡å¼ä¸‹è®­ç»ƒ
    if not is_inference_only:
        st.subheader("4. å¼€å§‹è®­ç»ƒ")
        t_prog = st.progress(0)
        t_status = st.empty()
        st_cb = train_utils.StreamlitKerasCallback(epochs, t_prog, t_status)
        
        history = model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[st_cb],
            verbose=0
        )
        # ä¸ºäº†åé¢ç”»å›¾ä¸æŠ¥é”™ï¼Œæ„é€ ä¸€ä¸ªå‡çš„ history å¯¹è±¡ç»™æ¨ç†æ¨¡å¼ç”¨
    else:
        st.subheader("4. ç›´æ¥è¯„ä¼° (è·³è¿‡è®­ç»ƒ)")
        st.write("æ­£åœ¨ä½¿ç”¨åŸºæ¨¡å‹å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹...")
        # ä¸ºäº†ä»£ç å…¼å®¹æ€§ï¼Œæ‰‹åŠ¨åˆ›å»ºä¸€ä¸ªç±»ä¼¼ history çš„ç»“æ„
        class MockHistory:
            def __init__(self): self.history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}
        history = MockHistory()

    
    # --- Step 5: ç»“æœ ---
    st.subheader("5. è¯„ä¼°æŠ¥å‘Š")
    
    # [MODIFIED] è·å–è¯„ä¼°ç»“æœ
    if is_inference_only:
        loss, final_acc = model.evaluate(X_test, y_test, verbose=0)
    else:
        final_acc = history.history['val_accuracy'][-1]
        
    st.metric("æµ‹è¯•é›†å‡†ç¡®ç‡", f"{final_acc:.2%}")
    
    # æ··æ·†çŸ©é˜µ
    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)
    cm = confusion_matrix(y_test, y_pred)
    
    c1, c2 = st.columns(2)
    with c1:
        if not is_inference_only:
            # åªæœ‰è®­ç»ƒè¿‡æ‰æœ‰æ›²çº¿
            fig, ax = plt.subplots()
            ax.plot(history.history['loss'], label='Train')
            ax.plot(history.history['val_loss'], label='Val')
            ax.legend()
            ax.set_title("Loss Curve")
            st.pyplot(fig)
        else:
            st.info("ç›´æ¥è¯„ä¼°æ¨¡å¼æ— è®­ç»ƒæ›²çº¿")
            
    with c2:
        fig2, ax2 = plt.subplots()
        names = [str(k) for k in label_map.keys()]
        sns.heatmap(cm, annot=True, fmt='d', xticklabels=names, yticklabels=names, cmap='Blues', ax=ax2)
        ax2.set_title("Confusion Matrix")
        st.pyplot(fig2)
        
    # ä¿å­˜æ¨¡å‹
    st.markdown("---")
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M")
    save_name = st.text_input("ä¿å­˜æ¨¡å‹åç§°", f"finetuned_nina_{ts}.keras")
    if st.button("ğŸ’¾ ä¿å­˜å½“å‰æ¨¡å‹"):
        if not os.path.exists("trained_models"): os.makedirs("trained_models")
        path = os.path.join("trained_models", save_name)
        model.save(path)
        st.success(f"å·²ä¿å­˜è‡³ {path}")