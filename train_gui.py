import streamlit as st
import os
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import pandas as pd

import datetime 
import json

# --- æ¨¡å—å¯¼å…¥ ---
import data_loader
import train_utils
import ui_helper
# åœ¨é¡¶éƒ¨ import åŒºåŸŸåŠ å…¥
from model import build_simple_cnn, build_advanced_crnn, build_resnet_model, build_tcn_model, build_dual_stream_model

# ================= 0. å…¨å±€è®¾ç½® =================
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

st.set_page_config(layout="wide", page_title="EMG è®­ç»ƒå·¥ä½œç«™")

# åˆå§‹åŒ– Session State
if 'trained_model' not in st.session_state:
    st.session_state['trained_model'] = None

if 'train_results' not in st.session_state:
    st.session_state['train_results'] = None

# ================= 1. ä¾§è¾¹æ é…ç½® =================
st.sidebar.header("ğŸš€ è®­ç»ƒæ¨¡å¼")
train_mode = st.sidebar.radio("é€‰æ‹©æ¨¡å¼", ["ä»é›¶å¼€å§‹è®­ç»ƒ", "åŸºäºåŸºæ¨¡å‹å¾®è°ƒ (Few-shot)"])

base_model_path = None
unfreeze_all = False
if train_mode == "åŸºäºåŸºæ¨¡å‹å¾®è°ƒ (Few-shot)":
    base_model_path = st.sidebar.file_uploader("ä¸Šä¼ åŸºæ¨¡å‹ (.h5)", type=["h5"])
    
    st.sidebar.markdown("---")
    st.sidebar.caption("å¾®è°ƒç­–ç•¥")
    # === å…¨é‡å¾®è°ƒå¼€å…³ ===
    unfreeze_all = st.sidebar.checkbox(
        " è§£å†»æ‰€æœ‰å±‚ (Full Fine-tuning)", 
        value=False,
        help="å‹¾é€‰æ­¤é¡¹ç”¨äº SGD æ¥åŠ›è®­ç»ƒã€‚å¦‚æœä¸å‹¾é€‰ï¼Œåˆ™é»˜è®¤ä¸ºå†»ç»“ç‰¹å¾å±‚åªè®­ç»ƒåˆ†ç±»å¤´ã€‚"
    )
    # ===============================
    
    if not unfreeze_all:
        num_finetune_samples = st.sidebar.slider("æ¯ä¸ªç±»åˆ«ç”¨äºå¾®è°ƒçš„æ ·æœ¬æ•°", 1, 10, 5)
    else:
        # å¦‚æœæ˜¯å…¨é‡å¾®è°ƒï¼Œé€šå¸¸ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œæˆ–è€…ä¹Ÿå¯ä»¥ç”±ç”¨æˆ·æŒ‡å®š
        # è¿™é‡Œä¸ºäº†å…¼å®¹ï¼Œå¯ä»¥ä¿æŒæ˜¾ç¤ºï¼Œæˆ–è€…æç¤ºç”¨æˆ·
        st.sidebar.info("å·²å¯ç”¨å…¨é‡å¾®è°ƒï¼šSGD å°†æ›´æ–°æ¨¡å‹æ‰€æœ‰å‚æ•°ã€‚")
        num_finetune_samples = 99999

with st.sidebar:
    st.header("1. æ•°æ®é€‰æ‹©")
    DATA_ROOT = "data"
    
    if not os.path.exists(DATA_ROOT):
        st.error(f"æœªæ‰¾åˆ° {DATA_ROOT} æ–‡ä»¶å¤¹")
        st.stop()
        
    structure, file_map = ui_helper.scan_data_folder(DATA_ROOT)
    
    all_subjects = sorted(structure.keys())
    selected_subjects = ui_helper.render_multiselect_with_all(
        "é€‰æ‹©æµ‹è¯•è€… (Subjects)", all_subjects, 'selected_subjects_key', default_first=True
    )
    
    available_dates = set()
    for s in selected_subjects:
        if s in structure: available_dates.update(structure[s].keys())
    selected_dates = ui_helper.render_multiselect_with_all(
        "é€‰æ‹©æ—¥æœŸ (Dates)", sorted(list(available_dates)), 'selected_dates_key', default_first=True
    )
    
    available_labels = set()
    for s in selected_subjects:
        for d in selected_dates:
            if s in structure and d in structure[s]: available_labels.update(structure[s][d])
    selected_labels = ui_helper.render_multiselect_with_all(
        "é€‰æ‹©åŠ¨ä½œ ID (Labels)", sorted(list(available_labels)), 'selected_labels_key', default_first=True
    )

    st.markdown("---")
    
    target_files = []
    for s in selected_subjects:
        for d in selected_dates:
            for l in selected_labels:
                key = (s, d, l)
                if key in file_map: target_files.extend(file_map[key])
    
    st.info(f"å·²é€‰ä¸­ **{len(target_files)}** ä¸ª CSV æ–‡ä»¶")

    st.header("2. å¢å¼ºä¸è®­ç»ƒé…ç½®")
    
    with st.expander("æ•°æ®å¢å¼ºä¸é‡‡æ ·", expanded=False):
        train_stride_ms = st.slider("åˆ‡ç‰‡æ­¥é•¿ (Stride ms)", 10, 200, 100)
        st.caption("è´Ÿæ ·æœ¬ç­–ç•¥")
        enable_rest = st.checkbox("åŠ å…¥é™æ¯ç±» (Rest, Label 0)", value=True)
        st.caption("å¢å¼ºç­–ç•¥")
        c1, c2 = st.columns(2)
        enable_scaling = c1.checkbox("å¹…åº¦ç¼©æ”¾", value=True)
        enable_noise = c2.checkbox("é«˜æ–¯å™ªå£°", value=True)
        enable_warp = c1.checkbox("æ—¶é—´æ‰­æ›²", value=False)
        enable_shift = c2.checkbox("æ—¶é—´å¹³ç§»", value=False)
        enable_mask = st.checkbox("é€šé“é®æŒ¡", value=False)
        
        aug_multiplier = 1
        if train_mode == "åŸºäºåŸºæ¨¡å‹å¾®è°ƒ (Few-shot)":
            aug_multiplier = st.slider("æ ·æœ¬å€å¢ç³»æ•°", 1, 50, 20)
        
        augment_config = {
            'enable_rest': enable_rest,
            'multiplier': aug_multiplier,
            'enable_scaling': enable_scaling, 
            'enable_noise': enable_noise,
            'enable_warp': enable_warp,
            'enable_shift': enable_shift,
            'enable_mask': enable_mask
        }
        
    st.markdown("---")
    st.header("æ¨¡å‹ä¸é«˜çº§ç­–ç•¥")
    
    # æ¨¡å‹é€‰æ‹©åˆ—è¡¨æ‰©å±•
    model_options = {
        "Lite: Simple CNN": build_simple_cnn,
        "Pro: Multi-Scale CRNN (Recommended)": build_advanced_crnn,
        "New: ResNet-1D (Deep Residual)": build_resnet_model,
        "New: TCN (Temporal ConvNet)": build_tcn_model,
        "New: Dual-Stream (Time + Freq Fusion)": build_dual_stream_model
    }
    model_choice = st.selectbox("é€‰æ‹©æ¨¡å‹æ¶æ„", list(model_options.keys()), index=1)

    st.caption("ğŸ”§ ä¼˜åŒ–å™¨é…ç½®")
    
    # å¸ƒå±€ï¼šå·¦è¾¹é€‰ä¼˜åŒ–å™¨ï¼Œå³è¾¹å¡«åŸºç¡€å­¦ä¹ ç‡
    c_opt1, c_opt2 = st.columns([1, 1])
    with c_opt1:
        # è¿™é‡Œçš„é€‰é¡¹å¯¹åº”æˆ‘ä»¬åœ¨è°ƒç ”ä¸­é€‰å‡ºçš„å‡ ä¸ª
        optimizer_name = st.selectbox(
            "é€‰æ‹©ä¼˜åŒ–å™¨", 
            ["Adam (Default)", "AdamW (SOTA)", "Nadam (RNN+)", "SGD (Expert)"], 
            index=0
        )
    with c_opt2:
        learning_rate = st.number_input(
            "å­¦ä¹ ç‡", 
            value=0.001, format="%.6f", step=0.0001,
            help="é€šå¸¸ Adam/Nadam ç”¨ 1e-3, SGD å»ºè®® 1e-2 æˆ–æ›´å°"
        )

    # åŠ¨æ€å‚æ•°åŒºåŸŸï¼šæ ¹æ®é€‰æ‹©æ˜¾ç¤ºç‰¹å®šå‚æ•°
    opt_params = {}
    if "AdamW" in optimizer_name:
        # AdamW æ ¸å¿ƒå‚æ•°æ˜¯ weight_decay
        st.caption("ğŸŒŠ AdamW ä¸“å±è®¾ç½®")
        wd = st.number_input("æƒé‡è¡°å‡ (Weight Decay)", value=1e-4, format="%.5f", step=1e-5, help="æ¨è 1e-4 ~ 1e-2")
        opt_params['weight_decay'] = wd
        
    elif "SGD" in optimizer_name:
        # SGD å¿…é¡»é…åˆ Momentum æ‰å¥½ç”¨
        st.caption("ğŸï¸ SGD ä¸“å±è®¾ç½®")
        momentum = st.slider("åŠ¨é‡ (Momentum)", 0.0, 0.99, 0.9, 0.01, help="é€šå¸¸è®¾ç½®ä¸º 0.9")
        opt_params['momentum'] = momentum
    
    # é«˜çº§æŠ€å·§å¼€å…³
    use_mixup = st.checkbox("ğŸ§ª å¯ç”¨ Mixup æ•°æ®æ··åˆ", value=False, help="æ··åˆä¸¤ä¸ªæ ·æœ¬åŠæ ‡ç­¾ï¼Œæå‡æ³›åŒ–èƒ½åŠ›")
    label_smoothing = st.slider("Label Smoothing (æ ‡ç­¾å¹³æ»‘)", 0.0, 0.5, 0.0, 0.01, help="é˜²æ­¢æ¨¡å‹å¯¹æ ‡ç­¾è¿‡åº¦è‡ªä¿¡ï¼Œ0.1é€šå¸¸æ˜¯ä¸ªå¥½å€¼")
    
    # æŠ•ç¥¨ Loss (ä¿æŒä¸å˜)
    use_voting_loss = st.checkbox("ğŸ—³ï¸ å¼€å¯æŠ•ç¥¨æœºåˆ¶è¾…åŠ©è®­ç»ƒ (Vote Loss)", value=False)
    voting_weight = 0.0
    samples_per_group = 5 # ç»™ä¸€ä¸ªé»˜è®¤å€¼ï¼Œé˜²æ­¢æŠ¥é”™
    
    if use_voting_loss:
        c1, c2 = st.columns(2)
        voting_weight = c1.slider("æŠ•ç¥¨ Loss æƒé‡", 0.1, 0.9, 0.5)
        samples_per_group = c2.slider("æ¯ç»„é‡‡æ ·åˆ‡ç‰‡æ•°", 2, 20, 5)
        
        # [NEW] æ–°å¢ï¼šæŠ•ç¥¨ä»‹å…¥æ—¶æœº
        voting_start_epoch = st.slider("æŠ•ç¥¨ä»‹å…¥ Epoch (Warm-up)", 0, 50, 10, 
                                       help="å‰ N è½®åªè®­ç»ƒåŸºç¡€å‡†ç¡®ç‡ï¼Œä¹‹åå†å¼€å¯æŠ•ç¥¨çº¦æŸï¼Œé˜²æ­¢åˆæœŸæ¢¯åº¦æ··ä¹±ã€‚")
    else:
        # ç»™é»˜è®¤å€¼é˜²æ­¢æŠ¥é”™
        voting_start_epoch = 0

    st.markdown("---")
    split_mode = st.radio("éªŒè¯ç­–ç•¥", ("1. æ··åˆåˆ‡åˆ†", "2. ç•™æ–‡ä»¶éªŒè¯", "3. ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯"))
    strategy_map = {
        "1. æ··åˆåˆ‡åˆ†": "æ··åˆåˆ‡åˆ† (çœ‹åˆ°æ‰€æœ‰å¤©/äºº)",
        "2. ç•™æ–‡ä»¶éªŒè¯": "ç•™æ–‡ä»¶éªŒè¯ (åŒå¤©/åŒäºº)",
        "3. ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯": "ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯ (æ³›åŒ–èƒ½åŠ›)"
    }
    selected_strategy = strategy_map[split_mode]

    manual_val_target = None
    if "ç•™æ–‡ä»¶" in selected_strategy and target_files:
        file_options = sorted(list(set([os.path.basename(f) for f in target_files])))
        manual_val_target = st.selectbox("ğŸ¯ æŒ‡å®šæµ‹è¯•æ–‡ä»¶", file_options)
    elif "ç•™æ—¥æœŸ" in selected_strategy and target_files:
        group_options = sorted(list(set([os.path.basename(os.path.dirname(f)) for f in target_files])))
        manual_val_target = st.selectbox("ğŸ¯ æŒ‡å®šæµ‹è¯•å¯¹è±¡/æ—¥æœŸ", group_options)

    st.markdown("---") 
    epochs = st.number_input("Epochs", 10, 200, 50)
    batch_size = st.selectbox("Batch Size (Groups if Voting)", [8, 16, 32, 64, 128, 256, 512], index=1)
    test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.01, 0.5, 0.2)
    
    run_btn = st.button("ğŸš€ å¼€å§‹å¤„ç†å¹¶è®­ç»ƒ", type="primary")

# ================= 2. ä¸»é€»è¾‘åŒºåŸŸ =================
st.title("ğŸ§  EMG äº¤äº’å¼è®­ç»ƒç³»ç»Ÿ")

if run_btn and target_files:
    # --- A. æ•°æ®å¤„ç† ---
    st.subheader("1. æ•°æ®é¢„å¤„ç†")
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    X, y, groups = data_loader.process_selected_files(
        target_files, 
        progress_callback=lambda p, t: (progress_bar.progress(p), status_text.text(t)),
        stride_ms=train_stride_ms,
        augment_config=augment_config
    )
    
    status_text.text("å¤„ç†å®Œæˆï¼")
    progress_bar.progress(100)

    X = X.astype(np.float32)
    
    if len(X) == 0:
        st.error("æ ·æœ¬æ•°ä¸º 0ï¼Œè¯·æ£€æŸ¥æ•°æ®ã€‚")
        st.stop()
        
    st.success(f"X={X.shape}, y={y.shape} | ç±»åˆ«: {np.unique(y)}")
    
    # --- B. æ¨¡å‹è®­ç»ƒå‡†å¤‡ ---
    st.subheader("2. æ¨¡å‹è®­ç»ƒ")
    if "AdamW" in optimizer_name:
        # éœ€è¦ TF 2.10+ï¼Œå¦‚æœæŠ¥é”™è¯·é™çº§å› Adam
        try:
            optimizer = tf.keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=opt_params['weight_decay'])
        except AttributeError:
            st.error("æ‚¨çš„ TensorFlow ç‰ˆæœ¬è¿‡ä½ï¼Œä¸æ”¯æŒ AdamWï¼Œå·²è‡ªåŠ¨åˆ‡æ¢å› Adamã€‚")
            optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
            
    elif "Nadam" in optimizer_name:
        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)
        
    elif "SGD" in optimizer_name:
        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=opt_params['momentum'])
        
    else: # Default Adam
        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    
    if train_mode == "åŸºäºåŸºæ¨¡å‹å¾®è°ƒ (Few-shot)":
        train_idx, test_idx = train_utils.get_few_shot_split(X, y, num_finetune_samples)
    else:
        train_idx, test_idx = train_utils.smart_split(
            X, y, groups, selected_strategy, test_size, manual_target=manual_val_target
        )

    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    groups_train = groups[train_idx] # è·å–è®­ç»ƒé›†çš„ç»„ä¿¡æ¯ï¼Œç”¨äºæŠ•ç¥¨è®­ç»ƒ
    
    unique_labels = np.unique(y)
    num_classes = len(unique_labels) 
    label_map = {original: new for new, original in enumerate(unique_labels)}
    y_train_mapped = np.array([label_map[i] for i in y_train])
    y_test_mapped = np.array([label_map[i] for i in y_test])

    # æ„å»ºæ¨¡å‹
    if train_mode == "åŸºäºåŸºæ¨¡å‹å¾®è°ƒ (Few-shot)":
        if base_model_path:
            # 1. ä¿å­˜å¹¶åŠ è½½åŸºæ¨¡å‹
            with open("temp_model.h5", "wb") as f: f.write(base_model_path.getbuffer())
            base_model = tf.keras.models.load_model("temp_model.h5")
            
            # === [MODIFIED] ä¿®æ”¹å¾®è°ƒé€»è¾‘ ===
            if unfreeze_all:
                # ç­–ç•¥ A: SGD æ¥åŠ›è®­ç»ƒ (å…¨é‡å¾®è°ƒ)
                base_model.trainable = True 
                
                # [FIX] æ£€æŸ¥ç±»åˆ«æ•°æ˜¯å¦ä¸€è‡´ï¼Œå¦‚æœä¸ä¸€è‡´å¿…é¡»é‡ç½®åˆ†ç±»å¤´
                # è·å–åŸºæ¨¡å‹æœ€åä¸€å±‚çš„è¾“å‡ºç»´åº¦
                old_classes = base_model.output_shape[-1]
                
                if old_classes == num_classes:
                    st.info(f"ç±»åˆ«æ•°ä¸€è‡´ ({num_classes})ï¼Œä¿æŒåŸè¾“å‡ºå±‚ç»“æ„ã€‚")
                    model = base_model
                else:
                    st.warning(f"æ£€æµ‹åˆ°ç±»åˆ«æ•°å˜åŒ– (åŸºæ¨¡å‹: {old_classes} -> å½“å‰: {num_classes})ï¼Œæ­£åœ¨é‡ç½®åˆ†ç±»å¤´...")
                    # å‰¥ç¦»æ—§çš„åˆ†ç±»å¤´ (å‡è®¾æœ€åä¸€å±‚æ˜¯ Dense)
                    # å¯»æ‰¾å€’æ•°ç¬¬äºŒä¸ªç‰¹å¾å±‚ (é€šå¸¸æ˜¯ GlobalAveragePooling æˆ– Dropout)
                    # è¿™é‡Œé‡‡ç”¨ä¸€ç§æ¯”è¾ƒé€šç”¨çš„åšæ³•ï¼šå–å€’æ•°ç¬¬äºŒå±‚çš„è¾“å‡º
                    feature_output = base_model.layers[-2].output 
                    
                    # é‡æ–°æ¥ä¸€ä¸ªæ–°çš„åˆ†ç±»å±‚
                    new_output = tf.keras.layers.Dense(num_classes, activation='softmax', name="new_dense_head")(feature_output)
                    model = tf.keras.models.Model(inputs=base_model.input, outputs=new_output)
                
                st.success(f"å·²åŠ è½½æ¨¡å‹ç”¨äº SGD å¾®è°ƒï¼Œæ‰€æœ‰å±‚å‡å¯è®­ç»ƒã€‚")
                
            else:
                # ç­–ç•¥ B: Few-shot (å†»ç»“ç‰¹å¾æå–å™¨) - ä¿æŒä½ åŸæ¥çš„ä»£ç 
                base_model.trainable = False 
                
                feature_output = None
                for layer in reversed(base_model.layers):
                    if "global_average_pooling" in layer.name or "flatten" in layer.name:
                        feature_output = layer.output
                        break
                if feature_output is None: feature_output = base_model.layers[-3].output
                
                x = feature_output
                x = tf.keras.layers.Dropout(0.5, name="ft_dropout_1")(x) 
                x = tf.keras.layers.Dense(64, activation='relu', 
                                          kernel_regularizer=tf.keras.regularizers.l2(0.01),
                                          name="ft_dense_1")(x)
                x = tf.keras.layers.Dropout(0.3, name="ft_dropout_2")(x)
                outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)
                
                model = tf.keras.models.Model(inputs=base_model.input, outputs=outputs)
            # ==============================

            # ç¼–è¯‘æ¨¡å‹ (ä½¿ç”¨ä½ åœ¨ä¸Šä¸€è½®å¯¹è¯ä¸­æ·»åŠ çš„ optimizer)
            model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
            
        else:
            st.error("è¯·ä¸Šä¼ åŸºæ¨¡å‹ (.h5 æ–‡ä»¶)")
            st.stop()
    else:
        input_shape = (X.shape[1], X.shape[2])
        selected_builder = model_options[model_choice]
        model = selected_builder(input_shape, num_classes)
        
        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    # --- C. å¼€å§‹è®­ç»ƒ (åˆ†æ”¯é€»è¾‘) ---
    st.caption("è®­ç»ƒç›‘æ§")
    train_progress = st.progress(0)
    train_status = st.empty()
    if use_voting_loss or use_mixup or label_smoothing > 0:
        # åªè¦å¼€å¯äº†ä»»æ„é«˜çº§ç‰¹æ€§ï¼Œéƒ½å»ºè®®èµ°è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ (train_utils.py)
        # å› ä¸º Keras åŸç”Ÿ fit() å¤„ç† Mixup æ¯”è¾ƒéº»çƒ¦
        
        st.info(f"ğŸ”µ å¯åŠ¨é«˜çº§è®­ç»ƒå¾ªç¯ (Voting={use_voting_loss}, Mixup={use_mixup}, Smoothing={label_smoothing})")
        
        history_dict = train_utils.train_with_voting_mechanism(
            model, X_train, y_train_mapped, groups_train,
            X_test, y_test_mapped,
            epochs=epochs,
            batch_size=batch_size,
            samples_per_group=samples_per_group,
            vote_weight=voting_weight if use_voting_loss else 0.0, # å¦‚æœæ²¡å¼€æŠ•ç¥¨ï¼Œæƒé‡ç½®0
            st_progress_bar=train_progress,
            st_status_text=train_status,
            use_mixup=use_mixup,
            label_smoothing=label_smoothing,
            voting_start_epoch=voting_start_epoch if use_voting_loss else 0,
            optimizer=optimizer
        )
        
        class HistoryShim:
            def __init__(self, h_dict): self.history = h_dict
        history = HistoryShim(history_dict)
        
    else:
        # æ ‡å‡†è®­ç»ƒæ¨¡å¼
        st_callback = train_utils.StreamlitKerasCallback(epochs, train_progress, train_status)
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)

        history = model.fit(
            X_train, y_train_mapped,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(X_test, y_test_mapped),
            callbacks=[EarlyStopping(patience=10, restore_best_weights=True), reduce_lr, st_callback],
            verbose=0
        )
    
    if hasattr(history, 'history'):
        final_history = history.history # KerasåŸç”Ÿå¯¹è±¡è½¬å­—å…¸
    else:
        final_history = history.history # è‡ªå®šä¹‰Shimå¯¹è±¡æœ¬èº«å°±æ˜¯å­—å…¸
        
    # 2. é¢„å…ˆè®¡ç®—é¢„æµ‹å€¼ (å­˜èµ·æ¥ï¼Œé¿å…æ¯æ¬¡åˆ·æ–°éƒ½é‡ç®—ï¼ŒèŠ‚çœæ—¶é—´)
    y_pred_probs = model.predict(X_test)
    y_pred = np.argmax(y_pred_probs, axis=1)

    # 3. å­˜å…¥ Session State
    st.session_state['train_results'] = {
        'history': final_history,
        'model': model,
        'X_test': X_test,
        'y_test_mapped': y_test_mapped,
        'test_groups': groups[test_idx], # åˆ‡åˆ†åçš„ç»„ä¿¡æ¯
        'test_idx': test_idx,
        'y_pred': y_pred,
        'label_map': label_map,
        'class_names': [str(k) for k in label_map.keys()],
        'optimizer_info': {
            'name': optimizer_name,
            'lr': learning_rate,
            'params': opt_params # è¿™æ˜¯æˆ‘ä»¬åœ¨ UI éƒ¨åˆ†å®šä¹‰çš„é‚£ä¸ªå­—å…¸
        }
    }
    
    # æ›´æ–°å…¨å±€æ¨¡å‹çŠ¶æ€
    st.session_state['trained_model'] = model
    st.success("è®­ç»ƒå®Œæˆï¼ç»“æœå·²ç¼“å­˜ã€‚")
    
if st.session_state['train_results'] is not None:
    
    # 1. ä»â€œä¿é™©ç®±â€é‡Œå–å‡ºæ‰€æœ‰æ•°æ®
    res = st.session_state['train_results']
    
    # è§£åŒ…å˜é‡ (æ–¹ä¾¿åé¢ä»£ç ç›´æ¥å¤ç”¨ï¼Œä¸ç”¨æ”¹å¤ªå¤šå˜é‡å)
    history_dict = res['history']
    model = res['model']
    X_test = res['X_test']
    y_test_mapped = res['y_test_mapped']
    test_groups = res['test_groups']
    y_pred = res['y_pred']
    label_map = res['label_map']
    class_names = res['class_names']
    num_classes = len(label_map)

    # --- D. ç»“æœå¯è§†åŒ– (ç›´æ¥å¤ç”¨ä½ åŸæ¥çš„ä»£ç ï¼Œåªéœ€æ”¹ä¸€ä¸‹ history å˜é‡å) ---
    col1, col2 = st.columns(2)
    with col1:
        fig, ax = plt.subplots()
        ax.plot(history_dict['accuracy'], label='Train')
        ax.plot(history_dict['val_accuracy'], label='Val')
        ax.set_title("Accuracy")
        ax.legend()
        st.pyplot(fig)
    with col2:
        fig, ax = plt.subplots()
        ax.plot(history_dict['loss'], label='Train')
        ax.plot(history_dict['val_loss'], label='Val')
        ax.set_title("Loss")
        ax.legend()
        st.pyplot(fig)
    
    # --- E. æ·±åº¦è¯„ä¼°æŠ¥å‘Š ---
    st.markdown("---")
    st.subheader("3. æ·±åº¦è¯„ä¼°æŠ¥å‘Š")
    
    # (1) æ··æ·†çŸ©é˜µ
    st.write("#### (1) æ··æ·†çŸ©é˜µ")
    cm = confusion_matrix(y_test_mapped, y_pred)
    fig_cm, ax_cm = plt.subplots(figsize=(6, 5))
    try:
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=class_names, yticklabels=class_names, ax=ax_cm)
    except:
        ax_cm.matshow(cm, cmap='Blues')
    
    # é™åˆ¶å›¾ç‰‡å®½åº¦
    c_small, _ = st.columns([1, 1])
    with c_small:
        st.pyplot(fig_cm)

    # (2) è¯¦ç»†æŒ‡æ ‡
    st.write("#### (2) è¯¦ç»†åˆ†ç±»æŒ‡æ ‡")
    report_dict = classification_report(y_test_mapped, y_pred, target_names=class_names, output_dict=True)
    report_df = pd.DataFrame(report_dict).transpose()
    st.dataframe(report_df.style.background_gradient(cmap='Greens', subset=['f1-score']))

    # (3) æŠ•ç¥¨åˆ†æ (è¿™å°±æ˜¯ä½ ä¹‹å‰ç‚¹ä¸€ä¸‹å°±åˆ·æ–°çš„åœ°æ–¹)
    # ç°åœ¨å› ä¸ºå®ƒåœ¨ st.session_state çš„ä¿æŠ¤ä¸‹ï¼Œåˆ·æ–°ä¹Ÿä¸ä¼šæ¶ˆå¤±äº†
    st.markdown("---")
    
    show_segment_analysis = use_voting_loss
    if not use_voting_loss:
        st.caption("â„¹ï¸ æç¤ºï¼šæœªå¼€å¯æŠ•ç¥¨è®­ç»ƒï¼Œä½†å¯æ‰‹åŠ¨æŸ¥çœ‹æŠ•ç¥¨è¯„ä¼°ã€‚")
        # ã€å…³é”®ã€‘è¿™ä¸ª checkbox ç‚¹å‡»åä¼šåˆ·æ–°é¡µé¢ï¼Œä½†å› ä¸º train_results è¿˜åœ¨ï¼Œ
        # æ‰€ä»¥ç¨‹åºä¼šå†æ¬¡è·‘è¿›è¿™ä¸ª if å—ï¼Œæ­£ç¡®æ˜¾ç¤ºç»“æœã€‚
        show_segment_analysis = st.checkbox("ğŸ” æ˜¾ç¤ºç‰‡æ®µçº§å¹³æ»‘/æŠ•ç¥¨è¯„ä¼°", value=False)
    
    if show_segment_analysis:
        st.write("#### (3) ğŸ—³ï¸ åŠ¨ä½œç‰‡æ®µçº§æŠ•ç¥¨è¯¦æƒ…")
        
        # --- æŠ•ç¥¨è®¡ç®—é€»è¾‘ (ç›´æ¥å¤ç”¨) ---
        voting_results = {}
        for i, g in enumerate(test_groups): # test_groups ä»ç¼“å­˜å–çš„
            if g not in voting_results: 
                voting_results[g] = {'true': y_test_mapped[i], 'preds': []}
            voting_results[g]['preds'].append(y_pred[i]) # y_pred ä»ç¼“å­˜å–çš„
            
        segment_stats = {cls: {'total': 0, 'correct': 0} for cls in label_map.keys()}
        total_segments = 0
        total_correct = 0

        for res in voting_results.values():
            true_label = res['true']
            vote_pred = np.argmax(np.bincount(res['preds'], minlength=num_classes))
            true_label_name = list(label_map.keys())[list(label_map.values()).index(true_label)]
            
            segment_stats[true_label_name]['total'] += 1
            total_segments += 1
            if vote_pred == true_label:
                segment_stats[true_label_name]['correct'] += 1
                total_correct += 1
                
        segment_acc = total_correct / total_segments if total_segments > 0 else 0
        st.metric("æœ€ç»ˆæ®µçº§å‡†ç¡®ç‡", f"{segment_acc*100:.2f}%")
        
        per_class_data = []
        for cls, stat in segment_stats.items():
            acc = (stat['correct'] / stat['total']) * 100 if stat['total'] > 0 else 0
            per_class_data.append({
                "åŠ¨ä½œID": cls, "æ€»æ•°": stat['total'], "æ­£ç¡®": stat['correct'], "å‡†ç¡®ç‡": f"{acc:.1f}%"
            })
        st.table(pd.DataFrame(per_class_data))

    st.markdown("---")
    st.subheader("4. è®­ç»ƒæ—¥å¿—å½’æ¡£")
    
    # 1. åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = "training_logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
        
    # 2. å‡†å¤‡æ—¥å¿—å†…å®¹
    current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"{log_dir}/log_{current_time}_{model_choice.split(':')[0].strip()}.txt"
    
    # æ”¶é›†æœ€ç»ˆæŒ‡æ ‡
    final_train_acc = history_dict['accuracy'][-1]
    final_val_acc = history_dict['val_accuracy'][-1]
    final_train_loss = history_dict['loss'][-1]
    final_val_loss = history_dict['val_loss'][-1]
    # æ„å»ºæ—¥å¿—æ–‡æœ¬
    log_content = []
    log_content.append(f"========================================")
    log_content.append(f"   EMG è®­ç»ƒå®éªŒæŠ¥å‘Š")
    log_content.append(f"   æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    log_content.append(f"========================================\n")
    opt_info = res.get('optimizer_info', {'name': 'Unknown', 'lr': 0, 'params': {}})
    
    log_content.append(f"[1. æ•°æ®é…ç½®]")
    log_content.append(f"æµ‹è¯•å¯¹è±¡ (Subjects): {selected_subjects}")
    log_content.append(f"æ•°æ®æ—¥æœŸ (Dates): {selected_dates}")
    log_content.append(f"åŠ¨ä½œæ ‡ç­¾ (Labels): {selected_labels}")
    log_content.append(f"æ–‡ä»¶æ€»æ•°: {len(target_files)}")
    log_content.append(f"åˆ‡ç‰‡æ­¥é•¿: {train_stride_ms} ms")
    log_content.append(f"å¢å¼ºé…ç½®: {json.dumps(augment_config, ensure_ascii=False)}\n")
    
    log_content.append(f"[2. æ¨¡å‹ä¸è®­ç»ƒé…ç½®]")
    log_content.append(f"æ¨¡å‹æ¶æ„: {model_choice}")
    log_content.append(f"ä¼˜åŒ–å™¨ (Optimizer): {opt_info['name']}")
    log_content.append(f"å­¦ä¹ ç‡ (Learning Rate): {opt_info['lr']}")
    if opt_info['params']:
        log_content.append(f"ä¼˜åŒ–å™¨å‚æ•° (Params): {json.dumps(opt_info['params'], ensure_ascii=False)}")
    log_content.append(f"éªŒè¯ç­–ç•¥: {selected_strategy}")
    log_content.append(f"Epochs: {epochs}")
    log_content.append(f"Batch Size: {batch_size}")
    log_content.append(f"é«˜çº§ç‰¹æ€§: Voting={use_voting_loss}, Mixup={use_mixup}, Smoothing={label_smoothing}")
    if use_voting_loss:
        log_content.append(f"  - Vote Weight: {voting_weight}")
        log_content.append(f"  - Samples/Group: {samples_per_group}")
        log_content.append(f"  - Start Epoch: {voting_start_epoch}")
    log_content.append("")

    log_content.append(f"[3. è®­ç»ƒç»“æœ (Window Level)]")
    log_content.append(f"Final Train Acc: {final_train_acc*100:.2f}%")
    log_content.append(f"Final Val Acc:   {final_val_acc*100:.2f}%")
    log_content.append(f"Final Train Loss: {final_train_loss:.4f}")
    log_content.append(f"Final Val Loss:   {final_val_loss:.4f}\n")
    
    log_content.append(f"[4. è¯¦ç»†åˆ†ç±»æŠ¥å‘Š (Val Set)]")
    # report_df æ˜¯å‰é¢ç”Ÿæˆçš„ DataFrameï¼Œåˆ©ç”¨ to_string è½¬ä¸ºæ–‡æœ¬è¡¨æ ¼
    log_content.append(report_df.to_string())
    log_content.append("")
    
    if 'segment_acc' in locals():
        log_content.append(f"[5. ç‰‡æ®µçº§è¯„ä¼° (Segment Level)]")
        log_content.append(f"æœ€ç»ˆæ®µçº§å‡†ç¡®ç‡: {segment_acc*100:.2f}%")
        # å°† per_class_data (åˆ—è¡¨) è½¬æ¢ä¸ºç®€å•çš„æ–‡æœ¬è¡¨æ ¼
        log_content.append(f"{'Label':<10} {'Total':<8} {'Correct':<8} {'Acc':<8}")
        for item in per_class_data:
            log_content.append(f"{str(item['åŠ¨ä½œID']):<10} {str(item['æ€»æ•°']):<8} {str(item['æ­£ç¡®']):<8} {item['å‡†ç¡®ç‡']:<8}")
    else:
        log_content.append(f"[5. ç‰‡æ®µçº§è¯„ä¼°]")
        log_content.append("æœªæ‰§è¡Œç‰‡æ®µçº§è¯„ä¼°ã€‚")

    # 3. å†™å…¥æ–‡ä»¶
    try:
        with open(log_filename, "w", encoding="utf-8") as f:
            f.write("\n".join(log_content))
        st.success(f"âœ… è®­ç»ƒæ—¥å¿—å·²è‡ªåŠ¨ä¿å­˜è‡³: `{log_filename}`")
        
        # æä¾›ä¸‹è½½æŒ‰é’® (æ–¹ä¾¿è¿œç¨‹æŸ¥çœ‹)
        with open(log_filename, "r", encoding="utf-8") as f:
            st.download_button("ğŸ“¥ ä¸‹è½½æœ¬æ¬¡æ—¥å¿—æ–‡ä»¶", f, file_name=os.path.basename(log_filename))
            
    except Exception as e:
        st.error(f"æ—¥å¿—ä¿å­˜å¤±è´¥: {e}")

if st.session_state['trained_model']: 
    st.markdown("---")
    c1, c2 = st.columns(2)
    save_name = c1.text_input("ä¿å­˜æ–‡ä»¶å", "my_model.keras")
    if c2.button("ä¿å­˜æ¨¡å‹"):
        try:
            st.session_state['trained_model'].save(save_name)
            st.success(f"å·²ä¿å­˜è‡³ {save_name}")
        except Exception as e:
            st.error(f"ä¿å­˜å¤±è´¥: {e}")

# è¿™ä¸ª elif æ˜¯ä¸ºäº†å¤„ç†è¿˜æ²¡ç‚¹å¼€å§‹çš„æƒ…å†µï¼Œä¹Ÿå¿…é¡»é¡¶æ ¼
elif run_btn and not target_files:
    st.warning("è¯·é€‰æ‹©æ•°æ®ï¼")
