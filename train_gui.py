import streamlit as st
import os
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import pandas as pd

# --- æ¨¡å—å¯¼å…¥ ---
import data_loader
import train_utils
import ui_helper
from model import build_simple_cnn, build_advanced_crnn

# ================= 0. å…¨å±€è®¾ç½® =================
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

st.set_page_config(layout="wide", page_title="EMG è®­ç»ƒå·¥ä½œç«™")

# åˆå§‹åŒ– Session State
if 'trained_model' not in st.session_state:
    st.session_state['trained_model'] = None

# ================= 1. ä¾§è¾¹æ é…ç½® =================
st.sidebar.header("ğŸš€ è®­ç»ƒæ¨¡å¼")
train_mode = st.sidebar.radio("é€‰æ‹©æ¨¡å¼", ["ä»é›¶å¼€å§‹è®­ç»ƒ", "åŸºäºåŸºæ¨¡å‹å¾®è°ƒ (Few-shot)"])

base_model_path = None
if train_mode == "åŸºäºåŸºæ¨¡å‹å¾®è°ƒ (Few-shot)":
    base_model_path = st.sidebar.file_uploader("ä¸Šä¼ åŸºæ¨¡å‹ (.h5)", type=["h5"])
    num_finetune_samples = st.sidebar.slider("æ¯ä¸ªç±»åˆ«ç”¨äºå¾®è°ƒçš„æ ·æœ¬æ•°", 1, 10, 5)

with st.sidebar:
    st.header("1. æ•°æ®é€‰æ‹©")
    DATA_ROOT = "data"
    
    if not os.path.exists(DATA_ROOT):
        st.error(f"æœªæ‰¾åˆ° {DATA_ROOT} æ–‡ä»¶å¤¹")
        st.stop()
        
    structure, file_map = ui_helper.scan_data_folder(DATA_ROOT)
    
    all_subjects = sorted(structure.keys())
    selected_subjects = ui_helper.render_multiselect_with_all(
        "é€‰æ‹©æµ‹è¯•è€… (Subjects)", all_subjects, 'selected_subjects_key', default_first=True
    )
    
    available_dates = set()
    for s in selected_subjects:
        if s in structure: available_dates.update(structure[s].keys())
    selected_dates = ui_helper.render_multiselect_with_all(
        "é€‰æ‹©æ—¥æœŸ (Dates)", sorted(list(available_dates)), 'selected_dates_key', default_first=True
    )
    
    available_labels = set()
    for s in selected_subjects:
        for d in selected_dates:
            if s in structure and d in structure[s]: available_labels.update(structure[s][d])
    selected_labels = ui_helper.render_multiselect_with_all(
        "é€‰æ‹©åŠ¨ä½œ ID (Labels)", sorted(list(available_labels)), 'selected_labels_key', default_first=True
    )

    st.markdown("---")
    
    target_files = []
    for s in selected_subjects:
        for d in selected_dates:
            for l in selected_labels:
                key = (s, d, l)
                if key in file_map: target_files.extend(file_map[key])
    
    st.info(f"å·²é€‰ä¸­ **{len(target_files)}** ä¸ª CSV æ–‡ä»¶")

    st.header("2. å¢å¼ºä¸è®­ç»ƒé…ç½®")
    
    with st.expander("æ•°æ®å¢å¼ºä¸é‡‡æ ·", expanded=False):
        train_stride_ms = st.slider("åˆ‡ç‰‡æ­¥é•¿ (Stride ms)", 10, 200, 100)
        st.caption("è´Ÿæ ·æœ¬ç­–ç•¥")
        enable_rest = st.checkbox("åŠ å…¥é™æ¯ç±» (Rest, Label 0)", value=True)
        st.caption("å¢å¼ºç­–ç•¥")
        c1, c2 = st.columns(2)
        enable_scaling = c1.checkbox("å¹…åº¦ç¼©æ”¾", value=True)
        enable_noise = c2.checkbox("é«˜æ–¯å™ªå£°", value=True)
        enable_warp = c1.checkbox("æ—¶é—´æ‰­æ›²", value=False)
        enable_shift = c2.checkbox("æ—¶é—´å¹³ç§»", value=False)
        enable_mask = st.checkbox("é€šé“é®æŒ¡", value=False)
        
        aug_multiplier = 1
        if train_mode == "åŸºäºåŸºæ¨¡å‹å¾®è°ƒ (Few-shot)":
            aug_multiplier = st.slider("æ ·æœ¬å€å¢ç³»æ•°", 1, 50, 20)
        
        augment_config = {
            'enable_rest': enable_rest,
            'multiplier': aug_multiplier,
            'enable_scaling': enable_scaling, 
            'enable_noise': enable_noise,
            'enable_warp': enable_warp,
            'enable_shift': enable_shift,
            'enable_mask': enable_mask
        }
        
    st.markdown("---")
    model_type = st.selectbox("é€‰æ‹©æ¨¡å‹æ ¸å¿ƒ", ["Lite: Simple CNN", "Pro: Multi-Scale CRNN"])

    # === æ–°å¢ï¼šæŠ•ç¥¨ Loss é…ç½®åŒº ===
    use_voting_loss = st.checkbox("ğŸ—³ï¸ å¼€å¯æŠ•ç¥¨æœºåˆ¶è¾…åŠ©è®­ç»ƒ (Vote Loss)", value=False, 
                                  help="å¼€å¯åï¼Œè®­ç»ƒå°†ä¸ä»…å…³æ³¨å•åˆ‡ç‰‡å‡†ç¡®ç‡ï¼Œè¿˜ä¼šä¼˜åŒ–æ•´ä¸ªåŠ¨ä½œç‰‡æ®µçš„å¹³å‡é¢„æµ‹ç»“æœã€‚")
    
    voting_weight = 0.0
    samples_per_group = 5
    if use_voting_loss:
        c1, c2 = st.columns(2)
        voting_weight = c1.slider("æŠ•ç¥¨ Loss æƒé‡", 0.1, 0.9, 0.5, help="æƒé‡è¶Šé«˜ï¼Œæ¨¡å‹è¶Šé‡è§†æ•´ç»„çš„ä¸€è‡´æ€§")
        samples_per_group = c2.slider("æ¯ç»„é‡‡æ ·åˆ‡ç‰‡æ•°", 2, 20, 5, help="æ¯æ¬¡ä»ä¸€ä¸ªåŠ¨ä½œä¸­æŠ½å–å¤šå°‘ä¸ªåˆ‡ç‰‡æ¥è®¡ç®—å¹³å‡å€¼")

    st.markdown("---")
    split_mode = st.radio("éªŒè¯ç­–ç•¥", ("1. æ··åˆåˆ‡åˆ†", "2. ç•™æ–‡ä»¶éªŒè¯", "3. ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯"))
    strategy_map = {
        "1. æ··åˆåˆ‡åˆ†": "æ··åˆåˆ‡åˆ† (çœ‹åˆ°æ‰€æœ‰å¤©/äºº)",
        "2. ç•™æ–‡ä»¶éªŒè¯": "ç•™æ–‡ä»¶éªŒè¯ (åŒå¤©/åŒäºº)",
        "3. ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯": "ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯ (æ³›åŒ–èƒ½åŠ›)"
    }
    selected_strategy = strategy_map[split_mode]

    manual_val_target = None
    if "ç•™æ–‡ä»¶" in selected_strategy and target_files:
        file_options = sorted(list(set([os.path.basename(f) for f in target_files])))
        manual_val_target = st.selectbox("ğŸ¯ æŒ‡å®šæµ‹è¯•æ–‡ä»¶", file_options)
    elif "ç•™æ—¥æœŸ" in selected_strategy and target_files:
        group_options = sorted(list(set([os.path.basename(os.path.dirname(f)) for f in target_files])))
        manual_val_target = st.selectbox("ğŸ¯ æŒ‡å®šæµ‹è¯•å¯¹è±¡/æ—¥æœŸ", group_options)

    st.markdown("---") 
    epochs = st.number_input("Epochs", 10, 200, 50)
    batch_size = st.selectbox("Batch Size (Groups if Voting)", [8, 16, 32, 64, 128, 256, 512], index=1)
    test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.01, 0.5, 0.2)
    
    run_btn = st.button("ğŸš€ å¼€å§‹å¤„ç†å¹¶è®­ç»ƒ", type="primary")

# ================= 2. ä¸»é€»è¾‘åŒºåŸŸ =================
st.title("ğŸ§  EMG äº¤äº’å¼è®­ç»ƒç³»ç»Ÿ")

if run_btn and target_files:
    # --- A. æ•°æ®å¤„ç† ---
    st.subheader("1. æ•°æ®é¢„å¤„ç†")
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    X, y, groups = data_loader.process_selected_files(
        target_files, 
        progress_callback=lambda p, t: (progress_bar.progress(p), status_text.text(t)),
        stride_ms=train_stride_ms,
        augment_config=augment_config
    )
    
    status_text.text("å¤„ç†å®Œæˆï¼")
    progress_bar.progress(100)
    
    if len(X) == 0:
        st.error("æ ·æœ¬æ•°ä¸º 0ï¼Œè¯·æ£€æŸ¥æ•°æ®ã€‚")
        st.stop()
        
    st.success(f"X={X.shape}, y={y.shape} | ç±»åˆ«: {np.unique(y)}")
    
    # --- B. æ¨¡å‹è®­ç»ƒå‡†å¤‡ ---
    st.subheader("2. æ¨¡å‹è®­ç»ƒ")
    
    if train_mode == "åŸºäºåŸºæ¨¡å‹å¾®è°ƒ (Few-shot)":
        train_idx, test_idx = train_utils.get_few_shot_split(X, y, num_finetune_samples)
    else:
        train_idx, test_idx = train_utils.smart_split(
            X, y, groups, selected_strategy, test_size, manual_target=manual_val_target
        )

    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    groups_train = groups[train_idx] # è·å–è®­ç»ƒé›†çš„ç»„ä¿¡æ¯ï¼Œç”¨äºæŠ•ç¥¨è®­ç»ƒ
    
    unique_labels = np.unique(y)
    num_classes = len(unique_labels) 
    label_map = {original: new for new, original in enumerate(unique_labels)}
    y_train_mapped = np.array([label_map[i] for i in y_train])
    y_test_mapped = np.array([label_map[i] for i in y_test])

    # æ„å»ºæ¨¡å‹
    if train_mode == "åŸºäºåŸºæ¨¡å‹å¾®è°ƒ (Few-shot)":
        if base_model_path:
            with open("temp_model.h5", "wb") as f: f.write(base_model_path.getbuffer())
            base_model = tf.keras.models.load_model("temp_model.h5")
            base_model.trainable = False 
            
            feature_output = None
            for layer in reversed(base_model.layers):
                if "global_average_pooling" in layer.name or "flatten" in layer.name:
                    feature_output = layer.output
                    break
            if feature_output is None: feature_output = base_model.layers[-3].output
            
            x = feature_output
            x = tf.keras.layers.Dropout(0.5, name="ft_dropout_1")(x) 
            x = tf.keras.layers.Dense(64, activation='relu', 
                                      kernel_regularizer=tf.keras.regularizers.l2(0.01),
                                      name="ft_dense_1")(x)
            x = tf.keras.layers.Dropout(0.3, name="ft_dropout_2")(x)
            outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)
            
            model = tf.keras.models.Model(inputs=base_model.input, outputs=outputs)
            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
        else:
            st.error("è¯·ä¸Šä¼ åŸºæ¨¡å‹ (.h5 æ–‡ä»¶)")
            st.stop()
    else:
        input_shape = (X.shape[1], X.shape[2])
        if "Lite" in model_type:
            model = build_simple_cnn(input_shape, num_classes)
        else:
            model = build_advanced_crnn(input_shape, num_classes)
        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    # --- C. å¼€å§‹è®­ç»ƒ (åˆ†æ”¯é€»è¾‘) ---
    st.caption("è®­ç»ƒç›‘æ§")
    train_progress = st.progress(0)
    train_status = st.empty()

    if use_voting_loss:
        st.info(f"ğŸ”µ æŠ•ç¥¨è®­ç»ƒæ¨¡å¼å·²æ¿€æ´» (Weight={voting_weight}, Samples/Group={samples_per_group})")
        
        # è°ƒç”¨æˆ‘ä»¬åœ¨ train_utils ä¸­æ–°å†™çš„è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯
        history_dict = train_utils.train_with_voting_mechanism(
            model, X_train, y_train_mapped, groups_train,
            X_test, y_test_mapped,
            epochs=epochs,
            batch_size=batch_size,
            samples_per_group=samples_per_group,
            vote_weight=voting_weight,
            st_progress_bar=train_progress,
            st_status_text=train_status
        )
        
        # ä¼ªè£…æˆ Keras history å¯¹è±¡ä»¥ä¾¿åé¢ç”»å›¾ä»£ç å¤ç”¨
        class HistoryShim:
            def __init__(self, h_dict): self.history = h_dict
        history = HistoryShim(history_dict)
        
    else:
        # æ ‡å‡†è®­ç»ƒæ¨¡å¼
        st_callback = train_utils.StreamlitKerasCallback(epochs, train_progress, train_status)
        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)

        history = model.fit(
            X_train, y_train_mapped,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(X_test, y_test_mapped),
            callbacks=[EarlyStopping(patience=10, restore_best_weights=True), reduce_lr, st_callback],
            verbose=0
        )
    
    st.success("è®­ç»ƒå®Œæˆï¼")
    
# --- D. ç»“æœå¯è§†åŒ– (åŸºç¡€æ›²çº¿) ---
    col1, col2 = st.columns(2)
    with col1:
        fig, ax = plt.subplots()
        ax.plot(history.history['accuracy'], label='Train')
        ax.plot(history.history['val_accuracy'], label='Val')
        ax.set_title("Window Level Accuracy")
        ax.legend()
        st.pyplot(fig)
    with col2:
        fig, ax = plt.subplots()
        ax.plot(history.history['loss'], label='Train')
        ax.plot(history.history['val_loss'], label='Val')
        ax.set_title("Loss Curve")
        ax.legend()
        st.pyplot(fig)
    
    # --- E. æ·±åº¦è¯„ä¼°æŠ¥å‘Š  ---
    st.markdown("---")
    st.subheader("3. æ·±åº¦è¯„ä¼°æŠ¥å‘Š")

    # 1. å‡†å¤‡é¢„æµ‹æ•°æ®
    # è·å–åˆ‡ç‰‡çº§é¢„æµ‹
    y_pred_probs = model.predict(X_test)
    y_pred = np.argmax(y_pred_probs, axis=1)
    
    # 2. æ··æ·†çŸ©é˜µ (Confusion Matrix)
    st.write("#### (1) æ··æ·†çŸ©é˜µ (Confusion Matrix)")
    st.caption("æ¨ªè½´ä¸ºé¢„æµ‹ç±»åˆ«ï¼Œçºµè½´ä¸ºçœŸå®ç±»åˆ«ã€‚å¯¹è§’çº¿é¢œè‰²è¶Šæ·±è¶Šå¥½ã€‚")
    
    cm = confusion_matrix(y_test_mapped, y_pred)
    class_names = [str(k) for k in label_map.keys()] # è·å–ç±»åˆ«åç§°
    
    fig_cm, ax_cm = plt.subplots(figsize=(8, 6))
    try:
        # å°è¯•ä½¿ç”¨ Seaborn ç»˜åˆ¶æ¼‚äº®çš„çƒ­åŠ›å›¾
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=class_names, 
                    yticklabels=class_names, ax=ax_cm)
    except:
        # å¦‚æœæ²¡æœ‰å®‰è£… seabornï¼Œä½¿ç”¨ matplotlib å…œåº•
        cax = ax_cm.matshow(cm, cmap='Blues')
        fig_cm.colorbar(cax)
        for (i, j), z in np.ndenumerate(cm):
            ax_cm.text(j, i, '{:0.1f}'.format(z), ha='center', va='center')
        ax_cm.set_xticklabels([''] + class_names)
        ax_cm.set_yticklabels([''] + class_names)
    
    ax_cm.set_xlabel('Predicted Label')
    ax_cm.set_ylabel('True Label')
    st.pyplot(fig_cm)

    # 3. è¯¦ç»†åˆ†ç±»æŒ‡æ ‡ (Classification Report)
    st.write("#### (2) è¯¦ç»†åˆ†ç±»æŒ‡æ ‡")
    report_dict = classification_report(y_test_mapped, y_pred, 
                                        target_names=class_names, 
                                        output_dict=True)
    # è½¬ä¸º DataFrame å¹¶é«˜äº®æ˜¾ç¤º
    report_df = pd.DataFrame(report_dict).transpose()
    st.dataframe(report_df.style.background_gradient(cmap='Greens', subset=['f1-score']))

    # 4. åŸºäºæŠ•ç¥¨çš„â€œåˆ†åŠ¨ä½œâ€å‡†ç¡®ç‡ (Per-Class Segment Accuracy)
    st.write("#### (3) ğŸ—³ï¸ åŠ¨ä½œç‰‡æ®µçº§æŠ•ç¥¨è¯¦æƒ… (Segment Level Analysis)")
    
    # --- æŠ•ç¥¨é€»è¾‘ ---
    test_groups = groups[test_idx]
    voting_results = {}
    
    # æ”¶é›†æ¯ä¸ªç‰‡æ®µçš„ç¥¨æ•°
    for i, g in enumerate(test_groups):
        if g not in voting_results: 
            voting_results[g] = {'true': y_test_mapped[i], 'preds': []}
        voting_results[g]['preds'].append(y_pred[i])
    
    # ç»Ÿè®¡ç»“æœ
    segment_stats = {} # è®°å½•æ¯ä¸ªç±»åˆ«çš„ {total: 0, correct: 0}
    for cls in label_map.keys():
        segment_stats[cls] = {'total': 0, 'correct': 0}

    total_segments = 0
    total_correct = 0

    for res in voting_results.values():
        true_label = res['true']
        # æ‰¾åˆ°ç¥¨æ•°æœ€å¤šçš„ç±»åˆ«
        vote_pred = np.argmax(np.bincount(res['preds'], minlength=num_classes))
        
        # è½¬æ¢å›åŸå§‹ Label åç§°ä»¥ä¾¿ç»Ÿè®¡
        true_label_name = list(label_map.keys())[list(label_map.values()).index(true_label)]
        
        segment_stats[true_label_name]['total'] += 1
        total_segments += 1
        if vote_pred == true_label:
            segment_stats[true_label_name]['correct'] += 1
            total_correct += 1
            
    # è®¡ç®—æ€»æŠ•ç¥¨å‡†ç¡®ç‡
    segment_acc = total_correct / total_segments if total_segments > 0 else 0
    
    # æ˜¾ç¤ºå¤§å­—æŒ‡æ ‡
    st.metric(" æœ€ç»ˆæ®µçº§å‡†ç¡®ç‡ (Segment Accuracy)", f"{segment_acc*100:.2f}%", 
              help="è¿™æ˜¯å®é™…ä½¿ç”¨æ—¶çš„é¢„æœŸå‡†ç¡®ç‡ï¼ˆç»è¿‡æŠ•ç¥¨ä¿®æ­£åï¼‰")
    
    # æ˜¾ç¤ºåˆ†åŠ¨ä½œè¯¦æƒ…è¡¨
    st.caption("ğŸ‘‡ æ¯ä¸ªåŠ¨ä½œç‹¬ç«‹è¡¨ç°ï¼š")
    per_class_data = []
    for cls, stat in segment_stats.items():
        acc = (stat['correct'] / stat['total']) * 100 if stat['total'] > 0 else 0
        per_class_data.append({
            "åŠ¨ä½œID (Label)": cls,
            "ç‰‡æ®µæ€»æ•°": stat['total'],
            "æ­£ç¡®è¯†åˆ«æ•°": stat['correct'],
            "å‡†ç¡®ç‡ (%)": f"{acc:.1f}%"
        })
    
    st.table(pd.DataFrame(per_class_data))

# --- F. æ¨¡å‹ä¿å­˜ ---
if st.session_state['trained_model']:
    st.markdown("---")
    c1, c2 = st.columns(2)
    save_name = c1.text_input("ä¿å­˜æ–‡ä»¶å", "my_model.h5")
    if c2.button("ä¿å­˜æ¨¡å‹"):
        st.session_state['trained_model'].save(save_name)
        st.success(f"å·²ä¿å­˜è‡³ {save_name}")

elif run_btn and not target_files:
    st.warning("è¯·é€‰æ‹©æ•°æ®ï¼")
