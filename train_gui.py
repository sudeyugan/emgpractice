import streamlit as st
import os
import tensorflow as tf

# è·å–æ‰€æœ‰å¯è§çš„ç‰©ç† GPU è®¾å¤‡
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        # è®¾ç½®æ˜¾å­˜æŒ‰éœ€å¢é•¿
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print("å·²å¼€å¯æ˜¾å­˜æŒ‰éœ€å¢é•¿æ¨¡å¼")
    except RuntimeError as e:
        print(e)
import glob
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import GroupShuffleSplit

# å¼•å…¥æˆ‘ä»¬è‡ªå®šä¹‰çš„æ¨¡å—
import data_loader
from model import build_simple_cnn, build_advanced_crnn

st.set_page_config(layout="wide", page_title="EMG è®­ç»ƒå·¥ä½œç«™")

# ================= 1. æ–‡ä»¶æ‰«æé€»è¾‘ =================
@st.cache_data
def scan_data_folder(root_dir):
    """æ‰«ææ–‡ä»¶å¤¹ï¼Œæ„å»º Subject -> Date -> Labels ç»“æ„"""
    structure = {}
    file_map = {} # å­˜å‚¨ label -> file_path listï¼Œç”¨äºå¿«é€Ÿæ£€ç´¢
    
    # æŸ¥æ‰¾æ‰€æœ‰ RAW_EMG æ–‡ä»¶
    pattern = os.path.join(root_dir, "*", "*", "RAW_EMG*.csv")
    files = glob.glob(pattern)
    
    for f in files:
        subject, date, label, fname = data_loader.parse_filename_info(f)
        if label is None: continue
        
        if subject not in structure: structure[subject] = {}
        if date not in structure[subject]: structure[subject][date] = set()
        
        structure[subject][date].add(label)
        
        # æ„å»ºç´¢å¼•é”®
        key = (subject, date, label)
        if key not in file_map: file_map[key] = []
        file_map[key].append(f)
        
    return structure, file_map

def smart_split(X, y, groups, strategy, test_size=0.2):
    """
    groups: è¿™é‡Œçš„ groups ä¼ å…¥çš„æ˜¯æ–‡ä»¶ååˆ—è¡¨ (æ¯ä¸ªæ ·æœ¬å¯¹åº”çš„æ–‡ä»¶å)
    """
    indices = np.arange(len(X))
    train_idx, test_idx = [], []
    
    unique_files = np.unique(groups)
    
    # --- ç­–ç•¥ 1: æ··åˆå¤§ä¹±ç‚– (File-Dependent / Intra-File) ---
    # é€»è¾‘ï¼šæ¯ä¸ªæ–‡ä»¶éƒ½åˆ‡ä¸€åˆ€ã€‚å¦‚æœä½ æœ‰Day1å’ŒDay2ï¼Œå®ƒä»¬éƒ½ä¼šè¢«åˆ‡åˆ†è¿›å…¥è®­ç»ƒé›†ã€‚
    # è§£å†³äº†ä½ çš„ç–‘æƒ‘ï¼šè¿™æ ·æ¨¡å‹å°±èƒ½å­¦åˆ°Day1å’ŒDay2çš„ç‰¹å¾äº†ã€‚
    if strategy == "æ··åˆåˆ‡åˆ† (çœ‹åˆ°æ‰€æœ‰å¤©/äºº)":
        for f in unique_files:
            # æ‰¾åˆ°å±äºè¿™ä¸ªæ–‡ä»¶çš„æ‰€æœ‰æ ·æœ¬
            f_indices = indices[groups == f]
            # å¿…é¡»æŒ‰æ—¶é—´é¡ºåºåˆ‡ï¼Œé˜²æ­¢æ»‘åŠ¨çª—å£æ³„éœ²
            split_point = int(len(f_indices) * (1 - test_size))
            
            # å‰é¢åšè®­ç»ƒï¼Œåé¢åšæµ‹è¯•
            train_idx.extend(f_indices[:split_point])
            test_idx.extend(f_indices[split_point:])
            
    # --- ç­–ç•¥ 2: ä¸¥æ ¼ç•™ä¸€æ–‡ä»¶ (Leave-One-File-Out) ---
    # é€»è¾‘ï¼šéšæœºé€‰å‡ ä¸ªæ–‡ä»¶åšæµ‹è¯•é›†ã€‚
    # é€‚ç”¨ï¼šåŒä¸€ä¸ªäººï¼ŒåŒä¸€ç§åŠ¨ä½œï¼Œå½•äº†5æ¬¡ï¼Œæƒ³çœ‹çœ‹ç¬¬6æ¬¡èƒ½ä¸èƒ½è¯†åˆ«ã€‚
    elif strategy == "ç•™æ–‡ä»¶éªŒè¯ (åŒå¤©/åŒäºº)":
        gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=42)
        # è¿™é‡ŒæŒ‰â€œæ–‡ä»¶åâ€åˆ†ç»„
        train_i, test_i = next(gss.split(X, y, groups=groups))
        train_idx, test_idx = indices[train_i], indices[test_i]

    # --- ç­–ç•¥ 3: ä¸¥æ ¼ç•™ä¸€æ—¥æœŸ/å¯¹è±¡ (Leave-Group-Out) ---
    # é€»è¾‘ï¼šè§£ææ–‡ä»¶åä¸­çš„ Date æˆ– Subjectï¼Œå®Œå…¨æ‰£é™¤ä¸€ç»„ã€‚
    # é€‚ç”¨ï¼šè·¨å¤©æµ‹è¯•ï¼ˆæå®¢æ¨¡å¼ï¼‰ï¼Œè·¨äººæµ‹è¯•ã€‚
    elif strategy == "ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯ (æ³›åŒ–èƒ½åŠ›)":
        # æˆ‘ä»¬éœ€è¦ä» groups (æ–‡ä»¶å) ä¸­æå–å‡ºæ—¥æœŸæˆ–äººå
        # å‡è®¾æ–‡ä»¶åæ ¼å¼åŒ…å«è·¯å¾„ï¼š data/Subject/Date/...
        # æˆ‘ä»¬å¯ä»¥ç®€åŒ–é€»è¾‘ï¼šè®© GUI ä¼ è¿›æ¥æ›´é«˜çº§çš„ group_labelsï¼Œæˆ–è€…åœ¨è¿™é‡Œè§£æ
        
        # ç®€æ˜“å®ç°ï¼šè¿™é‡Œæˆ‘ä»¬å‡è®¾ groups åˆ—è¡¨é‡Œå­˜çš„æ˜¯ full path
        # æå–ä¸Šä¸€çº§ç›®å½•åä½œä¸º Group ID (é€šå¸¸æ˜¯ Date æˆ– Subject)
        real_groups = [os.path.basename(os.path.dirname(f)) for f in groups]
        
        gss = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=42)
        train_i, test_i = next(gss.split(X, y, groups=real_groups))
        train_idx, test_idx = indices[train_i], indices[test_i]
        
    return np.array(train_idx), np.array(test_idx)

# ================= ç•Œé¢å¸ƒå±€ =================

st.title("ğŸ§  EMG äº¤äº’å¼è®­ç»ƒç³»ç»Ÿ")

with st.sidebar:
    st.header("1. æ•°æ®é€‰æ‹©")
    DATA_ROOT = "data"
    
    if not os.path.exists(DATA_ROOT):
        st.error(f"æœªæ‰¾åˆ° {DATA_ROOT} æ–‡ä»¶å¤¹")
        st.stop()
        
    structure, file_map = scan_data_folder(DATA_ROOT)
    
# --- çº§è”é€‰æ‹©å™¨ ---
    # 1. é€‰æ‹©å¯¹è±¡ (Subject)
    # é€»è¾‘ï¼šé»˜è®¤é€‰ä¸­ç¬¬ä¸€ä¸ª
    all_subjects = sorted(structure.keys())
    selected_subjects = st.multiselect(
        "é€‰æ‹©æµ‹è¯•è€… (Subjects)", 
        all_subjects, 
        default=all_subjects[:1] # ä¿æŒä¸å˜ï¼Œå·²ç»æ˜¯é»˜è®¤é€‰ç¬¬ä¸€ä¸ª
    )
    
    # 2. é€‰æ‹©æ—¥æœŸ (Date) - åŸºäºé€‰ä¸­çš„å¯¹è±¡
    available_dates = set()
    for s in selected_subjects:
        if s in structure:
            available_dates.update(structure[s].keys())
    
    # æ’åºæ—¥æœŸåˆ—è¡¨
    sorted_dates = sorted(list(available_dates))
    
    # ä¿®æ”¹ç‚¹ï¼šdefault=sorted_dates[:1] è¡¨ç¤ºé»˜è®¤åªé€‰ç¬¬ä¸€ä¸ª
    selected_dates = st.multiselect(
        "é€‰æ‹©æ—¥æœŸ (Dates)", 
        sorted_dates, 
        default=sorted_dates[:1] 
    )
    
    # 3. é€‰æ‹©åŠ¨ä½œ (Labels) - åŸºäºé€‰ä¸­çš„å¯¹è±¡å’Œæ—¥æœŸ
    available_labels = set()
    for s in selected_subjects:
        for d in selected_dates:
            if s in structure and d in structure[s]:
                available_labels.update(structure[s][d])
    
    # æ’åºæ ‡ç­¾åˆ—è¡¨
    sorted_labels = sorted(list(available_labels))
    
    # ä¿®æ”¹ç‚¹ï¼šdefault=sorted_labels[:1] è¡¨ç¤ºé»˜è®¤åªé€‰ç¬¬ä¸€ä¸ª
    selected_labels = st.multiselect(
        "é€‰æ‹©åŠ¨ä½œ ID (Labels)", 
        sorted_labels, 
        default=sorted_labels[:1]
    )

    st.markdown("---")
    
    # ç»Ÿè®¡é€‰ä¸­æ–‡ä»¶
    target_files = []
    for s in selected_subjects:
        for d in selected_dates:
            for l in selected_labels:
                key = (s, d, l)
                if key in file_map:
                    target_files.extend(file_map[key])
    
    st.info(f"å·²é€‰ä¸­ **{len(target_files)}** ä¸ª CSV æ–‡ä»¶")
    
    st.header("2. è®­ç»ƒé…ç½®")
    st.markdown("##### ğŸ§  æ¨¡å‹æ¶æ„é€‰æ‹©")
    model_type = st.selectbox(
        "é€‰æ‹©æ¨¡å‹æ ¸å¿ƒ",
        ["Lite: Simple CNN (æ¨èå•äºº)", "Pro: Multi-Scale CRNN (æ¨èå¤šäºº/è·¨å¤©)"],
        index=0,
        help="Liteç‰ˆï¼šè®­ç»ƒå¿«ï¼Œé€‚åˆå°æ ·æœ¬ï¼›Proç‰ˆï¼šæŠ—å¹²æ‰°å¼ºï¼Œéœ€è¦è¾ƒå¤šæ•°æ®ã€‚"
    )

    st.markdown("##### ğŸ§ª éªŒè¯ç­–ç•¥é€‰æ‹©")
    split_mode = st.radio(
        "ä½ æƒ³æ€ä¹ˆéªŒè¯æ¨¡å‹ï¼Ÿ",
        (
            "1. æ··åˆåˆ‡åˆ† (æ¨è)", 
            "2. ç•™æ–‡ä»¶éªŒè¯ (è¿›é˜¶)",
            "3. ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯ (é«˜éš¾)"
        ),
        index=0,
        help="æ··åˆåˆ‡åˆ†ï¼šå‡†ç¡®ç‡æœ€é«˜ï¼Œé€‚åˆç»å¤§å¤šæ•°æƒ…å†µã€‚\nç•™æ—¥æœŸéªŒè¯ï¼šéªŒè¯æ¨¡å‹æ˜¯å¦éœ€è¦æ¯å¤©é‡æ–°è®­ç»ƒã€‚"
    )
    
    # æ˜ å°„åˆ°å‡½æ•°å‚æ•°
    strategy_map = {
        "1. æ··åˆåˆ‡åˆ† (æ¨è)": "æ··åˆåˆ‡åˆ† (çœ‹åˆ°æ‰€æœ‰å¤©/äºº)",
        "2. ç•™æ–‡ä»¶éªŒè¯ (è¿›é˜¶)": "ç•™æ–‡ä»¶éªŒè¯ (åŒå¤©/åŒäºº)",
        "3. ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯ (é«˜éš¾)": "ç•™æ—¥æœŸ/å¯¹è±¡éªŒè¯ (æ³›åŒ–èƒ½åŠ›)"
    }
    selected_strategy = strategy_map[split_mode]


    st.markdown("---") 
    epochs = st.number_input("Epochs", 10, 200, 50)
    batch_size = st.selectbox("Batch Size", [16, 32, 64], index=1)
    test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.2)
    
    run_btn = st.button("ğŸš€ å¼€å§‹å¤„ç†å¹¶è®­ç»ƒ", type="primary")
# ================= ä¸»é€»è¾‘åŒºåŸŸ =================

if run_btn and target_files:
    # --- 1. æ•°æ®å¤„ç†é˜¶æ®µ ---
    st.subheader("1. æ•°æ®é¢„å¤„ç†")
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # è°ƒç”¨ data_loader å¤„ç†æ•°æ®
    X, y, groups = data_loader.process_selected_files(
        target_files, 
        progress_callback=lambda p, t: (progress_bar.progress(p), status_text.text(t))
    )
    
    status_text.text("å¤„ç†å®Œæˆï¼")
    progress_bar.progress(100)
    
    if len(X) == 0:
        st.error("ç”Ÿæˆçš„æ ·æœ¬æ•°ä¸º 0ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«æœ‰æ•ˆåŠ¨ä½œæ•°æ®ã€‚")
        st.stop()
        
    st.success(f"æˆåŠŸç”Ÿæˆæ ·æœ¬æ•°æ®: X={X.shape}, y={y.shape}")
    st.write(f"åŒ…å«åŠ¨ä½œç±»åˆ«: {np.unique(y)}")
    
    # --- 2. è®­ç»ƒé˜¶æ®µ ---
    st.subheader("2. æ¨¡å‹è®­ç»ƒ")
    
    train_idx, test_idx = smart_split(X, y, groups, selected_strategy, test_size)
    
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    st.info(f"æ•°æ®é›†åˆ’åˆ†ç»“æœ ({selected_strategy}):\n"
            f"- è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬\n"
            f"- æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬")

    # é‡æ–°æ˜ å°„æ ‡ç­¾ (ä¿æŒä¸å˜)
    unique_labels = np.unique(y)
    label_map = {original: new for new, original in enumerate(unique_labels)}
    y_train_mapped = np.array([label_map[i] for i in y_train])
    y_test_mapped = np.array([label_map[i] for i in y_test])
    
    # æ£€æŸ¥æµ‹è¯•é›†æ˜¯å¦åŒ…å«è®­ç»ƒé›†æ²¡æœ‰çš„æ ‡ç­¾ (é˜²æ­¢æŠ¥é”™)
    if len(np.unique(y_test)) < len(unique_labels) and "è·¨æ–‡ä»¶" in selected_strategy:
        st.warning("âš ï¸ æ³¨æ„ï¼šæµ‹è¯•é›†ä¸­æŸäº›åŠ¨ä½œç±»åˆ«å¯èƒ½ç¼ºå¤±ï¼Œè¿™é€šå¸¸æ˜¯å› ä¸ºé€‰ä¸­çš„æ–‡ä»¶å¤ªå°‘ï¼Œå¯¼è‡´æŒ‰æ–‡ä»¶åˆ‡åˆ†æ—¶æŠŠæŸä¸ªåŠ¨ä½œçš„æ‰€æœ‰æ–‡ä»¶éƒ½åˆ†åˆ°äº†è®­ç»ƒé›†ã€‚")
    
    num_classes = len(unique_labels)
    
    st.subheader(f"æ­£åœ¨æ„å»ºæ¨¡å‹: {model_type.split(':')[0]}")
    
    input_shape = (X.shape[1], X.shape[2])
    
    # === æ ¹æ®é€‰æ‹©è°ƒç”¨ä¸åŒçš„æ¨¡å‹æ„å»ºå‡½æ•° ===
    if "Lite" in model_type:
        model = build_simple_cnn(input_shape=input_shape, num_classes=num_classes)
        st.caption("å·²åŠ è½½ Simple CNNï¼šç»“æ„è½»é‡ï¼Œä¸“æ³¨å±€éƒ¨ç‰¹å¾ã€‚")
    else:
        model = build_advanced_crnn(input_shape=input_shape, num_classes=num_classes)
        st.caption("å·²åŠ è½½ Multi-Scale CRNNï¼šå¤šå°ºåº¦è§†é‡ + æ—¶åºè®°å¿†ã€‚")
        
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    
    # è®­ç»ƒå›è°ƒ (æ˜¾ç¤ºè®­ç»ƒè¿‡ç¨‹)
    with st.spinner("æ­£åœ¨è®­ç»ƒæ¨¡å‹..."):
        history = model.fit(
            X_train, y_train_mapped,
            epochs=epochs,
            batch_size=batch_size,
            validation_data=(X_test, y_test_mapped),
            callbacks=[EarlyStopping(patience=10, restore_best_weights=True)],
            verbose=0 # éšè—æ§åˆ¶å°è¾“å‡º
        )
    
    st.success("è®­ç»ƒå®Œæˆï¼")
    
    # --- 3. ç»“æœå¯è§†åŒ– ---
    st.subheader("3. è®­ç»ƒç»“æœ")
    
    col1, col2 = st.columns(2)
    
    # å‡†ç¡®ç‡æ›²çº¿
    with col1:
        fig1, ax1 = plt.subplots()
        ax1.plot(history.history['accuracy'], label='Train Acc')
        ax1.plot(history.history['val_accuracy'], label='Val Acc')
        ax1.set_title("Accuracy Curve")
        ax1.set_xlabel("Epoch")
        ax1.set_ylabel("Accuracy")
        ax1.legend()
        st.pyplot(fig1)
        
    # æŸå¤±æ›²çº¿
    with col2:
        fig2, ax2 = plt.subplots()
        ax2.plot(history.history['loss'], label='Train Loss')
        ax2.plot(history.history['val_loss'], label='Val Loss')
        ax2.set_title("Loss Curve")
        ax2.set_xlabel("Epoch")
        ax2.set_ylabel("Loss")
        ax2.legend()
        st.pyplot(fig2)
    
    # æœ€ç»ˆè¯„ä¼°
    loss, acc = model.evaluate(X_test, y_test_mapped, verbose=0)
    st.metric("æœ€ç»ˆæµ‹è¯•é›†å‡†ç¡®ç‡ (Test Accuracy)", f"{acc*100:.2f}%")
    
    # ä¿å­˜æ¨¡å‹é€‰é¡¹
    if st.button("ğŸ’¾ ä¿å­˜å½“å‰æ¨¡å‹"):
        model.save("custom_selection_model.h5")
        st.toast("æ¨¡å‹å·²ä¿å­˜ä¸º custom_selection_model.h5")

elif run_btn and not target_files:
    st.warning("è¯·å…ˆåœ¨å·¦ä¾§é€‰æ‹©æ•°æ®ï¼")