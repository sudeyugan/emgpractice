import os
import sys
import time
import glob
import datetime
import re
import numpy as np
import pandas as pd
import scipy.signal as signal
import scipy.ndimage as ndimage
import tensorflow as tf
from sklearn.metrics import classification_report
from sklearn.model_selection import GroupShuffleSplit

# å¼•ç”¨ç°æœ‰æ¨¡å— (ç¡®ä¿è¿™äº›æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹)
import train_utils
import model as model_lib

def time_warp(data, sigma=0.2, knot=4):
    """æ—¶é—´æ‰­æ›²"""
    orig_steps = np.arange(data.shape[0])
    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, data.shape[1]))
    ret = np.zeros_like(data)
    for i in range(data.shape[1]):
        time_warp = np.interp(orig_steps, np.linspace(0, data.shape[0]-1., num=knot+2), random_warps[:, i])
        cum_warp = np.cumsum(time_warp)
        scale = (data.shape[0]-1) / cum_warp[-1]
        new_times = cum_warp * scale
        ret[:, i] = np.interp(orig_steps, new_times, data[:, i])
    return ret

def time_shift(data, shift_limit=0.1):
    """æ—¶é—´å¹³ç§»"""
    shift_amt = int(data.shape[0] * shift_limit * np.random.uniform(-1, 1))
    return np.roll(data, shift_amt, axis=0)

def channel_mask(data, mask_prob=0.15):
    """é€šé“é®æŒ¡"""
    temp = data.copy()
    if np.random.random() < mask_prob:
        c = np.random.randint(0, data.shape[1])
        temp[:, c] = 0
    return temp

def load_and_resample_imu(emg_filepath, target_length):
    """
    æ ¹æ® EMG æ–‡ä»¶è·¯å¾„æŸ¥æ‰¾å¯¹åº”çš„ IMU æ–‡ä»¶ï¼Œå¹¶å°†å…¶é‡é‡‡æ ·åˆ° target_length (1000Hz)
    """
    # å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸€è‡´ï¼Œåªæ˜¯å‰ç¼€ä¸åŒï¼šRAW_EMG_... -> RAW_IMU_...
    imu_filepath = emg_filepath.replace("RAW_EMG", "RAW_IMU")
    
    if not os.path.exists(imu_filepath):
        # å°è¯•å¦ä¸€ç§å¸¸è§æƒ…å†µï¼šå¦‚æœæ–‡ä»¶å¤¹ç»“æ„ä¸åŒï¼Œå¯èƒ½éœ€è¦æ›´å¤æ‚çš„æŸ¥æ‰¾
        # è¿™é‡Œå‡è®¾å®ƒä»¬åœ¨åŒä¸€ç›®å½•ä¸‹
        return None

    try:
        df = pd.read_csv(imu_filepath)
        # ç¡®ä¿åˆ—åæ­£ç¡®ï¼Œæ ¹æ®ä½ æä¾›çš„æ–‡ä»¶å†…å®¹: AX,AY,AZ,GX,GY,GZ
        required_cols = ['AX', 'AY', 'AZ', 'GX', 'GY', 'GZ']
        
        # æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨
        if not all(col in df.columns for col in required_cols):
            print(f"Warning: IMU file {os.path.basename(imu_filepath)} missing columns.")
            return None
            
        imu_data = df[required_cols].values # Shape: (N_imu, 6)
        
        # --- é‡é‡‡æ ·é€»è¾‘ (200Hz -> 1000Hz) ---
        # ä½¿ç”¨çº¿æ€§æ’å€¼å°† IMU æ•°æ®æ‹‰ä¼¸åˆ°ä¸ EMG æ•°æ®ç›¸åŒçš„é•¿åº¦ (target_length)
        x_old = np.linspace(0, 1, len(imu_data))
        x_new = np.linspace(0, 1, target_length)
        
        imu_resampled = np.zeros((target_length, 6))
        for i in range(6):
            imu_resampled[:, i] = np.interp(x_new, x_old, imu_data[:, i])
            
        return imu_resampled

    except Exception as e:
        print(f"Error loading IMU {imu_filepath}: {e}")
        return None

def augment_dataset_in_memory(X, y, groups, config):
    """
    åœ¨å†…å­˜ä¸­å¯¹æ•°æ®é›†è¿›è¡Œå¢å¼ºã€‚
    X: (N, Time, Channels)
    """
    multiplier = config.get('multiplier', 1)
    if multiplier <= 1:
        return X, y, groups
    
    print(f"    âš¡ æ­£åœ¨æ‰§è¡Œå†…å­˜å¢å¼º (å€ç‡: {multiplier}x)...")
    
    X_aug_list, y_aug_list, groups_aug_list = [], [], []
    total = len(X)
    
    # æå–é…ç½®
    enable_warp = config.get('enable_warp', False)
    enable_shift = config.get('enable_shift', False)
    enable_scale = config.get('enable_scaling', False)
    enable_mask = config.get('enable_mask', False)
    enable_noise = config.get('enable_noise', False)
    
    for i in range(total):
        # 1. å§‹ç»ˆä¿ç•™åŸå§‹æ ·æœ¬
        X_aug_list.append(X[i])
        y_aug_list.append(y[i])
        groups_aug_list.append(groups[i])
        
        # 2. ç”Ÿæˆ (multiplier - 1) ä¸ªå¢å¼ºå‰¯æœ¬
        for _ in range(multiplier - 1):
            aug_x = X[i].copy() # å¿…é¡» copy
            
            # æ¦‚ç‡åº”ç”¨å¢å¼º (å‚æ•°å¯æ ¹æ®éœ€è¦å¾®è°ƒ)
            if enable_warp and np.random.random() > 0.5:
                aug_x = time_warp(aug_x)
            
            if enable_shift and np.random.random() > 0.5:
                aug_x = time_shift(aug_x)
                
            if enable_scale and np.random.random() > 0.3:
                # ç®€å•çš„å¹…åº¦ç¼©æ”¾å®ç°
                factor = np.random.uniform(0.8, 1.2)
                aug_x = aug_x * factor
                
            if enable_mask and np.random.random() > 0.7:
                aug_x = channel_mask(aug_x)
                
            if enable_noise: 
                # é«˜æ–¯å™ªå£°
                noise = np.random.normal(0, 0.02, aug_x.shape)
                aug_x = aug_x + noise
            
            X_aug_list.append(aug_x)
            y_aug_list.append(y[i])
            # å¢å¼ºæ ·æœ¬å…±äº«åŒä¸€ä¸ª Group IDï¼Œç¡®ä¿å®ƒä»¬æ€»æ˜¯è¢«åˆ†åœ¨ä¸€èµ·ï¼ˆè™½ç„¶è¿™é‡Œæ˜¯åœ¨ split åå¢å¼ºï¼Œä½†ä¿æŒ ID ä¸€è‡´æ˜¯ä¸ªå¥½ä¹ æƒ¯ï¼‰
            groups_aug_list.append(groups[i]) 
            
    return np.array(X_aug_list, dtype=np.float32), np.array(y_aug_list), np.array(groups_aug_list)

# ==================== 0. é…ç½®åŒºåŸŸ ====================

# 1. ç›®æ ‡è®¾ç½®
TARGET_SUBJECTS = ["charles", "gavvin", "gerard", "giland", "jessie", "legend"] 
TARGET_LABELS = [5, 6, 8]            # æŒ‡å®šåŠ¨ä½œæ ‡ç­¾
TARGET_DATES = None                     # None è¡¨ç¤ºæ‰€æœ‰æ—¥æœŸ

# 2. å®éªŒæ¨¡å‹ (Grid Search)
MODELS_TO_TEST = [
    ("Advanced_CRNN", model_lib.build_advanced_crnn),
    ("TCN", model_lib.build_tcn_model),
    ("ResNet1D", model_lib.build_resnet_model),
]

OPTIMIZERS_TO_TEST = [
    ("Adam", tf.keras.optimizers.Adam, 0.001, {}),
    ("AdamW", tf.keras.optimizers.AdamW, 0.001, {'weight_decay': 1e-4}),
    ("Nadam", tf.keras.optimizers.Nadam, 0.001, {}),
]

VOTING_OPTIONS = [False] # æ˜¯å¦å¼€å¯æŠ•ç¥¨

# 3. æ ¸å¿ƒå‚æ•° (Rhythm Logic)
CONFIG = {
    'fs': 1000,                # é‡‡æ ·ç‡
    'use_imu': True,
    'rhythm_interval_ms': 4000,# [å…³é”®] åŠ¨ä½œé—´éš” (èŠ‚æ‹å™¨é€Ÿåº¦)
    'rhythm_window_ms': 352,   # [å…³é”®] æ¯æ¬¡æˆªå–çš„çª—å£å¤§å° (ä»¥å³°å€¼ä¸ºä¸­å¿ƒ)
    'epochs': 100,
    'batch_size': 128,
    'window_ms': 350,          # è¾“å…¥æ¨¡å‹çš„çª—å£å¤§å°
    'stride_ms': 350,           # åˆ‡ç‰‡æ­¥é•¿
    'test_size': 0.2,
    'split_strategy': "æ··åˆåˆ‡åˆ† (çœ‹åˆ°æ‰€æœ‰å¤©/äºº)", 
}

# 4. æ•°æ®å¢å¼º
AUGMENT_CONFIG = {
    'enable_rest': True,       # æ˜¯å¦é‡‡é›†é™æ¯æ•°æ® (Label 0)
    'multiplier': 3,           # æ•°æ®å€å¢ç³»æ•°
    'enable_scaling': True,
    'enable_noise': True,
    'enable_warp': False,      # æ—¶é—´æ‰­æ›² (è€—æ—¶ï¼Œè§†æƒ…å†µå¼€å¯)
    'enable_shift': False,
    'enable_mask': False
}

LOG_DIR = "1.26_auto_train_logs_rhythm_withoutstride"
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

# ==================== 1. æ ¸å¿ƒç®—æ³• (ç§»æ¤è‡ª new_app_gui.py) ====================

def parse_filename_info(filepath):
    """è§£ææ–‡ä»¶åï¼Œè¿”å› (Subject, Date, Label, Timestamp)"""
    filename = os.path.basename(filepath)
    parts = filepath.split(os.sep)
    subject = parts[-3] if len(parts) >= 3 else "Unknown"
    date = parts[-2] if len(parts) >= 3 else "Unknown"
    
    label_match = re.search(r'DF(\d+)\.', filename)
    label = int(label_match.group(1)) if label_match else None
    return subject, date, label, filename

def get_rhythm_mask(energy, fs, interval_ms=4000, window_ms=300, noise_cv_threshold=0.2):
    """
    [Core Logic] 4s å›ºå®šèŠ‚å¥å³°å€¼æå–é€»è¾‘ + ç›¸ä½æŠ•ç¥¨
    """
    mask = np.zeros_like(energy, dtype=bool)
    
    # 1. å¯»æ‰¾å€™é€‰å³°
    min_dist = int(2.0 * fs) 
    noise_floor = np.percentile(energy, 10)
    peaks, _ = signal.find_peaks(energy, distance=min_dist, height=noise_floor * 1.5)
    
    if len(peaks) == 0:
        return mask
    
    # 2. ç›¸ä½æŠ•ç¥¨ (Phase Voting) ç¡®å®šé”šç‚¹
    interval_samples = int((interval_ms / 1000) * fs)
    if interval_samples < 1: interval_samples = 1

    phases = peaks % interval_samples
    
    bin_width = int(0.2 * fs) # 200ms å®¹å·®
    bins = np.arange(0, interval_samples + bin_width, bin_width)
    counts, bin_edges = np.histogram(phases, bins=bins)
    
    best_bin_idx = np.argmax(counts)
    phase_start = bin_edges[best_bin_idx]
    phase_end = bin_edges[best_bin_idx+1]
    
    # ç­›é€‰ On-beat peaks
    candidates_mask = (phases >= phase_start) & (phases < phase_end)
    candidates = peaks[candidates_mask]
    
    if len(candidates) > 0:
        # é€‰èƒ½é‡æœ€å¤§çš„åˆç¾¤å³°ä½œä¸º Anchor
        best_sub_idx = np.argmax(energy[candidates])
        anchor_peak = candidates[best_sub_idx]
    else:
        anchor_peak = peaks[0]

    # 3. ç”Ÿæˆç½‘æ ¼å¹¶æœç´¢
    half_win = int((window_ms / 1000) * fs) // 2
    search_radius = int(1.0 * fs)
    valid_centers = []
    max_len = len(energy)
    
    # Forward & Backward Search
    for direction in [1, -1]:
        curr_grid = anchor_peak if direction == 1 else anchor_peak - interval_samples
        
        while 0 <= curr_grid < max_len:
            s_start = max(0, curr_grid - search_radius)
            s_end = min(max_len, curr_grid + search_radius)
            region = energy[s_start:s_end]
            
            if len(region) > 0:
                local_max_idx = np.argmax(region)
                abs_center = s_start + local_max_idx
                # å†æ¬¡æ ¡éªŒå³°å€¼å¼ºåº¦ï¼Œé˜²æ­¢æå–åˆ°çº¯åº•å™ª
                if energy[abs_center] > noise_floor * 1.2:
                    valid_centers.append(abs_center)
            
            if direction == 1: curr_grid += interval_samples
            else: curr_grid -= interval_samples

    valid_centers = sorted(list(set(valid_centers)))
    
    # 4. ç”Ÿæˆ Mask (CV è¿‡æ»¤æŒç»­å™ªéŸ³)
    for c in valid_centers:
        s = max(0, c - half_win)
        e = min(max_len, c + half_win)
        
        seg_vals = energy[s:e]
        mean_e = np.mean(seg_vals)
        std_e = np.std(seg_vals)
        cv = std_e / (mean_e + 1e-6)
        
        ref_energy = energy[anchor_peak]
        # å¦‚æœèƒ½é‡å¾ˆå¤§ä½† CV å¾ˆå° (å¹³ç¨³å™ªéŸ³)ï¼Œå‰”é™¤
        if mean_e > ref_energy * 0.3 and cv < noise_cv_threshold:
             continue
             
        mask[s:e] = True
        
    return mask

# ==================== 2. æ•°æ®å¤„ç†æµæ°´çº¿ ====================

def process_files_with_rhythm(file_list, config, augment_config):
    """
    ä¿®æ”¹ç‰ˆï¼šå–æ¶ˆåˆ‡ç‰‡ (No Slicing/Stride)
    æ¯ä¸ªæ£€æµ‹åˆ°çš„åŠ¨ä½œæ®µåªå–ä¸­å¿ƒçš„ä¸€ä¸ªçª—å£ä½œä¸ºæ ·æœ¬ã€‚
    """
    X_list, y_list, groups_list = [], [], []
    
    fs = config['fs']
    win_size = int(fs * (config['window_ms'] / 1000))
    # stride å˜é‡ä¸å†éœ€è¦ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†æ»‘åŠ¨
    
    # å¢å¼ºå‚æ•°
    multiplier = augment_config.get('multiplier', 1)
    enable_rest = augment_config.get('enable_rest', True)
    
    # ç”¨äºè®¡ç®—é™æ¯æ ·æœ¬æ¯”ä¾‹
    total_act_samples = 0
    
    print(f"â³ æ­£åœ¨å¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶ (Mode: No Slicing, Center Crop)...")
    use_imu = config.get('use_imu', False)
    
    for i, f_path in enumerate(file_list):
        current_file_act_count = 0
        try:
            subject, date, label, fname = parse_filename_info(f_path)
            if label is None: continue

            df = pd.read_csv(f_path)
            cols = [c for c in df.columns if 'CH' in c]
            raw_emg = df[cols].values
            if raw_emg.shape[1] >= 5: raw_emg[:, 4] = raw_emg[:, 4] * 2.5
            
            # --- Load IMU & Merge ---
            if use_imu:
                imu_data = load_and_resample_imu(f_path, len(raw_emg))
                if imu_data is not None:
                    raw_data = np.hstack((raw_emg, imu_data))
                else:
                    print(f"âš ï¸ Skip {os.path.basename(f_path)}: IMU missing")
                    continue
            else:
                raw_data = raw_emg
                
            emg_cols = raw_emg.shape[1]
            data_proc = raw_data.copy()
                
            # --- Filter Chain ---
            b_notch, a_notch = signal.iirnotch(50, 30, fs)
            data_proc[:, :emg_cols] = signal.filtfilt(b_notch, a_notch, data_proc[:, :emg_cols], axis=0)
            
            b, a = signal.butter(4, [20, 450], btype='bandpass', fs=fs)
            data_clean = data_proc
            data_clean[:, :emg_cols] = signal.filtfilt(b, a, data_proc[:, :emg_cols], axis=0)
            
            # Energy Calculation
            emg_part = data_clean[:, :emg_cols]
            energy = np.sqrt(np.mean(emg_part**2, axis=1))
            win_len = int(0.1 * fs)
            energy_smooth = np.convolve(energy, np.ones(win_len)/win_len, mode='same')
            
            # --- Mask Logic ---
            mask = get_rhythm_mask(
                energy_smooth, fs, 
                interval_ms=config['rhythm_interval_ms'],
                window_ms=config['rhythm_window_ms'],
                noise_cv_threshold=0.2
            )
            
            # --- Active Segments Processing ---
            labeled, num_seg = ndimage.label(mask)
            
            for seg_idx in range(1, num_seg + 1):
                loc = np.where(labeled == seg_idx)[0]
            
                # ç¡®ä¿å½¢çŠ¶æ­£ç¡® (åŒé‡ä¿é™©)
                # 1. æ‰¾åˆ°è¯¥åŠ¨ä½œç‰‡æ®µåœ¨åŸå§‹æ•°æ®ä¸­çš„ä¸­å¿ƒç‚¹
                center_idx = loc[0] + len(loc) // 2
                
                # 2. è®¡ç®—éœ€è¦çš„èµ·å§‹å’Œç»“æŸä½ç½® (åœ¨åŸå§‹é•¿æ•°æ® data_clean ä¸­)
                half_win = win_size // 2
                w_start = center_idx - half_win
                w_end = w_start + win_size
                
                # 3. è¾¹ç•Œå¤„ç† (å¦‚æœåŠ¨ä½œåˆšå¥½åœ¨æ–‡ä»¶å¼€å¤´æˆ–ç»“å°¾)
                pad_left = 0
                pad_right = 0
                
                if w_start < 0:
                    pad_left = -w_start # éœ€è¦åœ¨å·¦è¾¹è¡¥å¤šå°‘0
                    w_start = 0
                if w_end > len(data_clean):
                    pad_right = w_end - len(data_clean) # éœ€è¦åœ¨å³è¾¹è¡¥å¤šå°‘0
                    w_end = len(data_clean)
                
                # 4. ä»åŸå§‹æ•°æ®ä¸­æˆªå– (è¿™æ ·å°±è‡ªåŠ¨åŒ…å«äº†åŠ¨ä½œå‘¨å›´çš„é™æ¯æ•°æ®ï¼Œè¡¥è¶³äº†æ—¶é•¿)
                seg_data = data_clean[w_start : w_end]
                
                # 5. å¦‚æœç¢°åˆ°æ–‡ä»¶è¾¹ç¼˜å¯¼è‡´é•¿åº¦ä¸å¤Ÿï¼Œè¿›è¡Œé›¶å¡«å…… (Padding)
                if pad_left > 0 or pad_right > 0:
                    # ((pad_left, pad_right), (0, 0)) è¡¨ç¤ºåªåœ¨æ—¶é—´ç»´åº¦(è¡Œ)å‰åè¡¥é›¶ï¼Œé€šé“ç»´åº¦(åˆ—)ä¸è¡¥
                    seg_data = np.pad(seg_data, ((pad_left, pad_right), (0, 0)), mode='constant', constant_values=0)
                
                # Z-Score Norm (Per segment)
                seg_mean = np.mean(seg_data, axis=0)
                seg_std = np.std(seg_data, axis=0)
                seg_norm = (seg_data - seg_mean) / (seg_std + 1e-6)
                
                # æˆªå–å”¯ä¸€çª—å£
                window = seg_norm
                
                # Original
                X_list.append(window)
                current_file_act_count += 1
                y_list.append(label)
                groups_list.append(f"{fname}_seg{seg_idx}")
                total_act_samples += 1
                
                # Augmentation
                for _ in range(multiplier - 1):
                    aug_win = window.copy()
                    if augment_config.get('enable_warp', False) and np.random.random() > 0.5:
                        aug_win = time_warp(aug_win)
                    if augment_config.get('enable_shift', False) and np.random.random() > 0.5:
                        aug_win = time_shift(aug_win) # è¿™é‡Œçš„ shift æ˜¯ rollï¼Œä¸æ”¹å˜é•¿åº¦ï¼Œä»ç„¶é€‚ç”¨
                    if augment_config.get('enable_scaling', True) and np.random.random() > 0.3:
                            aug_win *= np.random.uniform(0.8, 1.2)
                    if augment_config.get('enable_mask', False) and np.random.random() > 0.7:
                        aug_win = channel_mask(aug_win)
                    if augment_config.get('enable_noise', True):
                        aug_win += np.random.normal(0, 0.02, aug_win.shape)

                    X_list.append(aug_win)
                    y_list.append(label)
                    groups_list.append(f"{fname}_seg{seg_idx}")

            # --- Rest (Silence) Processing (ä¿®æ”¹ç‚¹ 2: éšæœºæŠ½å–) ---
            if enable_rest:
                noise_floor = np.percentile(energy_smooth, 10)
                peak_level = np.percentile(energy_smooth, 99)
                vad_threshold = noise_floor + 0.15 * (peak_level - noise_floor)
                vad_mask = energy_smooth > vad_threshold
                rest_mask = ~vad_mask
                
                safe_margin = int(0.15 * fs)
                rest_mask = ndimage.binary_erosion(rest_mask, structure=np.ones(safe_margin))
                
                labeled_rest, num_rest = ndimage.label(rest_mask)
                
                # ç›®æ ‡ï¼šé™æ¯æ ·æœ¬æ•°é‡ä¸ºåŠ¨ä½œæ ·æœ¬çš„ 20%
                target_rest = int(current_file_act_count * 0.2) + 2
                
                # æ”¶é›†æ‰€æœ‰è¶³å¤Ÿé•¿çš„é™æ¯æ®µ
                valid_rest_segments = []
                for r_idx in range(1, num_rest + 1):
                    r_loc = np.where(labeled_rest == r_idx)[0]
                    if len(r_loc) > win_size:
                        valid_rest_segments.append(data_clean[r_loc[0]:r_loc[-1]])
                
                # ä»è¿™äº›æ®µä¸­éšæœºæˆªå– target_rest ä¸ªæ ·æœ¬
                collected_rest = 0
                retries = 0
                max_retries = target_rest * 2 # é˜²æ­¢æ­»å¾ªç¯
                
                if valid_rest_segments:
                    while collected_rest < target_rest and retries < max_retries:
                        # éšæœºé€‰ä¸€ä¸ªæ®µ
                        seg = valid_rest_segments[np.random.randint(len(valid_rest_segments))]
                        if len(seg) <= win_size:
                            retries += 1
                            continue
                            
                        # éšæœºé€‰ä¸€ä¸ªèµ·å§‹ç‚¹
                        max_start = len(seg) - win_size
                        if max_start <= 0: start_idx = 0
                        else: start_idx = np.random.randint(0, max_start)
                        
                        r_raw_win = seg[start_idx : start_idx + win_size]
                        
                        # Norm
                        r_mean = np.mean(r_raw_win, axis=0)
                        r_std = np.std(r_raw_win, axis=0)
                        r_std = np.where(r_std < 0.01, 1.0, r_std)
                        r_norm = (r_raw_win - r_mean) / (r_std + 1e-6)
                        
                        X_list.append(r_norm)
                        y_list.append(0)
                        groups_list.append(f"{fname}_rest_{collected_rest}")
                        collected_rest += 1
                        retries += 1

        except Exception as e:
            print(f"Error reading {f_path}: {e}")
            import traceback
            traceback.print_exc()
            
    print(f"âœ… å¤„ç†å®Œæˆ: æ ·æœ¬æ•° {len(X_list)}")
    return np.array(X_list), np.array(y_list), np.array(groups_list)

# ==================== 3. è¾…åŠ©ç±» (Mock Streamlit) ====================
class MockProgressBar:
    def progress(self, value): pass

class MockStatusText:
    def text(self, msg): print(f"    â””â”€ {msg}")

# ==================== 4. ä¸»ç¨‹åº ====================

def run_automation():
    # 1. æŸ¥æ‰¾æ‰€æœ‰ç›®æ ‡æ–‡ä»¶
    search_pattern = os.path.join("data", "*", "*", "RAW_EMG*.csv")
    all_files = glob.glob(search_pattern)
    target_files = []
    
    print(f"ğŸ” æ­£åœ¨ç­›é€‰æ–‡ä»¶...")
    for f in all_files:
        s, d, l, _ = parse_filename_info(f)
        if s not in TARGET_SUBJECTS: continue
        if TARGET_DATES and d not in TARGET_DATES: continue
        if l not in TARGET_LABELS: continue
        target_files.append(f)
        
    if not target_files:
        print("âŒ æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶")
        return

    # === å…³é”®ä¿®æ”¹æ­¥éª¤ 1: åŠ è½½æ‰€æœ‰æ•°æ® (Clean Load) ===
    print(f"â³ æ­£åœ¨åŠ è½½æ‰€æœ‰æ•°æ® (å…± {len(target_files)} ä¸ªæ–‡ä»¶)...")
    
    # åˆ›å»ºä¸€ä¸ªâ€œçº¯å‡€â€çš„é…ç½®ï¼Œå¼ºåˆ¶ multiplier=1ï¼Œå…³é—­æ‰€æœ‰å¢å¼º
    # è¿™æ ·æˆ‘ä»¬åªåŠ è½½æœ€åŸå§‹çš„æ•°æ®
    clean_config = AUGMENT_CONFIG.copy()
    clean_config['multiplier'] = 1
    clean_config['enable_noise'] = False
    clean_config['enable_warp'] = False
    clean_config['enable_shift'] = False
    clean_config['enable_mask'] = False
    clean_config['enable_scaling'] = False
    # æ³¨æ„ï¼šenable_rest ä¿æŒåŸæ ·ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦é™æ¯æ•°æ®
    
    X_all, y_all, groups_all = process_files_with_rhythm(
        target_files, CONFIG, clean_config
    )
    
    if len(X_all) == 0:
        print("âŒ åŠ è½½æ•°æ®ä¸ºç©ºï¼Œé€€å‡ºã€‚")
        return

    print(f"âœ… åŸå§‹æ•°æ®åŠ è½½å®Œæ¯•: {X_all.shape}")

    # === å…³é”®ä¿®æ”¹æ­¥éª¤ 2: æ ·æœ¬çº§æ··åˆåˆ‡åˆ† (GroupShuffleSplit) ===
    # GroupShuffleSplit ä¼šä¿è¯åŒä¸€ä¸ª Group (å³åŒä¸€ä¸ªåŠ¨ä½œçš„æ‰€æœ‰åˆ‡ç‰‡) ä¸ä¼šè¢«æ‹†åˆ†åˆ° train å’Œ test
    # ç”±äºæˆ‘ä»¬è¿˜æ²¡æœ‰å¢å¼ºï¼Œç°åœ¨çš„ Group å°±æ˜¯åŸå§‹åŠ¨ä½œ ID (e.g., "filename_seg1")
    
    print(f"âœ‚ï¸ æ­£åœ¨æ‰§è¡Œæ··åˆåˆ‡åˆ† (Test Size: {CONFIG['test_size']})...")
    
    gss = GroupShuffleSplit(n_splits=1, test_size=CONFIG['test_size'], random_state=42)
    train_idx, test_idx = next(gss.split(X_all, y_all, groups=groups_all))
    
    X_train_raw, y_train_raw = X_all[train_idx], y_all[train_idx]
    X_test, y_test = X_all[test_idx], y_all[test_idx]
    
    # ä¿å­˜è®­ç»ƒé›†çš„ Groups ä¿¡æ¯ï¼Œç¨åå¢å¼ºæ—¶è¦ç”¨
    groups_train_raw = groups_all[train_idx]
    
    print(f"   Train (Raw): {X_train_raw.shape}")
    print(f"   Test  (Clean): {X_test.shape}")

    # === å…³é”®ä¿®æ”¹æ­¥éª¤ 3: ä»…å¯¹è®­ç»ƒé›†è¿›è¡Œå†…å­˜å¢å¼º ===
    # æ­¤æ—¶ä½¿ç”¨å…¨å±€å®šä¹‰çš„ AUGMENT_CONFIG (é‡Œé¢åŒ…å«äº† multiplier, noise ç­‰è®¾ç½®)
    
    if AUGMENT_CONFIG.get('multiplier', 1) > 1 or AUGMENT_CONFIG.get('enable_noise', False):
        print("ğŸš€ æ­£åœ¨å¯¹è®­ç»ƒé›†åº”ç”¨æ•°æ®å¢å¼º...")
        X_train, y_train, groups_train = augment_dataset_in_memory(
            X_train_raw, y_train_raw, groups_train_raw, AUGMENT_CONFIG
        )
    else:
        X_train, y_train, groups_train = X_train_raw, y_train_raw, groups_train_raw
    
    # 3. æ ‡ç­¾æ˜ å°„ (Label Mapping)
    all_labels = np.unique(np.concatenate([y_train, y_test]))
    label_map = {original: new for new, original in enumerate(all_labels)}
    
    y_train_mapped = np.array([label_map[i] for i in y_train])
    y_test_mapped = np.array([label_map[i] for i in y_test])
    
    print(f"ğŸ“Š æœ€ç»ˆæ•°æ®é›†è§„æ¨¡ (Ready for Training):")
    print(f"   Train: {X_train.shape} [Augmented]")
    print(f"   Test:  {X_test.shape} [Clean]")
    print(f"   Labels: {label_map}")
    # 4. è®­ç»ƒå¾ªç¯
    MODELS_DIR = "1.26_trained_models_rhythm_wihoutstride"
    if not os.path.exists(MODELS_DIR): os.makedirs(MODELS_DIR)
    
    total_exp = len(MODELS_TO_TEST) * len(OPTIMIZERS_TO_TEST) * len(VOTING_OPTIONS)
    curr_exp = 0
    
    input_shape = (X_train.shape[1], X_train.shape[2])
    num_classes = len(label_map)
    
    for model_name, model_builder in MODELS_TO_TEST:
        for opt_name, opt_cls, lr, opt_kwargs in OPTIMIZERS_TO_TEST:
            if opt_name == "SGD":
                current_epochs = 200
            else:
                current_epochs = CONFIG['epochs']
            for use_vote in VOTING_OPTIONS:
                curr_exp += 1
                exp_id = f"{model_name}_{opt_name}_Vote{use_vote}"
                print(f"\nğŸš€ [{curr_exp}/{total_exp}] Start: {exp_id}")
                
                tf.keras.backend.clear_session()
                model = model_builder(input_shape, num_classes)
                optimizer = opt_cls(learning_rate=lr, **opt_kwargs)
                
                # å¼€å§‹è®­ç»ƒ
                start_t = time.time()
                try:
                    history = train_utils.train_with_voting_mechanism(
                        model, X_train, y_train_mapped, groups_train,
                        X_test, y_test_mapped,
                        epochs=current_epochs,
                        batch_size=CONFIG['batch_size'],
                        samples_per_group=3,
                        vote_weight=0.5 if use_vote else 0.0,
                        st_progress_bar=MockProgressBar(),
                        st_status_text=MockStatusText(),
                        voting_start_epoch=25,
                        optimizer=optimizer
                    )
                except Exception as e:
                    print(f"âŒ Error: {e}")
                    continue
                    
                duration = time.time() - start_t
                
                # ä¿å­˜
                model.save(os.path.join(MODELS_DIR, f"{exp_id}.keras"))
                
                # è®°å½•
                save_log(
                    exp_id, 
                    model, 
                    history, 
                    X_test, 
                    y_test_mapped, 
                    label_map, 
                    duration,
                    opt_name,       # ä¼ å…¥ä¼˜åŒ–å™¨åç§°
                    lr,             # ä¼ å…¥å­¦ä¹ ç‡
                    use_vote,       # ä¼ å…¥æ˜¯å¦æŠ•ç¥¨
                    current_epochs  # åŠ¨æ€åˆ¤æ–­çš„ Epoch æ•°
                )

def save_log(exp_id, model, history, X_test, y_test, label_map, duration, opt_name, lr, use_voting, actual_epochs):
    # 1. è®¡ç®—é¢„æµ‹æŠ¥å‘Š
    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)
    
    # è·å–è¯¦ç»†å­—å…¸æ ¼å¼æŠ¥å‘Šï¼Œè½¬ä¸º DataFrame ä»¥ä¾¿ç¾è§‚æ‰“å°
    report_dict = classification_report(
        y_test, y_pred, 
        target_names=[str(k) for k in label_map.keys()],
        output_dict=True
    )
    report_df = pd.DataFrame(report_dict).transpose()
    
    # 2. å‡†å¤‡æ–‡ä»¶å
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')
    log_file = os.path.join(LOG_DIR, f"{timestamp}_{exp_id}.txt")
    
    final_acc = history['val_accuracy'][-1]
    final_loss = history['val_loss'][-1]

    # 3. å†™å…¥è¯¦ç»†ä¿¡æ¯ (è¿™éƒ¨åˆ†æ˜¯åŸç‰ˆ auto_train.py çš„ç²¾å)
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write(f"Experiment ID: {exp_id}\n")
        f.write(f"Date: {timestamp}\n")
        f.write(f"Duration: {duration:.1f}s\n")
        f.write("="*40 + "\n")
        f.write(f"Subjects: {TARGET_SUBJECTS}\n")
        f.write(f"Labels: {TARGET_LABELS}\n")
        f.write(f"Model: {model.name}\n")
        f.write(f"Optimizer: {opt_name} (LR={lr})\n")
        f.write(f"Epochs: {actual_epochs}\n")  # [å…³é”®] è®°å½•å®é™…è·‘äº†å¤šå°‘è½®
        f.write(f"Voting Mode: {'ON' if use_voting else 'OFF'}\n")
        f.write("-" * 20 + "\n")
        f.write(f"Batch Size: {CONFIG['batch_size']}\n")
        f.write(f"Augment Config: {AUGMENT_CONFIG}\n") # è®°å½•å¢å¼ºé…ç½®
        f.write("="*40 + "\n")
        f.write(f"Final Val Accuracy: {final_acc*100:.2f}%\n")
        f.write(f"Final Val Loss: {final_loss:.4f}\n")
        f.write("\n--- Classification Report ---\n")
        f.write(report_df.to_string())

    print(f"ğŸ’¾ Detailed Log saved: {log_file}")

if __name__ == "__main__":
    gpus = tf.config.list_physical_devices('GPU')
    if gpus: 
        try: tf.config.experimental.set_memory_growth(gpus[0], True)
        except: pass
    run_automation()