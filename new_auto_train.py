import os
import sys
import time
import glob
import datetime
import re
import numpy as np
import pandas as pd
import scipy.signal as signal
import scipy.ndimage as ndimage
import tensorflow as tf
from sklearn.metrics import classification_report

# å¼•ç”¨ç°æœ‰æ¨¡å— (ç¡®ä¿è¿™äº›æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹)
import train_utils
import model as model_lib

def time_warp(data, sigma=0.2, knot=4):
    """æ—¶é—´æ‰­æ›²"""
    orig_steps = np.arange(data.shape[0])
    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, data.shape[1]))
    ret = np.zeros_like(data)
    for i in range(data.shape[1]):
        time_warp = np.interp(orig_steps, np.linspace(0, data.shape[0]-1., num=knot+2), random_warps[:, i])
        cum_warp = np.cumsum(time_warp)
        scale = (data.shape[0]-1) / cum_warp[-1]
        new_times = cum_warp * scale
        ret[:, i] = np.interp(orig_steps, new_times, data[:, i])
    return ret

def time_shift(data, shift_limit=0.1):
    """æ—¶é—´å¹³ç§»"""
    shift_amt = int(data.shape[0] * shift_limit * np.random.uniform(-1, 1))
    return np.roll(data, shift_amt, axis=0)

def channel_mask(data, mask_prob=0.15):
    """é€šé“é®æŒ¡"""
    temp = data.copy()
    if np.random.random() < mask_prob:
        c = np.random.randint(0, data.shape[1])
        temp[:, c] = 0
    return temp

# ==================== 0. é…ç½®åŒºåŸŸ ====================

# 1. ç›®æ ‡è®¾ç½®
TARGET_SUBJECTS = ["charles", "gavvin", "gerard", "giland", "jessie", "legend"] 
TARGET_LABELS = [1, 3, 4, 5, 6, 7, 8, 9, 15]            # æŒ‡å®šåŠ¨ä½œæ ‡ç­¾
TARGET_DATES = None                     # None è¡¨ç¤ºæ‰€æœ‰æ—¥æœŸ

# 2. å®éªŒæ¨¡å‹ (Grid Search)
MODELS_TO_TEST = [
    ("Advanced_CRNN", model_lib.build_advanced_crnn),
    ("TCN", model_lib.build_tcn_model),
    ("ResNet1D", model_lib.build_resnet_model),
]

OPTIMIZERS_TO_TEST = [
    ("Adam", tf.keras.optimizers.Adam, 0.001, {}),
    ("AdamW", tf.keras.optimizers.AdamW, 0.001, {'weight_decay': 1e-4}),
    ("Nadam", tf.keras.optimizers.Nadam, 0.001, {}),
]

VOTING_OPTIONS = [False] # æ˜¯å¦å¼€å¯æŠ•ç¥¨

# 3. æ ¸å¿ƒå‚æ•° (Rhythm Logic)
CONFIG = {
    'fs': 1000,                # é‡‡æ ·ç‡
    'rhythm_interval_ms': 4000,# [å…³é”®] åŠ¨ä½œé—´éš” (èŠ‚æ‹å™¨é€Ÿåº¦)
    'rhythm_window_ms': 350,   # [å…³é”®] æ¯æ¬¡æˆªå–çš„çª—å£å¤§å° (ä»¥å³°å€¼ä¸ºä¸­å¿ƒ)
    'epochs': 100,
    'batch_size': 128,
    'window_ms': 250,          # è¾“å…¥æ¨¡å‹çš„çª—å£å¤§å°
    'stride_ms': 50,           # åˆ‡ç‰‡æ­¥é•¿
    'test_size': 0.2,
    'split_strategy': "æ··åˆåˆ‡åˆ† (çœ‹åˆ°æ‰€æœ‰å¤©/äºº)", 
}

# 4. æ•°æ®å¢å¼º
AUGMENT_CONFIG = {
    'enable_rest': True,       # æ˜¯å¦é‡‡é›†é™æ¯æ•°æ® (Label 0)
    'multiplier': 1,           # æ•°æ®å€å¢ç³»æ•°
    'enable_scaling': True,
    'enable_noise': True,
    'enable_warp': False,      # æ—¶é—´æ‰­æ›² (è€—æ—¶ï¼Œè§†æƒ…å†µå¼€å¯)
    'enable_shift': False,
    'enable_mask': False
}

LOG_DIR = "1.24_9_auto_train_logs_rhythm"
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

# ==================== 1. æ ¸å¿ƒç®—æ³• (ç§»æ¤è‡ª new_app_gui.py) ====================

def parse_filename_info(filepath):
    """è§£ææ–‡ä»¶åï¼Œè¿”å› (Subject, Date, Label, Timestamp)"""
    filename = os.path.basename(filepath)
    parts = filepath.split(os.sep)
    subject = parts[-3] if len(parts) >= 3 else "Unknown"
    date = parts[-2] if len(parts) >= 3 else "Unknown"
    
    label_match = re.search(r'DF(\d+)\.', filename)
    label = int(label_match.group(1)) if label_match else None
    return subject, date, label, filename

def get_rhythm_mask(energy, fs, interval_ms=4000, window_ms=300, noise_cv_threshold=0.2):
    """
    [Core Logic] 4s å›ºå®šèŠ‚å¥å³°å€¼æå–é€»è¾‘ + ç›¸ä½æŠ•ç¥¨
    """
    mask = np.zeros_like(energy, dtype=bool)
    
    # 1. å¯»æ‰¾å€™é€‰å³°
    min_dist = int(2.0 * fs) 
    noise_floor = np.percentile(energy, 10)
    peaks, _ = signal.find_peaks(energy, distance=min_dist, height=noise_floor * 1.5)
    
    if len(peaks) == 0:
        return mask
    
    # 2. ç›¸ä½æŠ•ç¥¨ (Phase Voting) ç¡®å®šé”šç‚¹
    interval_samples = int((interval_ms / 1000) * fs)
    if interval_samples < 1: interval_samples = 1

    phases = peaks % interval_samples
    
    bin_width = int(0.2 * fs) # 200ms å®¹å·®
    bins = np.arange(0, interval_samples + bin_width, bin_width)
    counts, bin_edges = np.histogram(phases, bins=bins)
    
    best_bin_idx = np.argmax(counts)
    phase_start = bin_edges[best_bin_idx]
    phase_end = bin_edges[best_bin_idx+1]
    
    # ç­›é€‰ On-beat peaks
    candidates_mask = (phases >= phase_start) & (phases < phase_end)
    candidates = peaks[candidates_mask]
    
    if len(candidates) > 0:
        # é€‰èƒ½é‡æœ€å¤§çš„åˆç¾¤å³°ä½œä¸º Anchor
        best_sub_idx = np.argmax(energy[candidates])
        anchor_peak = candidates[best_sub_idx]
    else:
        anchor_peak = peaks[0]

    # 3. ç”Ÿæˆç½‘æ ¼å¹¶æœç´¢
    half_win = int((window_ms / 1000) * fs) // 2
    search_radius = int(1.0 * fs)
    valid_centers = []
    max_len = len(energy)
    
    # Forward & Backward Search
    for direction in [1, -1]:
        curr_grid = anchor_peak if direction == 1 else anchor_peak - interval_samples
        
        while 0 <= curr_grid < max_len:
            s_start = max(0, curr_grid - search_radius)
            s_end = min(max_len, curr_grid + search_radius)
            region = energy[s_start:s_end]
            
            if len(region) > 0:
                local_max_idx = np.argmax(region)
                abs_center = s_start + local_max_idx
                # å†æ¬¡æ ¡éªŒå³°å€¼å¼ºåº¦ï¼Œé˜²æ­¢æå–åˆ°çº¯åº•å™ª
                if energy[abs_center] > noise_floor * 1.2:
                    valid_centers.append(abs_center)
            
            if direction == 1: curr_grid += interval_samples
            else: curr_grid -= interval_samples

    valid_centers = sorted(list(set(valid_centers)))
    
    # 4. ç”Ÿæˆ Mask (CV è¿‡æ»¤æŒç»­å™ªéŸ³)
    for c in valid_centers:
        s = max(0, c - half_win)
        e = min(max_len, c + half_win)
        
        seg_vals = energy[s:e]
        mean_e = np.mean(seg_vals)
        std_e = np.std(seg_vals)
        cv = std_e / (mean_e + 1e-6)
        
        ref_energy = energy[anchor_peak]
        # å¦‚æœèƒ½é‡å¾ˆå¤§ä½† CV å¾ˆå° (å¹³ç¨³å™ªéŸ³)ï¼Œå‰”é™¤
        if mean_e > ref_energy * 0.3 and cv < noise_cv_threshold:
             continue
             
        mask[s:e] = True
        
    return mask

# ==================== 2. æ•°æ®å¤„ç†æµæ°´çº¿ ====================

def process_files_with_rhythm(file_list, config, augment_config):
    """
    ä½¿ç”¨å›ºå®šèŠ‚å¥é€»è¾‘å¤„ç†æ–‡ä»¶åˆ—è¡¨
    """
    X_list, y_list, groups_list = [], [], []
    
    fs = config['fs']
    win_size = int(fs * (config['window_ms'] / 1000))
    stride = int(fs * (config['stride_ms'] / 1000))
    
    # å¢å¼ºå‚æ•°
    multiplier = augment_config.get('multiplier', 1)
    enable_rest = augment_config.get('enable_rest', True)
    
    # ç”¨äºè®¡ç®—é™æ¯æ ·æœ¬æ¯”ä¾‹
    total_act_samples = 0
    
    print(f"â³ æ­£åœ¨å¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶ (Mode: Rhythm Phase Voting)...")
    
    for i, f_path in enumerate(file_list):
        try:
            subject, date, label, fname = parse_filename_info(f_path)
            if label is None: continue
            
            # --- Load ---
            df = pd.read_csv(f_path)
            cols = [c for c in df.columns if 'CH' in c]
            raw_data = df[cols].values
            if raw_data.shape[1] >= 5: # CH5 fix
                raw_data[:, 4] = raw_data[:, 4] * 2.5
                
            # --- Filter Chain ---
            # 1. Notch
            b_notch, a_notch = signal.iirnotch(50, 30, fs)
            data_notch = signal.filtfilt(b_notch, a_notch, raw_data, axis=0)
            
            # 2. Bandpass
            b, a = signal.butter(4, [20, 450], btype='bandpass', fs=fs)
            data_clean = signal.filtfilt(b, a, data_notch, axis=0)
            
            # 3. Energy & Smooth (for Masking)
            energy = np.sqrt(np.mean(data_clean**2, axis=1))
            win_len = int(0.1 * fs) # 100ms smooth
            energy_smooth = np.convolve(energy, np.ones(win_len)/win_len, mode='same')
            
            # --- New Core Logic ---
            mask = get_rhythm_mask(
                energy_smooth, fs, 
                interval_ms=config['rhythm_interval_ms'],
                window_ms=config['rhythm_window_ms'],
                noise_cv_threshold=0.2
            )
            
            # --- Slicing Active Segments ---
            labeled, num_seg = ndimage.label(mask)
            
            for seg_idx in range(1, num_seg + 1):
                loc = np.where(labeled == seg_idx)[0]
                if len(loc) < win_size: continue # å¤ªçŸ­çš„ä¸å¤Ÿåˆ‡
                
                seg_data = data_clean[loc[0]:loc[-1]]
                
                # Z-Score Norm (Per segment)
                seg_mean = np.mean(seg_data, axis=0)
                seg_std = np.std(seg_data, axis=0)
                seg_norm = (seg_data - seg_mean) / (seg_std + 1e-6)
                
                # Sliding Window
                for w_start in range(0, len(seg_norm) - win_size + 1, stride):
                    window = seg_norm[w_start : w_start + win_size]
                    
                    # Original
                    X_list.append(window)
                    y_list.append(label)
                    groups_list.append(f"{fname}_seg{seg_idx}")
                    total_act_samples += 1
                    
                    # Augmentation (Simple Noise/Scale for now)
                    for _ in range(multiplier - 1):
                        aug_win = window.copy()

                        if augment_config.get('enable_warp', False) and np.random.random() > 0.5:
                            aug_win = time_warp(aug_win)
                            
                        # 2. æ—¶é—´å¹³ç§»
                        if augment_config.get('enable_shift', False) and np.random.random() > 0.5:
                            aug_win = time_shift(aug_win)
                            
                        # 3. å¹…åº¦ç¼©æ”¾
                        if augment_config.get('enable_scaling', True) and np.random.random() > 0.3:
                             aug_win *= np.random.uniform(0.8, 1.2)
                             
                        # 4. é€šé“é®æŒ¡
                        if augment_config.get('enable_mask', False) and np.random.random() > 0.7:
                            aug_win = channel_mask(aug_win)
                            
                        # 5. é«˜æ–¯å™ªå£° (é€šå¸¸æœ€ååŠ )
                        if augment_config.get('enable_noise', True):
                            aug_win += np.random.normal(0, 0.02, aug_win.shape)

                        X_list.append(aug_win)
                        y_list.append(label)
                        groups_list.append(f"{fname}_seg{seg_idx}_aug")

            # --- Slicing Rest (Silence) ---
            if enable_rest:
                # [ä¿®æ”¹] æ”¹å›ä½¿ç”¨èƒ½é‡é˜ˆå€¼å®šä¹‰é™æ¯ï¼Œè€Œä¸æ˜¯èŠ‚å¥ Mask çš„è¡¥é›†
                # 1. é‡æ–°è®¡ç®—ä¸€ä¸ªå®½æ³›çš„ VAD Mask (åŸºäºèƒ½é‡)
                noise_floor = np.percentile(energy_smooth, 10)
                peak_level = np.percentile(energy_smooth, 99)
                # é˜ˆå€¼ç³»æ•° 0.15 æ˜¯ç»éªŒå€¼ï¼Œä¸ GUI ä¿æŒä¸€è‡´
                vad_threshold = noise_floor + 0.15 * (peak_level - noise_floor)
                
                vad_mask = energy_smooth > vad_threshold
                
                # 2. å¯¹ VAD Mask å–åï¼Œå¾—åˆ°é™æ¯åŒº
                rest_mask = ~vad_mask
                
                # 3. è…èš€ (Erosion) ç¡®ä¿è¿œç¦»åŠ¨ä½œè¾¹ç¼˜ (è¿™é‡Œä¿æŒåŸæ ·æˆ–ç¨å¾®è°ƒå¤§ä¸€ç‚¹ margin)
                safe_margin = int(0.15 * fs) # 150ms margin
                rest_mask = ndimage.binary_erosion(rest_mask, structure=np.ones(safe_margin))
                
                # åé¢çš„é€»è¾‘ä¿æŒä¸å˜...
                labeled_rest, num_rest = ndimage.label(rest_mask)
                # éšæœºæŠ½å–ä¸€å®šæ•°é‡çš„ Restï¼Œé¿å…è¿‡å¤š
                target_rest = int(total_act_samples * 0.2) + 2 # æ¯æ–‡ä»¶çº¦ 20%
                
                rest_buffer = []
                for r_idx in range(1, num_rest + 1):
                    r_loc = np.where(labeled_rest == r_idx)[0]
                    if len(r_loc) > win_size:
                        r_data = data_clean[r_loc[0]:r_loc[-1]]
                        # Norm
                        r_mean = np.mean(r_data, axis=0)
                        r_std = np.std(r_data, axis=0)
                        r_std = np.where(r_std < 0.01, 1.0, r_std)
                        r_norm = (r_data - r_mean) / (r_std + 1e-6)
                        
                        # Big Stride for rest
                        for w_s in range(0, len(r_norm) - win_size, win_size):
                            rest_buffer.append(r_norm[w_s:w_s+win_size])
                            
                if rest_buffer:
                    np.random.shuffle(rest_buffer)
                    for rw in rest_buffer[:target_rest]:
                        X_list.append(rw)
                        y_list.append(0) # Label 0
                        groups_list.append(f"{fname}_rest")

        except Exception as e:
            print(f"Error reading {f_path}: {e}")
            
    print(f"âœ… å¤„ç†å®Œæˆ: æ ·æœ¬æ•° {len(X_list)}")
    return np.array(X_list), np.array(y_list), np.array(groups_list)

# ==================== 3. è¾…åŠ©ç±» (Mock Streamlit) ====================
class MockProgressBar:
    def progress(self, value): pass

class MockStatusText:
    def text(self, msg): print(f"    â””â”€ {msg}")

# ==================== 4. ä¸»ç¨‹åº ====================

def run_automation():
    # 1. æŸ¥æ‰¾æ–‡ä»¶
    search_pattern = os.path.join("data", "*", "*", "RAW_EMG*.csv")
    all_files = glob.glob(search_pattern)
    target_files = []
    
    print(f"ğŸ” æ­£åœ¨ç­›é€‰æ–‡ä»¶...")
    for f in all_files:
        s, d, l, _ = parse_filename_info(f)
        if s not in TARGET_SUBJECTS: continue
        if TARGET_DATES and d not in TARGET_DATES: continue
        if l not in TARGET_LABELS: continue
        target_files.append(f)
        
    if not target_files:
        print("âŒ æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶")
        return

    # 2. ç”Ÿæˆæ•°æ® (ä½¿ç”¨æ–°å‡½æ•°)
    X, y, groups = process_files_with_rhythm(target_files, CONFIG, AUGMENT_CONFIG)
    
    if len(X) == 0:
        print("âŒ æ ·æœ¬æ•°ä¸º0ï¼Œè¯·æ£€æŸ¥ rhythm_interval æˆ– æ–‡ä»¶çš„èƒ½é‡æ˜¯å¦è¿‡ä½")
        return
        
    # 3. å‡†å¤‡æ•°æ®
    # æ˜ å°„ Label: 5,6,7,8 -> 1,2,3,4 (0é¢„ç•™ç»™Rest)
    unique_labels = sorted(np.unique(y))
    label_map = {original: new for new, original in enumerate(unique_labels)}
    y_mapped = np.array([label_map[i] for i in y])
    
    # Split
    train_idx, test_idx = train_utils.smart_split(
        X, y_mapped, groups, CONFIG['split_strategy'], test_size=CONFIG['test_size']
    )
    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y_mapped[train_idx], y_mapped[test_idx]
    groups_train = groups[train_idx]
    
    print(f"ğŸ“Š è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}, ç±»åˆ«æ•°: {len(unique_labels)}")
    print(f"   Label Map: {label_map}")

    # 4. è®­ç»ƒå¾ªç¯
    MODELS_DIR = "1.24_9_trained_models_rhythm"
    if not os.path.exists(MODELS_DIR): os.makedirs(MODELS_DIR)
    
    total_exp = len(MODELS_TO_TEST) * len(OPTIMIZERS_TO_TEST) * len(VOTING_OPTIONS)
    curr_exp = 0
    
    input_shape = (X.shape[1], X.shape[2])
    num_classes = len(unique_labels)
    
    for model_name, model_builder in MODELS_TO_TEST:
        for opt_name, opt_cls, lr, opt_kwargs in OPTIMIZERS_TO_TEST:
            if opt_name == "SGD":
                current_epochs = 200
            else:
                current_epochs = CONFIG['epochs']
            for use_vote in VOTING_OPTIONS:
                curr_exp += 1
                exp_id = f"{model_name}_{opt_name}_Vote{use_vote}"
                print(f"\nğŸš€ [{curr_exp}/{total_exp}] Start: {exp_id}")
                
                tf.keras.backend.clear_session()
                model = model_builder(input_shape, num_classes)
                optimizer = opt_cls(learning_rate=lr, **opt_kwargs)
                
                # å¼€å§‹è®­ç»ƒ
                start_t = time.time()
                try:
                    history = train_utils.train_with_voting_mechanism(
                        model, X_train, y_train, groups_train,
                        X_test, y_test,
                        epochs=current_epochs,
                        batch_size=CONFIG['batch_size'],
                        samples_per_group=3,
                        vote_weight=0.5 if use_vote else 0.0,
                        st_progress_bar=MockProgressBar(),
                        st_status_text=MockStatusText(),
                        voting_start_epoch=25,
                        optimizer=optimizer
                    )
                except Exception as e:
                    print(f"âŒ Error: {e}")
                    continue
                    
                duration = time.time() - start_t
                
                # ä¿å­˜
                model.save(os.path.join(MODELS_DIR, f"{exp_id}.keras"))
                
                # è®°å½•
                save_log(
                    exp_id, 
                    model, 
                    history, 
                    X_test, 
                    y_test, 
                    label_map, 
                    duration,
                    opt_name,       # ä¼ å…¥ä¼˜åŒ–å™¨åç§°
                    lr,             # ä¼ å…¥å­¦ä¹ ç‡
                    use_vote,       # ä¼ å…¥æ˜¯å¦æŠ•ç¥¨
                    current_epochs  # åŠ¨æ€åˆ¤æ–­çš„ Epoch æ•°
                )

def save_log(exp_id, model, history, X_test, y_test, label_map, duration, opt_name, lr, use_voting, actual_epochs):
    # 1. è®¡ç®—é¢„æµ‹æŠ¥å‘Š
    y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)
    
    # è·å–è¯¦ç»†å­—å…¸æ ¼å¼æŠ¥å‘Šï¼Œè½¬ä¸º DataFrame ä»¥ä¾¿ç¾è§‚æ‰“å°
    report_dict = classification_report(
        y_test, y_pred, 
        target_names=[str(k) for k in label_map.keys()],
        output_dict=True
    )
    report_df = pd.DataFrame(report_dict).transpose()
    
    # 2. å‡†å¤‡æ–‡ä»¶å
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')
    log_file = os.path.join(LOG_DIR, f"{timestamp}_{exp_id}.txt")
    
    final_acc = history['val_accuracy'][-1]
    final_loss = history['val_loss'][-1]

    # 3. å†™å…¥è¯¦ç»†ä¿¡æ¯ (è¿™éƒ¨åˆ†æ˜¯åŸç‰ˆ auto_train.py çš„ç²¾å)
    with open(log_file, 'w', encoding='utf-8') as f:
        f.write(f"Experiment ID: {exp_id}\n")
        f.write(f"Date: {timestamp}\n")
        f.write(f"Duration: {duration:.1f}s\n")
        f.write("="*40 + "\n")
        f.write(f"Subjects: {TARGET_SUBJECTS}\n")
        f.write(f"Labels: {TARGET_LABELS}\n")
        f.write(f"Model: {model.name}\n")
        f.write(f"Optimizer: {opt_name} (LR={lr})\n")
        f.write(f"Epochs: {actual_epochs}\n")  # [å…³é”®] è®°å½•å®é™…è·‘äº†å¤šå°‘è½®
        f.write(f"Voting Mode: {'ON' if use_voting else 'OFF'}\n")
        f.write("-" * 20 + "\n")
        f.write(f"Batch Size: {CONFIG['batch_size']}\n")
        f.write(f"Augment Config: {AUGMENT_CONFIG}\n") # è®°å½•å¢å¼ºé…ç½®
        f.write("="*40 + "\n")
        f.write(f"Final Val Accuracy: {final_acc*100:.2f}%\n")
        f.write(f"Final Val Loss: {final_loss:.4f}\n")
        f.write("\n--- Classification Report ---\n")
        f.write(report_df.to_string())

    print(f"ğŸ’¾ Detailed Log saved: {log_file}")

if __name__ == "__main__":
    gpus = tf.config.list_physical_devices('GPU')
    if gpus: 
        try: tf.config.experimental.set_memory_growth(gpus[0], True)
        except: pass
    run_automation()