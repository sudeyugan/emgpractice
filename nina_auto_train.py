import scipy.io as sio
import os
import sys
import time
import glob
import datetime
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import classification_report
import scipy.ndimage as ndimage
import gc

# å¼•ç”¨ç°æœ‰æ¨¡å—
import train_utils
import nina_model as model_lib  # é¿å…å˜é‡åå†²çª

# ==================== 0. é…ç½®åŒºåŸŸ (æ ¹æ®éœ€æ±‚ä¿®æ”¹) ====================

# 1. ç›®æ ‡è®¾ç½®
TARGET_SUBJECTS = [f"s{i}" for i in range(1, 25)]  
TARGET_LABELS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]                       # åªå–è¿™8ä¸ªåŠ¨ä½œ

# 2. å®éªŒå˜é‡ (Grid Search)
MODELS_TO_TEST = [
    ("Simple_CNN", model_lib.build_simple_cnn),
    ("Advanced_CRNN", model_lib.build_advanced_crnn),
    ("TCN", model_lib.build_tcn_model),
    ("ResNet1D", model_lib.build_resnet_model),
]

OPTIMIZERS_TO_TEST = [
    ("Adam", tf.keras.optimizers.Adam, 0.001, {}),
    ("AdamW", tf.keras.optimizers.AdamW, 0.001, {'weight_decay': 1e-4}),
    ("Nadam", tf.keras.optimizers.Nadam, 0.001, {}),
]

VOTING_OPTIONS = [False] # æ˜¯å¦å¼€å¯æŠ•ç¥¨

# 3. å›ºå®šå‚æ•°
CONFIG = {
    'fs': 100, 
    'epochs': 100,
    'batch_size': 256,
    'test_size': 0.2,          # æµ‹è¯•é›†æ¯”ä¾‹
    'split_strategy': "ç•™æ–‡ä»¶éªŒè¯ (åŒå¤©/åŒäºº)",
    'label_smoothing': 0.1,    # æ ‡ç­¾å¹³æ»‘
    'voting_start_epoch': 20,  # æŠ•ç¥¨å¼€å¯æ—¶é—´
    'voting_weight': 0.5,      # æŠ•ç¥¨æƒé‡
    'samples_per_group': 5,    # æŠ•ç¥¨ç»„é‡‡æ ·æ•°
}

# 4. æ•°æ®å¢å¼ºé…ç½® (å¹…åº¦ç¼©æ”¾ + é«˜æ–¯å™ªå£°)
AUGMENT_CONFIG = {
    'enable_rest': True,       # åŠ å…¥é™æ¯
    'multiplier': 1,           # è‡ªåŠ¨åŒ–æµ‹è¯•é€šå¸¸ä¸æ— é™å€å¢ï¼Œè®¾ä¸º1æˆ–2å³å¯ï¼Œæˆ–è€…æŒ‰éœ€è°ƒæ•´
    'enable_scaling': True,    # å¹…åº¦ç¼©æ”¾
    'enable_noise': True,      # é«˜æ–¯å™ªå£°
    'enable_warp': False,
    'enable_shift': False,
    'enable_mask': False
}

LOG_DIR = "E1_auto_train_logs"
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

# ==================== 1. è¾…åŠ©ç±» (Mock Streamlit) ====================
# ä¸ºäº†å¤ç”¨ train_utils ä»£ç ï¼Œæˆ‘ä»¬éœ€è¦æ¨¡æ‹Ÿ Streamlit çš„è¿›åº¦æ¡å’Œæ–‡æœ¬å¯¹è±¡
class MockProgressBar:
    def progress(self, value):
        # é’ˆå¯¹ Log æ¨¡å¼çš„ä¼˜åŒ–ï¼šç›´æ¥è·³è¿‡ï¼Œä»€ä¹ˆéƒ½ä¸æ‰“å°
        # è¿™æ ·æ—¥å¿—æ–‡ä»¶é‡Œå°±ä¸ä¼šæœ‰æˆåƒä¸Šä¸‡è¡Œ "[======...]" äº†
        pass

class MockStatusText:
    def text(self, msg):
        # å°†å…³é”®ä¿¡æ¯æ‰“å°åˆ°æ§åˆ¶å°
        print(f"    â””â”€ {msg}")

# ==================== 2. æ•°æ®åŠ è½½å‡½æ•° ====================

def process_mat_files(data_root="data"):
    X_list = []
    y_list = []
    groups_list = []
    
    # 1. éå† s1 åˆ° s25
    for subject_id in range(1, 25):
        subject_name = f"s{subject_id}"
        # å¯»æ‰¾å¯¹åº”çš„ E1 æ–‡ä»¶: data/s1/S1_A1_E1.mat
        folder_path = os.path.join(data_root, subject_name)
        mat_file = os.path.join(folder_path, f"S{subject_id}_A1_E1.mat")
        
        if not os.path.exists(mat_file):
            print(f"âš ï¸ è·³è¿‡: æ‰¾ä¸åˆ° {mat_file}")
            continue
            
        print(f"æ­£åœ¨å¤„ç†: {mat_file}")
        
        try:
            # === è¯»å– MAT æ–‡ä»¶ ===
            mat_data = sio.loadmat(mat_file)
            
            # 1. è·å– EMG æ•°æ® (å–å‰8åˆ—ï¼Œå¹¶åšç‰¹å®šçš„å½’ä¸€åŒ–)
            # å‡è®¾ emg å˜é‡åå°±æ˜¯ 'emg'
            raw_emg = mat_data['emg'][:, :8] 
            emg_data = (raw_emg / 0.0024) - 1
            
            # 2. è·å–æ ‡ç­¾ (restimulus)
            # ä¹Ÿå°±æ˜¯ç¬¬ 12 åˆ— (å¦‚æœä»1å¼€å§‹æ•°)ï¼Œmatlabé‡Œå« restimulus
            stimulus = mat_data['restimulus'].flatten()

            subj_act_X, subj_act_y, subj_act_groups = [], [], []
            subj_rest_X, subj_rest_y, subj_rest_groups = [], [], []
            
            # === æ ¸å¿ƒåˆ‡ç‰‡é€»è¾‘ ===
            
            # æŠ€å·§ï¼šä½¿ç”¨ np.diff æ‰¾åˆ°çŠ¶æ€å˜åŒ–çš„è¾¹ç¼˜
            # 0->1 (åŠ¨ä½œå¼€å§‹), 1->0 (åŠ¨ä½œç»“æŸ)
            # ä¸ºäº†å¤„ç†æ–¹ä¾¿ï¼Œæˆ‘ä»¬åœ¨å‰åè¡¥0
            stim_padded = np.concatenate(([0], stimulus, [0]))
            diff = np.diff(stim_padded)
            
            # æ‰¾åˆ°æ‰€æœ‰åŠ¨ä½œçš„ èµ·å§‹ç´¢å¼• å’Œ ç»“æŸç´¢å¼•
            # where(diff != 0) ä¼šè¿”å›å˜åŒ–ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦æˆå¯¹å¤„ç†
            # è¿™ç§æ–¹æ³•å¯¹äºæ•´æ´çš„æ•°æ®ï¼ˆ000111000222000ï¼‰å¾ˆæœ‰æ•ˆ
            
            # ä½†æ›´ç®€å•çš„æ–¹æ³•å¯èƒ½æ˜¯ï¼šåˆ©ç”¨ ndimage.label æ‰¾è¿é€šåŸŸï¼ˆæ²¿ç”¨ä½ ä¹‹å‰çš„æ€è·¯ï¼‰
            labeled_array, num_features = ndimage.label(stimulus > 0)
            
            # --- æå–åŠ¨ä½œæ ·æœ¬ ---
            for i in range(1, num_features + 1):
                # æ‰¾åˆ°è¿™ä¸ªåŠ¨ä½œçš„æ‰€æœ‰ç´¢å¼•
                indices = np.where(labeled_array == i)[0]
                
                # è·å–è¯¥æ®µåŠ¨ä½œçš„æ ‡ç­¾ (å–ä¸­ä½æ•°å€¼ï¼Œé˜²æ­¢è¾¹ç¼˜æŠ–åŠ¨)
                current_label = int(np.median(stimulus[indices]))
                
                # è¿‡æ»¤ï¼šåªå– 1, 2, 5, 6
                if current_label not in TARGET_LABELS:
                    continue
                
                # æ‰¾åˆ°ä¸­é—´è¡Œ
                center_idx = int((indices[0] + indices[-1]) / 2)
                
                # ä¸Šä¸‹å„å– 150 (èŒƒå›´ï¼šcenter-150 åˆ° center+150)
                start_win = center_idx - 150
                end_win = center_idx + 150
                
                # è¾¹ç•Œæ£€æŸ¥
                if start_win < 0 or end_win > len(emg_data):
                    continue
                
                # åˆ‡ç‰‡ (æ³¨æ„ Python åˆ‡ç‰‡æ˜¯å·¦é—­å³å¼€ï¼Œæ‰€ä»¥ end_win è¦ä¸è¦+1å–å†³äºä½ æƒ³å–300è¿˜æ˜¯301ç‚¹)
                # "ä¸Šä¸‹å„150" é€šå¸¸æŒ‡ center ä¹Ÿæ˜¯ä¸€ä¸ªç‚¹ï¼Œæ€»å…± 150+1+150 = 301?
                # æˆ–è€…æ€»å…±300? è¿™é‡Œæš‚å– [center-150 : center+150] (é•¿åº¦300)
                window = emg_data[start_win:end_win]
                
                subj_act_X.append(window)
                subj_act_y.append(current_label)
                subj_act_groups.append(f"{subject_name}_act_{i}") # ç”¨äºåˆ†ç»„éªŒè¯
                
            # --- æå–é™æ¯æ ·æœ¬ (Rest) ---
            # é€»è¾‘ï¼šæ‰¾åˆ° restimulus ä¸º 0 çš„è¡Œï¼Œé¿å¼€åŠ¨ä½œè¾¹ç¼˜
            
            # 1. è†¨èƒ€åŠ¨ä½œåŒºåŸŸ (ä½œä¸º Buffer)
            # æ¯”å¦‚æˆ‘ä»¬è¦é¿å¼€åŠ¨ä½œå‰å 100 è¡Œ (1ç§’)
            buffer_size = 100
            mask_active = stimulus > 0
            # è†¨èƒ€ï¼šè®©åŠ¨ä½œåŒºå˜å¤§ï¼Œè¿™æ ·éåŠ¨ä½œåŒº(é™æ¯)å°±å˜å°äº†ï¼Œç›¸å½“äºåšäº† Erosion
            mask_forbidden = ndimage.binary_dilation(mask_active, structure=np.ones(buffer_size))
            
            mask_rest = ~mask_forbidden # å–åï¼Œå¾—åˆ°å®‰å…¨çš„é™æ¯åŒº
            
            labeled_rest, num_rest = ndimage.label(mask_rest)
            
            # è¿™é‡Œçš„é€»è¾‘å¯ä»¥çµæ´»ï¼šæ¯æ®µé™æ¯åŒºå–å‡ ä¸ªåˆ‡ç‰‡ï¼Ÿ
            # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åœ¨æ¯æ®µè¶³å¤Ÿé•¿çš„é™æ¯åŒºé‡Œï¼Œæ¯éš”ä¸€å®šè·ç¦»åˆ‡ä¸€ä¸ª
            for i in range(1, num_rest + 1):
                r_indices = np.where(labeled_rest == i)[0]
                
                # å¦‚æœè¿™æ®µé™æ¯å¤ªçŸ­ (å°äº 300)ï¼Œå°±è·³è¿‡
                if len(r_indices) < 300: continue
                
                # åœ¨è¿™æ®µé‡Œåˆ‡ç‰‡ï¼Œæ¯”å¦‚å–ä¸­é—´ï¼Œæˆ–è€…æ¯éš” 300 ç‚¹åˆ‡ä¸€ä¸ª
                # è¿™é‡Œæ¼”ç¤ºï¼šåªå–è¿™æ®µé™æ¯çš„æœ€ä¸­é—´ä¸€æ®µ
                center_idx = int((r_indices[0] + r_indices[-1]) / 2)
                start_win = center_idx - 150
                end_win = center_idx + 150
                
                window = emg_data[start_win:end_win]
                
                subj_rest_X.append(window)
                subj_rest_y.append(0) # Label 0
                subj_rest_groups.append(f"{subject_name}_rest_{i}")


            if len(subj_act_X) > 0:
                # 1. å°†åŠ¨ä½œæ•°æ®ç›´æ¥åŠ å…¥ä¸»åˆ—è¡¨
                X_list.extend(subj_act_X)
                y_list.extend(subj_act_y)
                groups_list.extend(subj_act_groups)

                # 2. è®¡ç®—åˆé€‚çš„é™æ¯æ•°é‡
                # ç­–ç•¥ï¼šè®©é™æ¯æ€»æ•° â‰ˆ å•ä¸ªåŠ¨ä½œç±»åˆ«çš„å¹³å‡æ•°é‡ (è¿™æ ·æ˜¯å®Œç¾çš„ 1:1:1:1:1)
                num_classes_found = len(np.unique(subj_act_y))
                target_rest_count = int(len(subj_act_X) / num_classes_found) 
                
                # 3. å¯¹é™æ¯æ•°æ®è¿›è¡Œéšæœºé‡‡æ ·
                if len(subj_rest_X) > target_rest_count and target_rest_count > 0:
                    selected_indices = np.random.choice(len(subj_rest_X), target_rest_count, replace=False)
                    for idx in selected_indices:
                        X_list.append(subj_rest_X[idx])
                        y_list.append(subj_rest_y[idx])
                        groups_list.append(subj_rest_groups[idx])
                else:
                    # å¦‚æœé™æ¯æœ¬æ¥å°±ä¸å¤Ÿï¼Œå°±å…¨éƒ½è¦
                    X_list.extend(subj_rest_X)
                    y_list.extend(subj_rest_y)
                    groups_list.extend(subj_rest_groups)


        except Exception as e:
            print(f"âŒ å¤„ç†å‡ºé”™ {mat_file}: {e}")
            import traceback
            traceback.print_exc()

    return np.array(X_list), np.array(y_list), np.array(groups_list)

# ==================== 3. æ ¸å¿ƒè®­ç»ƒå¾ªç¯ ====================
def run_automation():
    # 1. å‡†å¤‡æ•°æ®
    
    # æ¨¡æ‹Ÿè¿›åº¦æ¡
    mock_bar = MockProgressBar()
    mock_status = MockStatusText()
    
    # åŠ è½½æ•°æ® (åªåŠ è½½ä¸€æ¬¡ï¼ŒèŠ‚çœæ—¶é—´)
    X, y, groups = process_mat_files(data_root="data")
    
    if len(X) == 0:
        print("âŒ ç”Ÿæˆæ ·æœ¬æ•°ä¸º 0ï¼Œé€€å‡ºã€‚")
        return
        
    print(f"\nğŸ“Š æ•°æ®å‡†å¤‡å°±ç»ª: X={X.shape}, y={y.shape}, Classes={np.unique(y)}")
    
    # åˆ‡åˆ†æ•°æ®
    train_idx, test_idx = train_utils.smart_split(
        X, y, groups, CONFIG['split_strategy'], test_size=CONFIG['test_size']
    )    
    
    train_idx = np.array(train_idx).astype(int)
    test_idx = np.array(test_idx).astype(int)


    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    groups_train = groups[train_idx]
    
    # æ˜ å°„æ ‡ç­¾ (ç¡®ä¿æ˜¯ 0, 1, 2, 3...)
    unique_labels = np.unique(y)
    num_classes = len(unique_labels)
    label_map = {original: new for new, original in enumerate(unique_labels)}
    y_train_mapped = np.array([label_map[i] for i in y_train])
    y_test_mapped = np.array([label_map[i] for i in y_test])
    
    input_shape = (X.shape[1], X.shape[2])

    MODELS_DIR = "E1_nina_trained_models"  
    if not os.path.exists(MODELS_DIR):
        os.makedirs(MODELS_DIR)
    
    # 2. å¾ªç¯å®éªŒ
    total_experiments = len(MODELS_TO_TEST) * len(OPTIMIZERS_TO_TEST) * len(VOTING_OPTIONS)
    current_exp = 0
    
    for model_name, model_builder in MODELS_TO_TEST:
        for opt_name, opt_class, lr, opt_params in OPTIMIZERS_TO_TEST:
            for use_voting in VOTING_OPTIONS:
                current_exp += 1
                exp_id = f"{model_name}_{opt_name}_Vote{use_voting}"
                print(f"\n\nğŸš€ [{current_exp}/{total_experiments}] å¼€å§‹å®éªŒ: {exp_id}")
                print("-" * 50)
                
                # æ„å»ºæ¨¡å‹
                tf.keras.backend.clear_session() # æ¸…ç†å†…å­˜
                model = model_builder(input_shape, num_classes)
                
                # æ„å»ºä¼˜åŒ–å™¨
                try:
                    optimizer = opt_class(learning_rate=lr, **opt_params)
                except Exception as e:
                    print(f"ä¼˜åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}, è·³è¿‡ã€‚")
                    continue
                
                # è®­ç»ƒ
                start_time = time.time()
                try:
                    history_dict = train_utils.train_with_voting_mechanism(
                        model, 
                        X_train, y_train_mapped, groups_train,
                        X_test, y_test_mapped,
                        epochs=CONFIG['epochs'],
                        batch_size=CONFIG['batch_size'],
                        samples_per_group=CONFIG['samples_per_group'],
                        vote_weight=CONFIG['voting_weight'] if use_voting else 0.0,
                        st_progress_bar=mock_bar,
                        st_status_text=mock_status,
                        use_mixup=False, # è‡ªåŠ¨åŒ–è„šæœ¬æš‚ä¸å¼€å¯Mixupä»¥ä¿æŒç®€å•ï¼Œå¯æŒ‰éœ€å¼€å¯
                        label_smoothing=CONFIG['label_smoothing'],
                        voting_start_epoch=CONFIG['voting_start_epoch'] if use_voting else 0,
                        optimizer=optimizer
                    )
                except Exception as e:
                    print(f"\nâŒ è®­ç»ƒå´©æºƒ: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
                    
                duration = time.time() - start_time
                
                save_name = f"{exp_id}.keras" # TF 2.10+ æ¨è .kerasï¼Œæ—§ç‰ˆå¯ç”¨ .h5
                save_path = os.path.join(MODELS_DIR, save_name)
                
                try:
                    model.save(save_path)
                    print(f"    ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
                except Exception as e:
                    print(f"    âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")

                # 3. è¯„ä¼°ä¸æ—¥å¿—ä¿å­˜
                save_log(exp_id, model, history_dict, X_test, y_test_mapped, 
                         label_map, duration, opt_name, lr, use_voting)

def save_log(exp_id, model, history, X_test, y_test, label_map, duration, opt_name, lr, use_voting):
    # è®¡ç®—é¢„æµ‹
    y_pred_probs = model.predict(X_test, verbose=0)
    y_pred = np.argmax(y_pred_probs, axis=1)
    
    # ç”ŸæˆæŠ¥å‘Š
    report_dict = classification_report(
        y_test, y_pred, 
        target_names=[str(k) for k in label_map.keys()], 
        output_dict=True
    )
    report_df = pd.DataFrame(report_dict).transpose()
    
    # æ–‡ä»¶å
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
    filename = os.path.join(LOG_DIR, f"{timestamp}_{exp_id}.txt")
    
    final_acc = history['val_accuracy'][-1]
    final_loss = history['val_loss'][-1]
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"Experiment ID: {exp_id}\n")
        f.write(f"Date: {timestamp}\n")
        f.write(f"Duration: {duration:.1f} seconds\n")
        f.write("="*40 + "\n")
        f.write(f"Subjects: {TARGET_SUBJECTS}\n")
        f.write(f"Labels: {TARGET_LABELS}\n")
        f.write(f"Model: {model.name}\n")
        f.write(f"Optimizer: {opt_name} (LR={lr})\n")
        f.write(f"Voting Mode: {'ON' if use_voting else 'OFF'}")
        if use_voting:
            f.write(f" (Start Epoch: {CONFIG['voting_start_epoch']})\n")
        else:
            f.write("\n")
        f.write("-" * 20 + "\n")
        f.write(f"Epochs: {CONFIG['epochs']}\n")
        f.write(f"Batch Size: {CONFIG['batch_size']}\n")
        f.write(f"Augment: {AUGMENT_CONFIG}\n")
        f.write("="*40 + "\n")
        f.write(f"Final Val Accuracy: {final_acc*100:.2f}%\n")
        f.write(f"Final Val Loss: {final_loss:.4f}\n")
        f.write("\n--- Classification Report ---\n")
        f.write(report_df.to_string())
        
    print(f"\nğŸ’¾ æ—¥å¿—å·²ä¿å­˜: {filename}")

if __name__ == "__main__":
    # è®¾ç½® GPU æ˜¾å­˜å¢é•¿ï¼Œé˜²æ­¢ OOM
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            print(e)
            
    run_automation()