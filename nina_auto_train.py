import scipy.io as sio
import os
import sys
import time
import glob
import datetime
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.metrics import classification_report
import scipy.ndimage as ndimage
import gc

# å¼•ç”¨ç°æœ‰æ¨¡å—
import train_utils
import nina_model as model_lib  # é¿å…å˜é‡åå†²çª

# ==================== 0. é…ç½®åŒºåŸŸ (æ ¹æ®éœ€æ±‚ä¿®æ”¹) ====================

# 1. ç›®æ ‡è®¾ç½®
TARGET_SUBJECTS = [f"s{i}" for i in range(1, 39)]  
TARGET_LABELS = [1, 2, 3, 4, 5, 6, 7, 8]                       # åªå–è¿™8ä¸ªåŠ¨ä½œ

# 2. å®éªŒå˜é‡ (Grid Search)
MODELS_TO_TEST = [
    ("Simple_CNN", model_lib.build_simple_cnn),
    ("Advanced_CRNN", model_lib.build_advanced_crnn),
    ("TCN", model_lib.build_tcn_model),
    ("ResNet1D", model_lib.build_resnet_model),
]

OPTIMIZERS_TO_TEST = [
    ("Adam", tf.keras.optimizers.Adam, 0.001, {}),
    ("AdamW", tf.keras.optimizers.AdamW, 0.001, {'weight_decay': 1e-4}),
    ("Nadam", tf.keras.optimizers.Nadam, 0.001, {}),
]

VOTING_OPTIONS = [False] # æ˜¯å¦å¼€å¯æŠ•ç¥¨

# 3. å›ºå®šå‚æ•°
CONFIG = {
    'fs': 2000,                # é‡‡æ ·ç‡                  
    'epochs': 100,
    'batch_size': 256,
    'test_size': 0.2,          # æµ‹è¯•é›†æ¯”ä¾‹
    'split_strategy': "ç•™æ–‡ä»¶éªŒè¯ (åŒå¤©/åŒäºº)",
    'label_smoothing': 0.1,    # æ ‡ç­¾å¹³æ»‘
    'voting_start_epoch': 20,  # æŠ•ç¥¨å¼€å¯æ—¶é—´
    'voting_weight': 0.5,      # æŠ•ç¥¨æƒé‡
    'samples_per_group': 5,    # æŠ•ç¥¨ç»„é‡‡æ ·æ•°
}

# 4. æ•°æ®å¢å¼ºé…ç½® (å¹…åº¦ç¼©æ”¾ + é«˜æ–¯å™ªå£°)
AUGMENT_CONFIG = {
    'enable_rest': True,       # åŠ å…¥é™æ¯
    'multiplier': 1,           # è‡ªåŠ¨åŒ–æµ‹è¯•é€šå¸¸ä¸æ— é™å€å¢ï¼Œè®¾ä¸º1æˆ–2å³å¯ï¼Œæˆ–è€…æŒ‰éœ€è°ƒæ•´
    'enable_scaling': True,    # å¹…åº¦ç¼©æ”¾
    'enable_noise': True,      # é«˜æ–¯å™ªå£°
    'enable_warp': False,
    'enable_shift': False,
    'enable_mask': False
}

LOG_DIR = "ninaDB2_E1_auto_train_logs"
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)

# ==================== 1. è¾…åŠ©ç±» (Mock Streamlit) ====================
# ä¸ºäº†å¤ç”¨ train_utils ä»£ç ï¼Œæˆ‘ä»¬éœ€è¦æ¨¡æ‹Ÿ Streamlit çš„è¿›åº¦æ¡å’Œæ–‡æœ¬å¯¹è±¡
class MockProgressBar:
    def progress(self, value):
        # é’ˆå¯¹ Log æ¨¡å¼çš„ä¼˜åŒ–ï¼šç›´æ¥è·³è¿‡ï¼Œä»€ä¹ˆéƒ½ä¸æ‰“å°
        # è¿™æ ·æ—¥å¿—æ–‡ä»¶é‡Œå°±ä¸ä¼šæœ‰æˆåƒä¸Šä¸‡è¡Œ "[======...]" äº†
        pass

class MockStatusText:
    def text(self, msg):
        # å°†å…³é”®ä¿¡æ¯æ‰“å°åˆ°æ§åˆ¶å°
        print(f"    â””â”€ {msg}")

# ==================== 2. æ•°æ®åŠ è½½å‡½æ•° ====================

def process_mat_files(data_root="data40"):
    X_list = []
    y_list = []
    groups_list = []
    
    # 1. éå† s1 åˆ° s38
    for subject_id in range(1, 39):
        subject_name = f"s{subject_id}"
        # å¯»æ‰¾å¯¹åº”çš„ E1 æ–‡ä»¶: data/s1/S1_A1_E1.mat
        folder_path = os.path.join(data_root, subject_name)
        mat_file = os.path.join(folder_path, f"S{subject_id}_A1_E1.mat")
        
        if not os.path.exists(mat_file):
            print(f"âš ï¸ è·³è¿‡: æ‰¾ä¸åˆ° {mat_file}")
            continue
            
        print(f"æ­£åœ¨å¤„ç†: {mat_file}")
        
        try:
            # === è¯»å– MAT æ–‡ä»¶ ===
            mat_data = sio.loadmat(mat_file)
            
            # 1. è·å– EMG æ•°æ® (å–å‰8åˆ—ï¼Œå¹¶åšç‰¹å®šçš„å½’ä¸€åŒ–)
            # å‡è®¾ emg å˜é‡åå°±æ˜¯ 'emg'
            raw_emg = mat_data['emg'][:, :8] 
            raw_emg = raw_emg[::2, :]
            stimulus = mat_data['restimulus'].flatten()[::2]
            emg_data = raw_emg

            subj_act_X, subj_act_y, subj_act_groups = [], [], []
            subj_rest_X, subj_rest_y, subj_rest_groups = [], [], []
            
            # === æ ¸å¿ƒåˆ‡ç‰‡é€»è¾‘ ===
            
            # æŠ€å·§ï¼šä½¿ç”¨ np.diff æ‰¾åˆ°çŠ¶æ€å˜åŒ–çš„è¾¹ç¼˜
            # 0->1 (åŠ¨ä½œå¼€å§‹), 1->0 (åŠ¨ä½œç»“æŸ)
            # ä¸ºäº†å¤„ç†æ–¹ä¾¿ï¼Œæˆ‘ä»¬åœ¨å‰åè¡¥0
            stim_padded = np.concatenate(([0], stimulus, [0]))
            diff = np.diff(stim_padded)
            
            # æ‰¾åˆ°æ‰€æœ‰åŠ¨ä½œçš„ èµ·å§‹ç´¢å¼• å’Œ ç»“æŸç´¢å¼•
            # where(diff != 0) ä¼šè¿”å›å˜åŒ–ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦æˆå¯¹å¤„ç†
            # è¿™ç§æ–¹æ³•å¯¹äºæ•´æ´çš„æ•°æ®ï¼ˆ000111000222000ï¼‰å¾ˆæœ‰æ•ˆ
            
            # ä½†æ›´ç®€å•çš„æ–¹æ³•å¯èƒ½æ˜¯ï¼šåˆ©ç”¨ ndimage.label æ‰¾è¿é€šåŸŸï¼ˆæ²¿ç”¨ä½ ä¹‹å‰çš„æ€è·¯ï¼‰
            labeled_array, num_features = ndimage.label(stimulus > 0)
            WIN_RADIUS = 1500
            # --- æå–åŠ¨ä½œæ ·æœ¬ ---
            for i in range(1, num_features + 1):
                indices = np.where(labeled_array == i)[0]
                current_label = int(np.median(stimulus[indices]))
                
                if current_label not in TARGET_LABELS:
                    continue
                
                center_idx = int((indices[0] + indices[-1]) / 2)
                
                start_win = center_idx - WIN_RADIUS
                end_win = center_idx + WIN_RADIUS
                
                if start_win < 0 or end_win > len(emg_data):
                    continue
                
                window = emg_data[start_win:end_win]
                
                #  Instance Normalization (æ®µå†… Z-Score)
                # å°†ä»»æ„é‡çº§çš„æ•°æ®æ ‡å‡†åŒ–åˆ° å‡å€¼0ï¼Œæ–¹å·®1
                mean = np.mean(window, axis=0)
                std = np.std(window, axis=0)
                std[std < 1e-8] = 1.0 # é˜²æ­¢é™¤é›¶
                
                window_norm = (window - mean) / std
                
                subj_act_X.append(window_norm)
                subj_act_y.append(current_label)
                subj_act_groups.append(f"{subject_name}_act_{i}")
                
            # --- æå–é™æ¯æ ·æœ¬ (Rest) ---
            buffer_size = 100 * 10 # é™é‡‡æ ·å Buffer ä¹Ÿç›¸åº”å‡åŠ (åŸ 100*20)
            mask_active = stimulus > 0
            mask_forbidden = ndimage.binary_dilation(mask_active, structure=np.ones(buffer_size))
            mask_rest = ~mask_forbidden 
            
            labeled_rest, num_rest = ndimage.label(mask_rest)
            
            for i in range(1, num_rest + 1):
                r_indices = np.where(labeled_rest == i)[0]
                
                # é•¿åº¦æ£€æŸ¥ (åŸ 300*20 -> ç° 300*10)
                if len(r_indices) < 300 * 10: continue
                
                center_idx = int((r_indices[0] + r_indices[-1]) / 2)
                start_win = center_idx - WIN_RADIUS
                end_win = center_idx + WIN_RADIUS
                
                if start_win < 0 or end_win > len(emg_data): continue

                window = emg_data[start_win:end_win]
                
                # é™æ¯æ•°æ®ä¹Ÿè¦åšåŒæ ·çš„æ ‡å‡†åŒ–ï¼Œä¿è¯åˆ†å¸ƒä¸€è‡´
                mean = np.mean(window, axis=0)
                std = np.std(window, axis=0)
                std[std < 1e-8] = 1.0
                window_norm = (window - mean) / std
                
                subj_rest_X.append(window_norm)
                subj_rest_y.append(0) 
                subj_rest_groups.append(f"{subject_name}_rest_{i}")

            # åˆå¹¶æ•°æ®
            if len(subj_act_X) > 0:
                X_list.extend(subj_act_X)
                y_list.extend(subj_act_y)
                groups_list.extend(subj_act_groups)

                # å¹³è¡¡é™æ¯æ ·æœ¬
                num_classes_found = len(np.unique(subj_act_y))
                if num_classes_found > 0:
                    target_rest_count = int(len(subj_act_X) / num_classes_found)
                else:
                    target_rest_count = 0
                
                if len(subj_rest_X) > target_rest_count and target_rest_count > 0:
                    selected_indices = np.random.choice(len(subj_rest_X), target_rest_count, replace=False)
                    for idx in selected_indices:
                        X_list.append(subj_rest_X[idx])
                        y_list.append(subj_rest_y[idx])
                        groups_list.append(subj_rest_groups[idx])
                else:
                    X_list.extend(subj_rest_X)
                    y_list.extend(subj_rest_y)
                    groups_list.extend(subj_rest_groups)

        except Exception as e:
            print(f"âŒ å¤„ç†å‡ºé”™ {mat_file}: {e}")
            # import traceback
            # traceback.print_exc()

    return np.array(X_list), np.array(y_list), np.array(groups_list)

# ==================== 3. æ ¸å¿ƒè®­ç»ƒå¾ªç¯ ====================
def run_automation():
    # 1. å‡†å¤‡æ•°æ®
    
    # æ¨¡æ‹Ÿè¿›åº¦æ¡
    mock_bar = MockProgressBar()
    mock_status = MockStatusText()
    
    # åŠ è½½æ•°æ® (åªåŠ è½½ä¸€æ¬¡ï¼ŒèŠ‚çœæ—¶é—´)
    X, y, groups = process_mat_files(data_root="data")
    
    if len(X) == 0:
        print("âŒ ç”Ÿæˆæ ·æœ¬æ•°ä¸º 0ï¼Œé€€å‡ºã€‚")
        return
        
    print(f"\nğŸ“Š æ•°æ®å‡†å¤‡å°±ç»ª: X={X.shape}, y={y.shape}, Classes={np.unique(y)}")
    
    # åˆ‡åˆ†æ•°æ®
    train_idx, test_idx = train_utils.smart_split(
        X, y, groups, CONFIG['split_strategy'], test_size=CONFIG['test_size']
    )    
    
    train_idx = np.array(train_idx).astype(int)
    test_idx = np.array(test_idx).astype(int)


    X_train, X_test = X[train_idx], X[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]
    groups_train = groups[train_idx]
    
    # æ˜ å°„æ ‡ç­¾ (ç¡®ä¿æ˜¯ 0, 1, 2, 3...)
    unique_labels = np.unique(y)
    num_classes = len(unique_labels)
    label_map = {original: new for new, original in enumerate(unique_labels)}
    y_train_mapped = np.array([label_map[i] for i in y_train])
    y_test_mapped = np.array([label_map[i] for i in y_test])
    
    input_shape = (X.shape[1], X.shape[2])

    MODELS_DIR = "ninaDB2_trained_models"  
    if not os.path.exists(MODELS_DIR):
        os.makedirs(MODELS_DIR)
    
    # 2. å¾ªç¯å®éªŒ
    total_experiments = len(MODELS_TO_TEST) * len(OPTIMIZERS_TO_TEST) * len(VOTING_OPTIONS)
    current_exp = 0
    
    for model_name, model_builder in MODELS_TO_TEST:
        for opt_name, opt_class, lr, opt_params in OPTIMIZERS_TO_TEST:
            for use_voting in VOTING_OPTIONS:
                current_exp += 1
                exp_id = f"{model_name}_{opt_name}_Vote{use_voting}"
                print(f"\n\nğŸš€ [{current_exp}/{total_experiments}] å¼€å§‹å®éªŒ: {exp_id}")
                print("-" * 50)
                
                # æ„å»ºæ¨¡å‹
                tf.keras.backend.clear_session() # æ¸…ç†å†…å­˜
                model = model_builder(input_shape, num_classes)
                
                # æ„å»ºä¼˜åŒ–å™¨
                try:
                    optimizer = opt_class(learning_rate=lr, **opt_params)
                except Exception as e:
                    print(f"ä¼˜åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}, è·³è¿‡ã€‚")
                    continue
                
                # è®­ç»ƒ
                start_time = time.time()
                try:
                    history_dict = train_utils.train_with_voting_mechanism(
                        model, 
                        X_train, y_train_mapped, groups_train,
                        X_test, y_test_mapped,
                        epochs=CONFIG['epochs'],
                        batch_size=CONFIG['batch_size'],
                        samples_per_group=CONFIG['samples_per_group'],
                        vote_weight=CONFIG['voting_weight'] if use_voting else 0.0,
                        st_progress_bar=mock_bar,
                        st_status_text=mock_status,
                        use_mixup=False, # è‡ªåŠ¨åŒ–è„šæœ¬æš‚ä¸å¼€å¯Mixupä»¥ä¿æŒç®€å•ï¼Œå¯æŒ‰éœ€å¼€å¯
                        label_smoothing=CONFIG['label_smoothing'],
                        voting_start_epoch=CONFIG['voting_start_epoch'] if use_voting else 0,
                        optimizer=optimizer
                    )
                except Exception as e:
                    print(f"\nâŒ è®­ç»ƒå´©æºƒ: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
                    
                duration = time.time() - start_time
                
                save_name = f"{exp_id}.keras" # TF 2.10+ æ¨è .kerasï¼Œæ—§ç‰ˆå¯ç”¨ .h5
                save_path = os.path.join(MODELS_DIR, save_name)
                
                try:
                    model.save(save_path)
                    print(f"    ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")
                except Exception as e:
                    print(f"    âš ï¸ æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")

                # 3. è¯„ä¼°ä¸æ—¥å¿—ä¿å­˜
                save_log(exp_id, model, history_dict, X_test, y_test_mapped, 
                         label_map, duration, opt_name, lr, use_voting)

def save_log(exp_id, model, history, X_test, y_test, label_map, duration, opt_name, lr, use_voting):
    # è®¡ç®—é¢„æµ‹
    y_pred_probs = model.predict(X_test, verbose=0)
    y_pred = np.argmax(y_pred_probs, axis=1)
    
    # ç”ŸæˆæŠ¥å‘Š
    report_dict = classification_report(
        y_test, y_pred, 
        target_names=[str(k) for k in label_map.keys()], 
        output_dict=True
    )
    report_df = pd.DataFrame(report_dict).transpose()
    
    # æ–‡ä»¶å
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M")
    filename = os.path.join(LOG_DIR, f"{timestamp}_{exp_id}.txt")
    
    final_acc = history['val_accuracy'][-1]
    final_loss = history['val_loss'][-1]
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"Experiment ID: {exp_id}\n")
        f.write(f"Date: {timestamp}\n")
        f.write(f"Duration: {duration:.1f} seconds\n")
        f.write("="*40 + "\n")
        f.write(f"Subjects: {TARGET_SUBJECTS}\n")
        f.write(f"Labels: {TARGET_LABELS}\n")
        f.write(f"Model: {model.name}\n")
        f.write(f"Optimizer: {opt_name} (LR={lr})\n")
        f.write(f"Voting Mode: {'ON' if use_voting else 'OFF'}")
        if use_voting:
            f.write(f" (Start Epoch: {CONFIG['voting_start_epoch']})\n")
        else:
            f.write("\n")
        f.write("-" * 20 + "\n")
        f.write(f"Epochs: {CONFIG['epochs']}\n")
        f.write(f"Batch Size: {CONFIG['batch_size']}\n")
        f.write(f"Augment: {AUGMENT_CONFIG}\n")
        f.write("="*40 + "\n")
        f.write(f"Final Val Accuracy: {final_acc*100:.2f}%\n")
        f.write(f"Final Val Loss: {final_loss:.4f}\n")
        f.write("\n--- Classification Report ---\n")
        f.write(report_df.to_string())
        
    print(f"\nğŸ’¾ æ—¥å¿—å·²ä¿å­˜: {filename}")

if __name__ == "__main__":
    # è®¾ç½® GPU æ˜¾å­˜å¢é•¿ï¼Œé˜²æ­¢ OOM
    gpus = tf.config.list_physical_devices('GPU')
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            print(e)
            
    run_automation()